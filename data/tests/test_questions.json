[
  {
    "id": 1,
    "question": "LangChain Expression Language (LCEL)을 사용하여 간단한 Prompt, Model, OutputParser 체인을 구성하는 파이썬 코드를 보여줘.",
    "expected_answer": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template('tell me a joke about {topic}')\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nchain.invoke({'topic': 'ice cream'})",
    "source_url": "https://python.langchain.com/docs/expression_language/get_started"
  },
  {
    "id": 2,
    "question": "RAG 애플리케이션에서 ChromaDB 벡터 저장소에 문서를 저장하려면 어떤 단계를 거쳐야 해?",
    "expected_answer": "문서 로드, 청킹, 임베딩 모델 선택 후, Chroma.from_documents() 메서드를 사용해 저장합니다. 필요한 임포트와 코드 예시는 다음과 같습니다....",
    "source_url": "https://python.langchain.com/docs/integrations/vectorstores/chroma/"
  },
  {
    "id": 3,
    "question": "LangChain에서 ReAct 프롬프트 구조는 어떻게 정의하고 Agent에 적용할 수 있어?",
    "expected_answer": "ReAct 프롬프트는 Thought, Action, Action Input을 포함하는 구조로, Agent가 추론하고 행동하도록 돕습니다. 프롬프트 템플릿 정의와 Agent 초기화 예시 코드를 참고하세요....",
    "source_url": "https://python.langchain.com/docs/modules/agents/agent_types/react/"
  },
  {
    "id": 4,
    "question": "RecursiveCharacterTextSplitter를 사용할 때, 코드 블록을 보존하기 위한 청킹 전략(청크 크기, 오버랩)을 어떻게 설정해야 해?",
    "expected_answer": "코드 블록이 쪼개지지 않도록 chunk_size는 일반적인 함수 길이보다 크게 설정하고, 청크 오버랩(chunk_overlap)을 겹치게 하여 문맥을 보존해야 합니다. 구체적인 파라미터 예시는...",
    "source_url": "https://python.langchain.com/docs/modules/data_connection/document_loaders/html/"
  },
  {
    "id": 5,
    "question": "WebBaseLoader를 사용하여 웹 페이지 내용을 로드하고, RecursiveCharacterTextSplitter로 분할하는 코드 예시를 보여주세요.",
    "expected_answer": "from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nloader = WebBaseLoader('https://lilianweng.github.io/posts/2023-06-23-agent/')\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nsplits = text_splitter.split_documents(docs)",
    "source_url": "https://python.langchain.com/docs/how_to/recursive_splitter"
  },
  {
    "id": 6,
    "question": "LangChain의 콜백(Callback) 시스템은 어떤 용도로 사용되나요?",
    "expected_answer": "콜백은 LLM 애플리케이션의 다양한 이벤트(예: 체인 시작, LLM 호출, 에러 발생 등)를 로깅, 모니터링, 스트리밍하는 데 사용됩니다. LangSmith와 같은 도구와 연동할 때 핵심적인 역할을 합니다.",
    "source_url": "https://python.langchain.com/docs/modules/callbacks/"
  },
  {
    "id": 7,
    "question": "VectorStoreRetriever에서 검색 유형(search_type)을 'mmr'로 설정하면 어떻게 동작하나요?",
    "expected_answer": "검색 유형을 'mmr'(Maximal Marginal Relevance)로 설정하면, 검색 결과의 유사도뿐만 아니라 다양성도 함께 고려하여 문서를 선택합니다. 이는 서로 비슷하지만 약간 다른 정보를 가진 문서를 함께 반환하는 데 도움이 됩니다.",
    "source_url": "https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore"
  },
  {
    "id": 8,
    "question": "PydanticOutputParser를 사용하는 전체 코드 예제를 보여주세요.",
    "expected_answer": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template('tell me a joke about {topic}')\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nchain.invoke({'topic': 'ice cream'})",
    "source_url": "https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic"
  },
  {
    "id": 9,
    "question": "ConversationBufferMemory를 RAG 체인에 추가하는 코드 예시를 알려주세요.",
    "expected_answer": "from langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n\nchain = RunnableWithMessageHistory(\n    runnable=rag_chain, \n    get_session_history=lambda session_id: ... , \n    input_messages_key='question', \n    history_messages_key='chat_history'\n)",
    "source_url": "https://python.langchain.com/docs/expression_language/how_to/message_history"
  },
  {
    "id": 10,
    "question": "RunnablePassthrough를 사용해서 retriever가 찾은 context를 LLM에 전달하는 코드 예시를 보여주세요.",
    "expected_answer": "from langchain_core.runnables import RunnablePassthrough\n\nretriever = vector.as_retriever()\n\ndef format_docs(docs):\n    return '\\n\\n'.join(doc.page_content for doc in docs)\n\nchain = (\n    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)",
    "source_url": "https://python.langchain.com/docs/expression_language/how_to/retrieval_chain"
  }
]