{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164ac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChain\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By # css, tag 선택자\n",
    "from selenium.webdriver.common.keys import Keys # 선택자가 선택을 한 부분에 대해 컨트롤러 (엔터, 탭, ESC, 스페이스바 )\n",
    "from selenium.webdriver.support.ui import WebDriverWait # 조건이 만족할 때 까지 지능적으로 '대기'\n",
    "'''\n",
    "time sleep 을 거는 경우 (복습)\n",
    "1.웹 마다 늦게 뜨는 경우\n",
    "2. 봇이 아닌척\n",
    "'''\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "# '대기' 조건 설정 가능\n",
    "# HTML 페이지 안에 , 슬라이더가 보일 때, 잠깐 기다려야 점진적으로 보이는 경우, 팝업이 접속 이후 보이는 경우\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "URL = f\"https://docs.langchain.com/oss/python/langchain/overview\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--headless\") # 로컬 기준 헤드리스를 안주고 실행하는 경우 : 개발할때\n",
    "# 로컬에서 헤드리스를 주는 경우 (백그라운드) : 운영 할 때  -> 빠르고 GUI 자원 안먹고, 디버깅은 어려움\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "# 공유 메모리 사용 비활성화 도커같은 가상환경 상에서 안정성 향상, 메모리 부족으로 크래쉬 방지\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "# 샌드박스 브라우저 프로세스를 격리 , 시스템 보안을 강화\n",
    "# 노 샌드 박스 -> 보안 크롤링을 더 자유롭게 할 수 있게함 -> 양날의칼\n",
    "\n",
    "#Selenium을 사용 \n",
    "driver = webdriver.Chrome(\n",
    "    options=options\n",
    ")\n",
    "\n",
    "#Selenium을 사용 브라우저 로드\n",
    "driver.get(URL)\n",
    "title = driver.title\n",
    "#driver.close()\n",
    "\n",
    "print(title)\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀레니움에서 실행한 브라우저를 종료하기\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#스크레이핑 함수 정의\n",
    "def get_scraping_soup3(side_link, head_menu_name, side_menu_name):\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    content_area = soup.find(\"div\", id=\"content-area\")\n",
    "\n",
    "    # 결과를 저장할 리스트와 문단을 임시 저장할 버퍼를 초기화합니다.\n",
    "    results = []\n",
    "    current_title = None\n",
    "    paragraph_buffer = []\n",
    "\n",
    "    def flush_paragraph_buffer():\n",
    "        \"\"\"버퍼에 쌓인 문단들을 하나의 텍스트로 합쳐 results에 추가하는 함수\"\"\"\n",
    "        nonlocal results, current_title, paragraph_buffer\n",
    "        if paragraph_buffer:\n",
    "            # 버퍼의 모든 문단을 줄바꿈 두 번으로 연결하여 하나의 텍스트로 만듭니다.\n",
    "            combined_text = \"\\n\\n\".join(paragraph_buffer)\n",
    "            \n",
    "            # 합쳐진 텍스트의 길이가 조건에 맞는지 확인합니다.\n",
    "            if 50 < len(combined_text) < 2000: # 최대 길이를 2000으로 늘렸습니다.\n",
    "                results.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": combined_text,\n",
    "                    \"side_link\": side_link,\n",
    "                    \"head_menu_name\": head_menu_name,\n",
    "                    \"side_menu_name\": side_menu_name\n",
    "                })\n",
    "            # 버퍼를 비웁니다.\n",
    "            paragraph_buffer = []\n",
    "\n",
    "    # content_area가 없을 경우를 대비한 예외 처리\n",
    "    if not content_area:\n",
    "        print(\"id가 'content-area'인 div를 찾을 수 없어 스크레이핑을 건너뜁니다.\")\n",
    "        return []\n",
    "\n",
    "    # content_area 내에서 제목, 코드, 문단 역할을 하는 span 태그를 찾습니다.\n",
    "    for tag in content_area.find_all([\"h1\", \"h2\", \"code\", \"span\"]):\n",
    "        \n",
    "        # 1. 태그가 h1 또는 h2인 경우\n",
    "        #if tag.name in [\"h1\", \"h2\"]:\n",
    "        #if tag.name in [\"h1\", \"h2\"] or (tag.name == \"span\" and tag.has_attr('class') == 'cursor-pointer'):    \n",
    "        if tag.name in [\"h1\", \"h2\"] or (tag.name == \"span\" and 'cursor-pointer' in tag.get('class', [])):\n",
    "            # 새로운 제목이 나왔으므로, 이전에 쌓아둔 문단 버퍼를 처리합니다.\n",
    "            flush_paragraph_buffer()\n",
    "            # 현재 제목을 업데이트합니다.\n",
    "            current_title = tag.get_text(strip=True)\n",
    "\n",
    "        # 2. 태그가 code인 경우 - 이 부분이 수정되었습니다.\n",
    "        elif tag.name == \"code\":\n",
    "            # 코드가 나왔으므로, 이전에 쌓아둔 문단 버퍼를 처리합니다.\n",
    "            flush_paragraph_buffer()\n",
    "            \n",
    "            # 코드 내용을 가져옵니다.\n",
    "            code_content = tag.get_text(strip=False)\n",
    "            \n",
    "            # 코드 내용의 길이가 조건에 맞는지 확인합니다.\n",
    "            if 20 < len(code_content) < 2000:\n",
    "                # 코드 내용을 results에 직접 추가합니다.\n",
    "                results.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"type\": \"code\",\n",
    "                    \"content\": code_content,\n",
    "                    \"side_link\": side_link,\n",
    "                    \"head_menu_name\": head_menu_name,\n",
    "                    \"side_menu_name\": side_menu_name\n",
    "                })\n",
    "\n",
    "        # 3. 태그가 문단(span)인 경우\n",
    "        elif tag.name == \"span\" and tag.get('data-as') == 'p':\n",
    "            # 문단 내용을 버퍼에 추가합니다.\n",
    "            text = tag.get_text(separator=\" \", strip=True)\n",
    "            if text:\n",
    "                paragraph_buffer.append(text)\n",
    "\n",
    "    # 루프가 끝난 후, 버퍼에 아직 처리되지 않은 문단이 남아있을 수 있으므로 마지막으로 처리합니다.\n",
    "    flush_paragraph_buffer()\n",
    "\n",
    "    print(f\"총 {len(results)}개의 데이터 조각이 수집되었습니다.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13990682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메뉴 링크의 href 속성만 미리 추출\n",
    "menu_elements = driver.find_elements(By.CSS_SELECTOR, \"a.link.nav-tabs-item.group\")\n",
    "menu_hrefs = [elem.get_attribute(\"href\") for elem in menu_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cdc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이드바 메뉴 a 태그 클래스에 해당하는 요소들 찾기\n",
    "#group flex items-center pr-3\n",
    "sidebar_links = driver.find_elements(By.CSS_SELECTOR, \"a.group.flex.items-center.pr-3\")\n",
    "\n",
    "# href 속성만 추출하여 리스트로 저장\n",
    "sidebar_hrefs = [link.get_attribute(\"href\") for link in sidebar_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030985a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(menu_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f37e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상단 메뉴: 'LangChain' (https://docs.langchain.com/oss/python/langchain/overview)\n",
      "사이드 메뉴 수집 시작 #######\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 시작...\n",
      "총 7개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Release notes' 스크레이핑 시작...\n",
      "총 44개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Release notes' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Migration guide' 스크레이핑 시작...\n",
      "총 86개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Migration guide' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Install' 스크레이핑 시작...\n",
      "총 4개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Install' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Quickstart' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Quickstart' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Philosophy' 스크레이핑 시작...\n",
      "총 5개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Philosophy' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Agents' 스크레이핑 시작...\n",
      "총 59개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Agents' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Models' 스크레이핑 시작...\n",
      "총 113개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Models' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Messages' 스크레이핑 시작...\n",
      "총 76개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Messages' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Tools' 스크레이핑 시작...\n",
      "총 30개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Tools' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Short-term memory' 스크레이핑 시작...\n",
      "총 43개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Short-term memory' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Streaming' 스크레이핑 시작...\n",
      "총 20개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Streaming' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Middleware' 스크레이핑 시작...\n",
      "총 101개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Middleware' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Structured output' 스크레이핑 시작...\n",
      "총 52개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Structured output' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Guardrails' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Guardrails' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Runtime' 스크레이핑 시작...\n",
      "총 11개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Runtime' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Context engineering' 스크레이핑 시작...\n",
      "총 45개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Context engineering' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Model Context Protocol (MCP)' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Model Context Protocol (MCP)' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Human-in-the-loop' 스크레이핑 시작...\n",
      "총 15개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Human-in-the-loop' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Multi-agent' 스크레이핑 시작...\n",
      "총 15개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Multi-agent' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Retrieval' 스크레이핑 시작...\n",
      "총 19개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Retrieval' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Long-term memory' 스크레이핑 시작...\n",
      "총 8개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Long-term memory' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Studio' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Studio' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Test' 스크레이핑 시작...\n",
      "총 50개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Test' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Deploy' 스크레이핑 시작...\n",
      "총 2개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Deploy' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Agent Chat UI' 스크레이핑 시작...\n",
      "총 9개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Agent Chat UI' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Observability' 스크레이핑 시작...\n",
      "총 19개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Observability' 스크레이핑 끝\n",
      "'LangChain' 메뉴의 사이드바 수집 완료 #######\n",
      "상단 메뉴: 'LangGraph' (https://docs.langchain.com/oss/python/langgraph/overview)\n",
      "사이드 메뉴 수집 시작 #######\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 시작...\n",
      "총 6개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Release notes' 스크레이핑 시작...\n",
      "총 8개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Release notes' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Migration guide' 스크레이핑 시작...\n",
      "총 17개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Migration guide' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Install' 스크레이핑 시작...\n",
      "총 4개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Install' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Quickstart' 스크레이핑 시작...\n",
      "총 15개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Quickstart' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Local server' 스크레이핑 시작...\n",
      "총 19개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Local server' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Thinking in LangGraph' 스크레이핑 시작...\n",
      "총 45개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Thinking in LangGraph' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Workflows + agents' 스크레이핑 시작...\n",
      "총 17개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Workflows + agents' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Persistence' 스크레이핑 시작...\n",
      "총 91개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Persistence' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Durable execution' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Durable execution' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Streaming' 스크레이핑 시작...\n",
      "총 51개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Streaming' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Interrupts' 스크레이핑 시작...\n",
      "총 46개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Interrupts' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Time travel' 스크레이핑 시작...\n",
      "총 20개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Time travel' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Memory' 스크레이핑 시작...\n",
      "총 53개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Memory' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Subgraphs' 스크레이핑 시작...\n",
      "총 27개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Subgraphs' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Application structure' 스크레이핑 시작...\n",
      "총 17개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Application structure' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Studio' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Studio' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Test' 스크레이핑 시작...\n",
      "총 10개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Test' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Deploy' 스크레이핑 시작...\n",
      "총 6개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Deploy' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Agent Chat UI' 스크레이핑 시작...\n",
      "총 9개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Agent Chat UI' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Observability' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Observability' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Runtime' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Runtime' 스크레이핑 끝\n",
      "'LangGraph' 메뉴의 사이드바 수집 완료 #######\n",
      "상단 메뉴: 'Deep Agents' (https://docs.langchain.com/oss/python/deepagents/overview)\n",
      "사이드 메뉴 수집 시작 #######\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 시작...\n",
      "총 8개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Quickstart' 스크레이핑 시작...\n",
      "총 8개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Quickstart' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Customization' 스크레이핑 시작...\n",
      "총 9개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Customization' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Agent harness' 스크레이핑 시작...\n",
      "총 11개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Agent harness' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Backends' 스크레이핑 시작...\n",
      "총 36개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Backends' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Subagents' 스크레이핑 시작...\n",
      "총 34개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Subagents' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Human-in-the-loop' 스크레이핑 시작...\n",
      "총 25개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Human-in-the-loop' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Long-term memory' 스크레이핑 시작...\n",
      "총 32개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Long-term memory' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Middleware' 스크레이핑 시작...\n",
      "총 16개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Middleware' 스크레이핑 끝\n",
      "'Deep Agents' 메뉴의 사이드바 수집 완료 #######\n",
      "상단 메뉴: 'Integrations' (https://docs.langchain.com/oss/python/integrations/providers/overview)\n",
      "사이드 메뉴 수집 시작 #######\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 시작...\n",
      "총 10개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'All providers' 스크레이핑 시작...\n",
      "총 12개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'All providers' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'OpenAI' 스크레이핑 시작...\n",
      "총 6개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'OpenAI' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Anthropic (Claude)' 스크레이핑 시작...\n",
      "총 1개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Anthropic (Claude)' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Google' 스크레이핑 시작...\n",
      "총 292개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Google' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'AWS (Amazon)' 스크레이핑 시작...\n",
      "총 54개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'AWS (Amazon)' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Hugging Face' 스크레이핑 시작...\n",
      "총 38개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Hugging Face' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Microsoft' 스크레이핑 시작...\n",
      "총 114개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Microsoft' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Ollama' 스크레이핑 시작...\n",
      "총 3개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Ollama' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Chat models' 스크레이핑 시작...\n",
      "총 14개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Chat models' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Tools and toolkits' 스크레이핑 시작...\n",
      "총 9개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Tools and toolkits' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Retrievers' 스크레이핑 시작...\n",
      "총 12개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Retrievers' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Text splitters' 스크레이핑 시작...\n",
      "총 8개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Text splitters' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Embedding models' 스크레이핑 시작...\n",
      "총 18개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Embedding models' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Vector stores' 스크레이핑 시작...\n",
      "총 77개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Vector stores' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Document loaders' 스크레이핑 시작...\n",
      "총 15개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Document loaders' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Key-value stores' 스크레이핑 시작...\n",
      "총 8개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Key-value stores' 스크레이핑 끝\n",
      "'Integrations' 메뉴의 사이드바 수집 완료 #######\n",
      "상단 메뉴: 'Learn' (https://docs.langchain.com/oss/python/learn)\n",
      "사이드 메뉴 수집 시작 #######\n",
      "  -> 사이드바 메뉴: 'Learn' 스크레이핑 시작...\n",
      "총 16개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Learn' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Memory' 스크레이핑 시작...\n",
      "총 16개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Memory' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Context' 스크레이핑 시작...\n",
      "총 10개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Context' 스크레이핑 끝\n",
      "건너뜀: Graph API\n",
      "건너뜀: Functional API\n",
      "건너뜀: LangChain Academy\n",
      "  -> 사이드바 메뉴: 'Case studies' 스크레이핑 시작...\n",
      "총 1개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Case studies' 스크레이핑 끝\n",
      "'Learn' 메뉴의 사이드바 수집 완료 #######\n",
      "상단 메뉴: 'Reference' (https://docs.langchain.com/oss/python/reference/overview)\n",
      "상단 메뉴: 'Contribute' (https://docs.langchain.com/oss/python/contributing/overview)\n",
      "사이드 메뉴 수집 시작 #######\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 시작...\n",
      "총 9개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Overview' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Documentation' 스크레이핑 시작...\n",
      "총 29개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Documentation' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Code' 스크레이핑 시작...\n",
      "총 48개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Code' 스크레이핑 끝\n",
      "  -> 사이드바 메뉴: 'Co-marketing' 스크레이핑 시작...\n",
      "총 2개의 데이터 조각이 수집되었습니다.\n",
      "  -> 사이드바 메뉴: 'Co-marketing' 스크레이핑 끝\n",
      "'Contribute' 메뉴의 사이드바 수집 완료 #######\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    results_all = []\n",
    "    \n",
    "    # 메뉴 링크와 텍스트를 함께 추출합니다.\n",
    "    menu_elements = driver.find_elements(By.CSS_SELECTOR, \"a.link.nav-tabs-item.group\")\n",
    "    menu_info = [{\"href\": elem.get_attribute(\"href\"), \"text\": elem.text} for elem in menu_elements]\n",
    "\n",
    "    for head_info in menu_info:\n",
    "        head_href = head_info[\"href\"]\n",
    "        head_menu_name = head_info[\"text\"] # 헤더 메뉴 이름\n",
    "        \n",
    "        driver.get(head_href)\n",
    "        print(f\"상단 메뉴: '{head_menu_name}' ({head_href})\")\n",
    "        \n",
    "        if \"reference\" not in head_href:\n",
    "            print(f\"사이드 메뉴 수집 시작 #######\")\n",
    "            \n",
    "            # 사이드바 링크와 텍스트를 함께 추출합니다.\n",
    "            sidebar_links = driver.find_elements(By.CSS_SELECTOR, \"a.group.flex.items-center.pr-3\")\n",
    "            sidebar_info = []\n",
    "            for link in sidebar_links:\n",
    "                try:\n",
    "                    inner_div = link.find_element(By.CSS_SELECTOR, \"div > div\")\n",
    "                    text = inner_div.text\n",
    "                except NoSuchElementException:\n",
    "                    text = link.text\n",
    "                sidebar_info.append({\"href\": link.get_attribute(\"href\"), \"text\": text})\n",
    "\n",
    "            for side_info in sidebar_info:\n",
    "                side_href = side_info[\"href\"] # 사이드 메뉴 링크\n",
    "                side_menu_name = side_info[\"text\"] # 사이드 메뉴 이름\n",
    "\n",
    "                # 예외 처리 로직\n",
    "                non_valides = [\n",
    "                    \"https://docs.langchain.com/oss/python/langgraph/graph-api\",\n",
    "                    \"https://docs.langchain.com/oss/python/langgraph/functional-api\",\n",
    "                ]\n",
    "                if head_href == \"https://docs.langchain.com/oss/python/learn\":\n",
    "                    if side_href in non_valides or \"academy.langchain.com\" in side_href:\n",
    "                        print(f\"건너뜀: {side_menu_name}\")\n",
    "                        continue\n",
    "                \n",
    "                # 페이지 이동 및 스크레이핑\n",
    "                driver.get(side_href)\n",
    "                print(f\"  -> 사이드바 메뉴: '{side_menu_name}' 스크레이핑 시작...\")\n",
    "                \n",
    "                # 메타데이터를 인자로 전달합니다.\n",
    "                scraped_data = get_scraping_soup3(\n",
    "                    side_link=side_href, \n",
    "                    head_menu_name=head_menu_name, \n",
    "                    side_menu_name=side_menu_name\n",
    "                )\n",
    "                results_all.extend(scraped_data)\n",
    "                \n",
    "                print(f\"  -> 사이드바 메뉴: '{side_menu_name}' 스크레이핑 끝\")\n",
    "                time.sleep(1)  # 페이지 로딩 대기 시간을 1초로 줄였습니다.\n",
    "\n",
    "            print(f\"'{head_menu_name}' 메뉴의 사이드바 수집 완료 #######\")\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e525d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LangChain overview',\n",
       " 'type': 'text',\n",
       " 'content': 'LangChain v1.0 is now available!\\n\\nFor a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\\n\\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\\n\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\n\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\n\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.',\n",
       " 'side_link': 'https://docs.langchain.com/oss/python/langchain/overview',\n",
       " 'head_menu_name': 'LangChain',\n",
       " 'side_menu_name': 'Overview'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35fad656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2439"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e57b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2366"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results_all 중복 제거 \n",
    "unique_data = [dict(t) for t in {tuple(d.items()) for d in results_all}]\n",
    "len(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6e3ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'File structure',\n",
       "  'type': 'text',\n",
       "  'content': 'Below are examples of directory structures for applications:',\n",
       "  'side_link': 'https://docs.langchain.com/oss/python/langgraph/application-structure',\n",
       "  'head_menu_name': 'LangGraph',\n",
       "  'side_menu_name': 'Application structure'},\n",
       " {'title': 'Custom stores',\n",
       "  'type': 'text',\n",
       "  'content': 'You can also implement your own custom store by extending the BaseStore class. See the store interface documentation for more details.',\n",
       "  'side_link': 'https://docs.langchain.com/oss/python/integrations/stores',\n",
       "  'head_menu_name': 'Integrations',\n",
       "  'side_menu_name': 'Key-value stores'},\n",
       " {'title': 'Log to a project',\n",
       "  'type': 'code',\n",
       "  'content': 'export LANGSMITH_PROJECT=my-agent-project\\n',\n",
       "  'side_link': 'https://docs.langchain.com/oss/python/langchain/observability',\n",
       "  'head_menu_name': 'LangChain',\n",
       "  'side_menu_name': 'Observability'},\n",
       " {'title': 'langchain-classic',\n",
       "  'type': 'code',\n",
       "  'content': 'uv pip install langchain-classic\\n',\n",
       "  'side_link': 'https://docs.langchain.com/oss/python/migrate/langchain-v1',\n",
       "  'head_menu_name': 'LangChain',\n",
       "  'side_menu_name': 'Migration guide'},\n",
       " {'title': 'ScaNN (Local Index)',\n",
       "  'type': 'code',\n",
       "  'content': 'pip install scann langchain-community # Requires langchain-community\\n',\n",
       "  'side_link': 'https://docs.langchain.com/oss/python/integrations/providers/google',\n",
       "  'head_menu_name': 'Integrations',\n",
       "  'side_menu_name': 'Google'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type===> code\n",
      "title===> Log to a project\n",
      "content===> export LANGSMITH_PROJECT=my-agent-project\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> langchain-classic\n",
      "content===> uv pip install langchain-classic\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ScaNN (Local Index)\n",
      "content===> pip install scann langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prebuilt middleware\n",
      "content===> HumanInTheLoopMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Encryption\n",
      "content===> from_pycryptodome_aes\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Storage\n",
      "content===> from langchain_google_community import GCSDirectoryLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma local from Kaggle\n",
      "content===> from langchain_google_vertexai.gemma import GemmaChatLocalKaggle\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deprecations\n",
      "content===> AgentStateWithStructuredResponsePydantic\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke\n",
      "content===> from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
      "\n",
      "conversation = [\n",
      "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n",
      "    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n",
      "    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n",
      "    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n",
      "]\n",
      "\n",
      "response = model.invoke(conversation)\n",
      "print(response)  # AIMessage(\"J'adore créer des applications.\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured outputs\n",
      "content===> with_structured_output\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"COHERE_API_KEY\"):\n",
      "  os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n",
      "\n",
      "from langchain_cohere import CohereEmbeddings\n",
      "\n",
      "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Comprehend Moderation Chain\n",
      "content===> pip install boto3 nltk\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Memorystore for Redis\n",
      "content===> pip install langchain-google-memorystore-redis\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceEndpointEmbeddings\n",
      "content===> from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> El Carro for Oracle Workloads\n",
      "content===> from langchain_google_el_carro import ElCarroLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> make lint\n",
      "make test\n",
      "\n",
      "# For bugfixes involving integrations, also run:\n",
      "make integration_tests\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceEndpointEmbeddings\n",
      "content===> HuggingFaceEndpointEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Runtime context\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_id: str\n",
      "    session_id: str\n",
      "\n",
      "agent = create_agent(\n",
      "    model=model,\n",
      "    tools=tools,\n",
      "    context_schema=Context  \n",
      ")\n",
      "\n",
      "result = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n",
      "    context=Context(user_id=\"123\", session_id=\"abc\")  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using tasks in nodes\n",
      "content===> from typing import NotRequired\n",
      "from typing_extensions import TypedDict\n",
      "import uuid\n",
      "\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "import requests\n",
      "\n",
      "# Define a TypedDict to represent the state\n",
      "class State(TypedDict):\n",
      "    url: str\n",
      "    result: NotRequired[str]\n",
      "\n",
      "def call_api(state: State):\n",
      "    \"\"\"Example node that makes an API request.\"\"\"\n",
      "    result = requests.get(state['url']).text[:100]  # Side-effect  #\n",
      "    return {\n",
      "        \"result\": result\n",
      "    }\n",
      "\n",
      "# Create a StateGraph builder and add a node for the call_api function\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"call_api\", call_api)\n",
      "\n",
      "# Connect the start and end nodes to the call_api node\n",
      "builder.add_edge(START, \"call_api\")\n",
      "builder.add_edge(\"call_api\", END)\n",
      "\n",
      "# Specify a checkpointer\n",
      "checkpointer = InMemorySaver()\n",
      "\n",
      "# Compile the graph with the checkpointer\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "# Define a config with a thread ID.\n",
      "thread_id = uuid.uuid4()\n",
      "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
      "\n",
      "# Invoke the graph\n",
      "graph.invoke({\"url\": \"https://www.example.com\"}, config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quickstart\n",
      "content===> agent = create_deep_agent(backend=FilesystemBackend(root_dir=\"/Users/nh/Desktop/\"))\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic Usage\n",
      "content===> (<user_id>, \"memories\")\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-mistralai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 5. Test the API\n",
      "content===> pip install langgraph-sdk\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/deploy\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Choose models by task\n",
      "content===> subagents = [\n",
      "    {\n",
      "        \"name\": \"contract-reviewer\",\n",
      "        \"description\": \"Reviews legal documents and contracts\",\n",
      "        \"system_prompt\": \"You are an expert legal reviewer...\",\n",
      "        \"tools\": [read_document, analyze_contract],\n",
      "        \"model\": \"claude-sonnet-4-5-20250929\",  # Large context for long documents\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"financial-analyst\",\n",
      "        \"description\": \"Analyzes financial data and market trends\",\n",
      "        \"system_prompt\": \"You are an expert financial analyst...\",\n",
      "        \"tools\": [get_stock_price, analyze_fundamentals],\n",
      "        \"model\": \"openai:gpt-4o\",  # Better for numerical analysis\n",
      "    },\n",
      "]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Trends\n",
      "content===> pip install google-search-results langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Migrate tocreate_agent\n",
      "content===> langgraph.prebuilt.create_react_agent\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Kendra\n",
      "content===> from langchain_aws import AmazonKendraRetriever\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Model call limit\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[\n",
      "        ModelCallLimitMiddleware(\n",
      "            thread_limit=10,  # Max 10 calls per thread (across runs)\n",
      "            run_limit=5,  # Max 5 calls per run (single invocation)\n",
      "            exit_behavior=\"end\",  # Or \"error\" to raise exception\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bedrock Chat\n",
      "content===> Retrieval Augmented Generation\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool retry\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import ToolRetryMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[search_tool, database_tool],\n",
      "    middleware=[\n",
      "        ToolRetryMiddleware(\n",
      "            max_retries=3,  # Retry up to 3 times\n",
      "            backoff_factor=2.0,  # Exponential backoff multiplier\n",
      "            initial_delay=1.0,  # Start with 1 second delay\n",
      "            max_delay=60.0,  # Cap delays at 60 seconds\n",
      "            jitter=True,  # Add random jitter to avoid thundering herd\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use descriptive paths\n",
      "content===> # ✅ Good: Organized and descriptive\n",
      "/memories/user_preferences/language.txt\n",
      "/memories/projects/project_alpha/status.txt\n",
      "/memories/research/quantum_computing/sources.txt\n",
      "\n",
      "# ❌ Bad: Generic and unorganized\n",
      "/memories/temp.txt\n",
      "/memories/data.txt\n",
      "/memories/file1.txt\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware1.after_model()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Vision loader\n",
      "content===> from langchain_google_community.vision import CloudVisionLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a real-world agent\n",
      "content===> agent = create_agent(\n",
      "    model=model,\n",
      "    system_prompt=SYSTEM_PROMPT,\n",
      "    tools=[get_user_location, get_weather_for_location],\n",
      "    context_schema=Context,\n",
      "    response_format=ResponseFormat,\n",
      "    checkpointer=checkpointer\n",
      ")\n",
      "\n",
      "# `thread_id` is a unique identifier for a given conversation.\n",
      "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "response = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
      "    config=config,\n",
      "    context=Context(user_id=\"1\")\n",
      ")\n",
      "\n",
      "print(response['structured_response'])\n",
      "# ResponseFormat(\n",
      "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
      "#     weather_conditions=\"It's always sunny in Florida!\"\n",
      "# )\n",
      "\n",
      "\n",
      "# Note that we can continue the conversation using the same `thread_id`.\n",
      "response = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
      "    config=config,\n",
      "    context=Context(user_id=\"1\")\n",
      ")\n",
      "\n",
      "print(response['structured_response'])\n",
      "# ResponseFormat(\n",
      "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
      "#     weather_conditions=None\n",
      "# )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> High-level API\n",
      "content===> from typing import TypedDict\n",
      "\n",
      "from langgraph.constants import START\n",
      "from langgraph.graph import StateGraph\n",
      "\n",
      "class Essay(TypedDict):\n",
      "    topic: str\n",
      "    content: str | None\n",
      "    score: float | None\n",
      "\n",
      "def write_essay(essay: Essay):\n",
      "    return {\n",
      "        \"content\": f\"Essay about {essay['topic']}\",\n",
      "    }\n",
      "\n",
      "def score_essay(essay: Essay):\n",
      "    return {\n",
      "        \"score\": 10\n",
      "    }\n",
      "\n",
      "builder = StateGraph(Essay)\n",
      "builder.add_node(write_essay)\n",
      "builder.add_node(score_essay)\n",
      "builder.add_edge(START, \"write_essay\")\n",
      "builder.add_edge(\"write_essay\", \"score_essay\")\n",
      "\n",
      "# Compile the graph.\n",
      "# This will return a Pregel instance.\n",
      "graph = builder.compile()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom MCP servers\n",
      "content===> from mcp.server.fastmcp import FastMCP\n",
      "\n",
      "mcp = FastMCP(\"Math\")\n",
      "\n",
      "@mcp.tool()\n",
      "def add(a: int, b: int) -> int:\n",
      "    \"\"\"Add two numbers\"\"\"\n",
      "    return a + b\n",
      "\n",
      "@mcp.tool()\n",
      "def multiply(a: int, b: int) -> int:\n",
      "    \"\"\"Multiply two numbers\"\"\"\n",
      "    return a * b\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    mcp.run(transport=\"stdio\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Playwright URL Loader\n",
      "content===> pip install playwright unstructured\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> gathered = None\n",
      "for chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n",
      "    gathered = chunk if gathered is None else gathered + chunk\n",
      "    print(gathered.tool_calls)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic configuration\n",
      "content===> from langchain_core.tools import tool\n",
      "from deepagents import create_deep_agent\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "@tool\n",
      "def delete_file(path: str) -> str:\n",
      "    \"\"\"Delete a file from the filesystem.\"\"\"\n",
      "    return f\"Deleted {path}\"\n",
      "\n",
      "@tool\n",
      "def read_file(path: str) -> str:\n",
      "    \"\"\"Read a file from the filesystem.\"\"\"\n",
      "    return f\"Contents of {path}\"\n",
      "\n",
      "@tool\n",
      "def send_email(to: str, subject: str, body: str) -> str:\n",
      "    \"\"\"Send an email.\"\"\"\n",
      "    return f\"Sent email to {to}\"\n",
      "\n",
      "# Checkpointer is REQUIRED for human-in-the-loop\n",
      "checkpointer = MemorySaver()\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[delete_file, read_file, send_email],\n",
      "    interrupt_on={\n",
      "        \"delete_file\": True,  # Default: approve, edit, reject\n",
      "        \"read_file\": False,   # No interrupts needed\n",
      "        \"send_email\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing\n",
      "    },\n",
      "    checkpointer=checkpointer  # Required!\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Test writing guidelines\n",
      "content===> def test_document_processor_handles_empty_input():\n",
      "    \"\"\"Test processor gracefully handles empty document list.\"\"\"\n",
      "    processor = DocumentProcessor()\n",
      "\n",
      "    result = processor.process([])\n",
      "\n",
      "    assert result.success\n",
      "    assert result.processed_count == 0\n",
      "    assert len(result.errors) == 0\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> create_trajectory_llm_as_judge\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. View your agent in Studio\n",
      "content===> http://127.0.0.1:2024\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> first_model = init_chat_model(\n",
      "        model=\"gpt-4.1-mini\",\n",
      "        temperature=0,\n",
      "        configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
      "        config_prefix=\"first\",  # Useful when you have a chain with multiple models\n",
      ")\n",
      "\n",
      "first_model.invoke(\"what's your name\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Control the input to the subagent\n",
      "content===> from langchain.agents import AgentState\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "class CustomState(AgentState):\n",
      "    example_state_key: str\n",
      "\n",
      "@tool(\n",
      "    \"subagent1_name\",\n",
      "    description=\"subagent1_description\"\n",
      ")\n",
      "def call_subagent1(query: str, runtime: ToolRuntime[None, CustomState]):\n",
      "    # Apply any logic needed to transform the messages into a suitable input\n",
      "    subagent_input = some_logic(query, runtime.state[\"messages\"])\n",
      "    result = subagent1.invoke({\n",
      "        \"messages\": subagent_input,\n",
      "        # You could also pass other state keys here as needed.\n",
      "        # Make sure to define these in both the main and subagent's\n",
      "        # state schemas.\n",
      "        \"example_state_key\": runtime.state[\"example_state_key\"]\n",
      "    })\n",
      "    return result[\"messages\"][-1].content\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> CouchbaseSearchVectorStore\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Translate\n",
      "content===> pip install langchain-google-community[translate]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services\n",
      "content===> pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware1.after_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interrupts in tools\n",
      "content===> from langchain.tools import tool\n",
      "from langgraph.types import interrupt\n",
      "\n",
      "@tool\n",
      "def send_email(to: str, subject: str, body: str):\n",
      "    \"\"\"Send an email to a recipient.\"\"\"\n",
      "\n",
      "    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n",
      "    response = interrupt({\n",
      "        \"action\": \"send_email\",\n",
      "        \"to\": to,\n",
      "        \"subject\": subject,\n",
      "        \"body\": body,\n",
      "        \"message\": \"Approve sending this email?\"\n",
      "    })\n",
      "\n",
      "    if response.get(\"action\") == \"approve\":\n",
      "        # Resume value can override inputs before executing\n",
      "        final_to = response.get(\"to\", to)\n",
      "        final_subject = response.get(\"subject\", subject)\n",
      "        final_body = response.get(\"body\", body)\n",
      "        return f\"Email sent to {final_to} with subject '{final_subject}'\"\n",
      "    return \"Email cancelled by user\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure ML\n",
      "content===> from langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Firestore (Native Mode)\n",
      "content===> from langchain_google_firestore import FirestoreLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> pip install langchain_core langchain-anthropic langgraph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quick edit: fix a typo\n",
      "content===> fix(docs): summary of change\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"DEEPSEEK_API_KEY\"):\n",
      "  os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter API key for DeepSeek: \")\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure OpenAI\n",
      "content===> from langchain_openai import AzureOpenAI\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft OneDrive\n",
      "content===> from langchain_community.document_loaders import OneDriveLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Initialize a model\n",
      "content===> response = model.invoke(\"Why do parrots talk?\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> DatabricksVectorSearch\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Jobs\n",
      "content===> from langchain_community.tools.google_jobs import GoogleJobsQueryRun\n",
      "# Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility\n",
      "# from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamically selecting tools\n",
      "content===> from dataclasses import dataclass\n",
      "from typing import Literal, Callable\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
      "from langchain_core.tools import tool\n",
      "\n",
      "\n",
      "@tool\n",
      "def github_create_issue(repo: str, title: str) -> dict:\n",
      "    \"\"\"Create an issue in a GitHub repository.\"\"\"\n",
      "    return {\"url\": f\"https://github.com/{repo}/issues/1\", \"title\": title}\n",
      "\n",
      "@tool\n",
      "def gitlab_create_issue(project: str, title: str) -> dict:\n",
      "    \"\"\"Create an issue in a GitLab project.\"\"\"\n",
      "    return {\"url\": f\"https://gitlab.com/{project}/-/issues/1\", \"title\": title}\n",
      "\n",
      "all_tools = [github_create_issue, gitlab_create_issue]\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    provider: Literal[\"github\", \"gitlab\"]\n",
      "\n",
      "class ToolSelectorMiddleware(AgentMiddleware):\n",
      "    def wrap_model_call(\n",
      "        self,\n",
      "        request: ModelRequest,\n",
      "        handler: Callable[[ModelRequest], ModelResponse],\n",
      "    ) -> ModelResponse:\n",
      "        \"\"\"Select tools based on the VCS provider.\"\"\"\n",
      "        provider = request.runtime.context.provider\n",
      "\n",
      "        if provider == \"gitlab\":\n",
      "            selected_tools = [t for t in request.tools if t.name == \"gitlab_create_issue\"]\n",
      "        else:\n",
      "            selected_tools = [t for t in request.tools if t.name == \"github_create_issue\"]\n",
      "\n",
      "        request.tools = selected_tools\n",
      "        return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=all_tools,\n",
      "    middleware=[ToolSelectorMiddleware()],\n",
      "    context_schema=Context,\n",
      ")\n",
      "\n",
      "# Invoke with GitHub context\n",
      "agent.invoke(\n",
      "    {\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": \"Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`\"}]\n",
      "    },\n",
      "    context=Context(provider=\"github\"),\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content blocks\n",
      "content===> [{'type': 'reasoning',\n",
      "  'reasoning': '...',\n",
      "  'extras': {'signature': 'WaUjzkyp...'}},\n",
      " {'type': 'text', 'text': '...'}]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Finance\n",
      "content===> from langchain_community.tools.google_finance import GoogleFinanceQueryRun\n",
      "from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> MongoDBAtlasVectorSearch\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SystemMessageto string\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[check_weather],\n",
      "    system_prompt=\"You are a helpful assistant\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "from langgraph.checkpoint.redis import RedisSaver  \n",
      "\n",
      "model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
      "\n",
      "DB_URI = \"redis://localhost:6379\"\n",
      "with RedisSaver.from_conn_string(DB_URI) as checkpointer:  \n",
      "    # checkpointer.setup()\n",
      "\n",
      "    def call_model(state: MessagesState):\n",
      "        response = model.invoke(state[\"messages\"])\n",
      "        return {\"messages\": response}\n",
      "\n",
      "    builder = StateGraph(MessagesState)\n",
      "    builder.add_node(call_model)\n",
      "    builder.add_edge(START, \"call_model\")\n",
      "\n",
      "    graph = builder.compile(checkpointer=checkpointer)  \n",
      "\n",
      "    config = {\n",
      "        \"configurable\": {\n",
      "            \"thread_id\": \"1\"\n",
      "        }\n",
      "    }\n",
      "\n",
      "    for chunk in graph.stream(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
      "        config,  \n",
      "        stream_mode=\"values\"\n",
      "    ):\n",
      "        chunk[\"messages\"][-1].pretty_print()\n",
      "\n",
      "    for chunk in graph.stream(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
      "        config,  \n",
      "        stream_mode=\"values\"\n",
      "    ):\n",
      "        chunk[\"messages\"][-1].pretty_print()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Testing and validation\n",
      "content===> make lint\n",
      "make format\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Featured providers\n",
      "content===> langchain-google-genai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ToolStrategy(\n",
      "    schema=ProductRating,\n",
      "    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-voyageai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Response Format\n",
      "content===> ProviderStrategy[StructuredResponseT]\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tokens\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[get_weather],\n",
      ")\n",
      "for token, metadata in agent.stream(  \n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
      "    stream_mode=\"messages\",\n",
      "):\n",
      "    print(f\"node: {metadata['langgraph_node']}\")\n",
      "    print(f\"content: {token.content_blocks}\")\n",
      "    print(\"\\n\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> System prompt\n",
      "content===> agent = create_agent(\n",
      "    model,\n",
      "    tools,\n",
      "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Testing requirements\n",
      "content===> make integration_tests\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"NOMIC_API_KEY\"):\n",
      "  os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n",
      "\n",
      "from langchain_nomic import NomicEmbeddings\n",
      "\n",
      "embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom state schema\n",
      "content===> from langchain.agents.middleware import AgentState, AgentMiddleware\n",
      "from typing_extensions import NotRequired\n",
      "from typing import Any\n",
      "\n",
      "class CustomState(AgentState):\n",
      "    model_call_count: NotRequired[int]\n",
      "    user_id: NotRequired[str]\n",
      "\n",
      "class CallCounterMiddleware(AgentMiddleware[CustomState]):\n",
      "    state_schema = CustomState\n",
      "\n",
      "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
      "        # Access custom state properties\n",
      "        count = state.get(\"model_call_count\", 0)\n",
      "\n",
      "        if count > 10:\n",
      "            return {\"jump_to\": \"end\"}\n",
      "\n",
      "        return None\n",
      "\n",
      "    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
      "        # Update custom state\n",
      "        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text content\n",
      "content===> response = model.invoke([\n",
      "  HumanMessage(\"What is machine learning?\")\n",
      "])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for MySQL\n",
      "content===> from langchain_google_cloud_sql_mysql import MySQLLoader # MySQLEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image editor\n",
      "content===> from langchain_google_vertexai.vision_models import VertexAIImageEditorChat\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> pip install -U pymongo langgraph langgraph-checkpoint-mongodb\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quickstart\n",
      "content===> agent = create_deep_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Subagent middleware\n",
      "content===> from langchain.agents import create_agent\n",
      "from deepagents.middleware.subagents import SubAgentMiddleware\n",
      "from deepagents import CompiledSubAgent\n",
      "from langgraph.graph import StateGraph\n",
      "\n",
      "# Create a custom LangGraph graph\n",
      "def create_weather_graph():\n",
      "    workflow = StateGraph(...)\n",
      "    # Build your custom graph\n",
      "    return workflow.compile()\n",
      "\n",
      "weather_graph = create_weather_graph()\n",
      "\n",
      "# Wrap it in a CompiledSubAgent\n",
      "weather_subagent = CompiledSubAgent(\n",
      "    name=\"weather\",\n",
      "    description=\"This subagent can get weather in cities.\",\n",
      "    runnable=weather_graph\n",
      ")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    middleware=[\n",
      "        SubAgentMiddleware(\n",
      "            default_model=\"claude-sonnet-4-5-20250929\",\n",
      "            default_tools=[],\n",
      "            subagents=[weather_subagent],\n",
      "        )\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> AzureCogsImageAnalysisTool\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamic model selection\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import (\n",
      "    AgentMiddleware, ModelRequest, ModelRequestHandler\n",
      ")\n",
      "from langchain.messages import AIMessage\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "\n",
      "basic_model = ChatOpenAI(model=\"gpt-5-nano\")\n",
      "advanced_model = ChatOpenAI(model=\"gpt-5\")\n",
      "\n",
      "class DynamicModelMiddleware(AgentMiddleware):\n",
      "\n",
      "    def wrap_model_call(self, request: ModelRequest, handler: ModelRequestHandler) -> AIMessage:\n",
      "        if len(request.state.messages) > self.messages_threshold:\n",
      "            model = advanced_model\n",
      "        else:\n",
      "            model = basic_model\n",
      "\n",
      "        return handler(request.replace(model=model))\n",
      "\n",
      "    def __init__(self, messages_threshold: int) -> None:\n",
      "        self.messages_threshold = messages_threshold\n",
      "\n",
      "agent = create_agent(\n",
      "    model=basic_model,\n",
      "    tools=tools,\n",
      "    middleware=[DynamicModelMiddleware(messages_threshold=10)]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> WriteResult(error=...)\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Excel\n",
      "content===> from langchain_community.document_loaders import UnstructuredExcelLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a real-world agent\n",
      "content===> from langgraph.checkpoint.memory import InMemorySaver\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LangGraph\n",
      "content===> This issue is blocked by #123 and related to #456.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> for chunk in graph.stream(\n",
      "    {\"foo\": \"foo\"},\n",
      "    subgraphs=True, \n",
      "    stream_mode=\"updates\",\n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bring-your-own documents\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Examples\n",
      "content===> from langgraph.channels import EphemeralValue\n",
      "from langgraph.pregel import Pregel, NodeBuilder\n",
      "\n",
      "node1 = (\n",
      "    NodeBuilder().subscribe_only(\"a\")\n",
      "    .do(lambda x: x + x)\n",
      "    .write_to(\"b\")\n",
      ")\n",
      "\n",
      "app = Pregel(\n",
      "    nodes={\"node1\": node1},\n",
      "    channels={\n",
      "        \"a\": EphemeralValue(str),\n",
      "        \"b\": EphemeralValue(str),\n",
      "    },\n",
      "    input_channels=[\"a\"],\n",
      "    output_channels=[\"b\"],\n",
      ")\n",
      "\n",
      "app.invoke({\"a\": \"foo\"})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke a graph from a node\n",
      "content===> ((), {'parent_1': {'my_key': 'hi Bob'}})\n",
      "(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n",
      "(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n",
      "((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n",
      "((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma on Vertex AI Model Garden\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Isolate storage by assistant ID\n",
      "content===> config = {\n",
      "    \"configurable\": {\n",
      "        \"thread_id\": \"thread-123\",\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"assistant_id\": \"user-456\"  # Namespace isolation\n",
      "    }\n",
      "}\n",
      "\n",
      "agent.invoke({\"messages\": [...]}, config=config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Post-model hook\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[read_email, send_email],\n",
      "    middleware=[HumanInTheLoopMiddleware(\n",
      "        interrupt_on={\n",
      "            \"send_email\": True,\n",
      "            \"description\": \"Please review this email before sending\"\n",
      "        },\n",
      "    )]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Office 365 individual tools\n",
      "content===> from langchain_community.tools.office365 import O365CreateDraftMessage\n",
      "from langchain_community.tools.office365 import O365SearchEmails\n",
      "from langchain_community.tools.office365 import O365SearchEvents\n",
      "from langchain_community.tools.office365 import O365SendEvent\n",
      "from langchain_community.tools.office365 import O365SendMessage\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> from langgraph.checkpoint.postgres import PostgresSaver\n",
      "\n",
      "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
      "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n",
      "    builder = StateGraph(...)\n",
      "    graph = builder.compile(checkpointer=checkpointer)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deprecations\n",
      "content===> AgentStateWithStructuredResponse\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Word\n",
      "content===> from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Neptune with SPARQL\n",
      "content===> from langchain_aws.graphs import NeptuneRdfGraph\n",
      "from langchain_aws.chains import create_neptune_sparql_qa_chain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Wrong subagent being selected\n",
      "content===> subagents = [\n",
      "    {\n",
      "        \"name\": \"quick-researcher\",\n",
      "        \"description\": \"For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.\",\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"deep-researcher\",\n",
      "        \"description\": \"For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.\",\n",
      "    }\n",
      "]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Messages\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
      "from typing import Callable\n",
      "\n",
      "@wrap_model_call\n",
      "def inject_file_context(\n",
      "    request: ModelRequest,\n",
      "    handler: Callable[[ModelRequest], ModelResponse]\n",
      ") -> ModelResponse:\n",
      "    \"\"\"Inject context about files user has uploaded this session.\"\"\"\n",
      "    # Read from State: get uploaded files metadata\n",
      "    uploaded_files = request.state.get(\"uploaded_files\", [])  \n",
      "\n",
      "    if uploaded_files:\n",
      "        # Build context about available files\n",
      "        file_descriptions = []\n",
      "        for file in uploaded_files:\n",
      "            file_descriptions.append(\n",
      "                f\"- {file['name']} ({file['type']}): {file['summary']}\"\n",
      "            )\n",
      "\n",
      "        file_context = f\"\"\"Files you have access to in this conversation:\n",
      "{chr(10).join(file_descriptions)}\n",
      "\n",
      "Reference these files when answering questions.\"\"\"\n",
      "\n",
      "        # Inject file context before recent messages\n",
      "        messages = [  \n",
      "            *request.messages,\n",
      "            {\"role\": \"user\", \"content\": file_context},\n",
      "        ]\n",
      "        request = request.override(messages=messages)  \n",
      "\n",
      "    return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[inject_file_context]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ProviderStrategy\n",
      "content===> from langchain.agents.structured_output import ProviderStrategy\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    response_format=ProviderStrategy(ContactInfo)\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deprecations\n",
      "content===> langchain.agents.create_agent\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Chat Completions API\n",
      "content===> from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(\n",
      "    model=\"...\",  # Specify a model available on OpenRouter\n",
      "    api_key=\"OPENROUTER_API_KEY\",\n",
      "    base_url=\"https://openrouter.ai/api/v1\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deprecations\n",
      "content===> langchain.agents.middleware.human_in_the_loop.InterruptOnConfig\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Pause usinginterrupt\n",
      "content===> from langgraph.types import interrupt\n",
      "\n",
      "def approval_node(state: State):\n",
      "    # Pause and ask for approval\n",
      "    approved = interrupt(\"Do you approve this action?\")\n",
      "\n",
      "    # When you resume, Command(resume=...) returns that value here\n",
      "    return {\"approved\": approved}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use the same thread ID\n",
      "content===> # First call\n",
      "config = {\"configurable\": {\"thread_id\": \"my-thread\"}}\n",
      "result = agent.invoke(input, config=config)\n",
      "\n",
      "# Resume (use same config)\n",
      "result = agent.invoke(Command(resume={...}), config=config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gmail\n",
      "content===> # Load the whole toolkit\n",
      "from langchain_google_community import GmailToolkit\n",
      "\n",
      "# Or use individual tools\n",
      "from langchain_google_community.gmail.create_draft import GmailCreateDraft\n",
      "from langchain_google_community.gmail.get_message import GmailGetMessage\n",
      "from langchain_google_community.gmail.get_thread import GmailGetThread\n",
      "from langchain_google_community.gmail.search import GmailSearch\n",
      "from langchain_google_community.gmail.send_message import GmailSendMessage\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agent progress\n",
      "content===> stream_mode=\"updates\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Search\n",
      "content===> from langchain_google_community import GoogleSearchAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Provider strategy\n",
      "content===> create_agent.response_format\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Drive\n",
      "content===> from langchain_google_community import GoogleDriveLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use MCP tools\n",
      "content===> from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "client = MultiServerMCPClient(  \n",
      "    {\n",
      "        \"math\": {\n",
      "            \"transport\": \"stdio\",  # Local subprocess communication\n",
      "            \"command\": \"python\",\n",
      "            # Absolute path to your math_server.py file\n",
      "            \"args\": [\"/path/to/math_server.py\"],\n",
      "        },\n",
      "        \"weather\": {\n",
      "            \"transport\": \"streamable_http\",  # HTTP-based remote server\n",
      "            # Ensure you start your weather server on port 8000\n",
      "            \"url\": \"http://localhost:8000/mcp\",\n",
      "        }\n",
      "    }\n",
      ")\n",
      "\n",
      "tools = await client.get_tools()  \n",
      "agent = create_agent(\n",
      "    \"claude-sonnet-4-5-20250929\",\n",
      "    tools  \n",
      ")\n",
      "math_response = await agent.ainvoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n",
      ")\n",
      "weather_response = await agent.ainvoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tool selector\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import LLMToolSelectorMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[tool1, tool2, tool3, tool4, tool5, ...],  # Many tools\n",
      "    middleware=[\n",
      "        LLMToolSelectorMiddleware(\n",
      "            model=\"gpt-4o-mini\",  # Use cheaper model for selection\n",
      "            max_tools=3,  # Limit to 3 most relevant tools\n",
      "            always_include=[\"search\"],  # Always include certain tools\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic Usage\n",
      "content===> from langgraph.store.memory import InMemoryStore\n",
      "in_memory_store = InMemoryStore()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Firestore (Datastore Mode)\n",
      "content===> from langchain_google_datastore import DatastoreLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke a graph from a node\n",
      "content===> from typing_extensions import TypedDict\n",
      "from langgraph.graph.state import StateGraph, START\n",
      "\n",
      "# Define subgraph\n",
      "class SubgraphState(TypedDict):\n",
      "    # note that none of these keys are shared with the parent graph state\n",
      "    bar: str\n",
      "    baz: str\n",
      "\n",
      "def subgraph_node_1(state: SubgraphState):\n",
      "    return {\"baz\": \"baz\"}\n",
      "\n",
      "def subgraph_node_2(state: SubgraphState):\n",
      "    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n",
      "\n",
      "subgraph_builder = StateGraph(SubgraphState)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_node(subgraph_node_2)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Define parent graph\n",
      "class ParentState(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "def node_1(state: ParentState):\n",
      "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
      "\n",
      "def node_2(state: ParentState):\n",
      "    # Transform the state to the subgraph state\n",
      "    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n",
      "    # Transform response back to the parent state\n",
      "    return {\"foo\": response[\"bar\"]}\n",
      "\n",
      "\n",
      "builder = StateGraph(ParentState)\n",
      "builder.add_node(\"node_1\", node_1)\n",
      "builder.add_node(\"node_2\", node_2)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "builder.add_edge(\"node_1\", \"node_2\")\n",
      "graph = builder.compile()\n",
      "\n",
      "for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a real-world agent\n",
      "content===> SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
      "\n",
      "You have access to two tools:\n",
      "\n",
      "- get_weather_for_location: use this to get the weather for a specific location\n",
      "- get_user_location: use this to get the user's location\n",
      "\n",
      "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon OpenSearch Service\n",
      "content===> OpenSearch Dashboards\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Durability modes\n",
      "content===> checkpoint_during=True\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceInstructEmbeddings\n",
      "content===> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Static runtime context\n",
      "content===> from dataclasses import dataclass\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ContextSchema:\n",
      "    user_name: str\n",
      "\n",
      "@dynamic_prompt\n",
      "def personalized_prompt(request: ModelRequest) -> str:  \n",
      "    user_name = request.runtime.context.user_name\n",
      "    return f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_weather],\n",
      "    middleware=[personalized_prompt],\n",
      "    context_schema=ContextSchema\n",
      ")\n",
      "\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
      "    context=ContextSchema(user_name=\"John Smith\")  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agentic RAG\n",
      "content===> import requests\n",
      "from langchain.tools import tool\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "@tool\n",
      "def fetch_url(url: str) -> str:\n",
      "    \"\"\"Fetch text content from a URL\"\"\"\n",
      "    response = requests.get(url, timeout=10.0)\n",
      "    response.raise_for_status()\n",
      "    return response.text\n",
      "\n",
      "system_prompt = \"\"\"\\\n",
      "Use fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n",
      "\"\"\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[fetch_url], # A tool for retrieval\n",
      "    system_prompt=system_prompt,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Route to different backends\n",
      "content===> \"/memories/projects/\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add metadata to traces\n",
      "content===> with ls.tracing_context(\n",
      "    project_name=\"email-agent-test\",\n",
      "    enabled=True,\n",
      "    tags=[\"production\", \"email-assistant\", \"v1.0\"],\n",
      "    metadata={\"user_id\": \"user_123\", \"session_id\": \"session_456\", \"environment\": \"production\"}):\n",
      "    response = agent.invoke(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> from langchain_community.document_loaders.csv_loader import CSVLoader\n",
      "\n",
      "loader = CSVLoader(\n",
      "    ...  # Integration-specific parameters here\n",
      ")\n",
      "\n",
      "# Load all documents\n",
      "documents = loader.load()\n",
      "\n",
      "# For large datasets, lazily load documents\n",
      "for document in loader.lazy_load():\n",
      "    print(document)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gmail\n",
      "content===> pip install langchain-google-community[gmail]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 3. Install dependencies\n",
      "content===> cd path/to/your/app\n",
      "pip install -e .\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware1.before_model()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling strategy\n",
      "content===> Callable[[Exception], str]\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> How it works\n",
      "content===> Input: Hello\n",
      "Token: Hi\n",
      "Token:  there\n",
      "Token: !\n",
      "Token:  How\n",
      "Token:  can\n",
      "Token:  I\n",
      "...\n",
      "Full message: Hi there! How can I help today?\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Review and edit state\n",
      "content===> from langgraph.types import interrupt\n",
      "\n",
      "def review_node(state: State):\n",
      "    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\n",
      "    edited_content = interrupt({\n",
      "        \"instruction\": \"Review and edit this content\",\n",
      "        \"content\": state[\"generated_text\"]\n",
      "    })\n",
      "\n",
      "    # Update the state with the edited version\n",
      "    return {\"generated_text\": edited_content}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Subagent interrupts\n",
      "content===> agent = create_deep_agent(\n",
      "    tools=[delete_file, read_file],\n",
      "    interrupt_on={\n",
      "        \"delete_file\": True,\n",
      "        \"read_file\": False,\n",
      "    },\n",
      "    subagents=[{\n",
      "        \"name\": \"file-manager\",\n",
      "        \"description\": \"Manages file operations\",\n",
      "        \"system_prompt\": \"You are a file management assistant.\",\n",
      "        \"tools\": [delete_file, read_file],\n",
      "        \"interrupt_on\": {\n",
      "            # Override: require approval for reads in this subagent\n",
      "            \"delete_file\": True,\n",
      "            \"read_file\": True,  # Different from main agent!\n",
      "        }\n",
      "    }],\n",
      "    checkpointer=checkpointer\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cross-thread persistence\n",
      "content===> import uuid\n",
      "\n",
      "# Thread 1: Write to long-term memory\n",
      "config1 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Save my preferences to /memories/preferences.txt\"}]\n",
      "}, config=config1)\n",
      "\n",
      "# Thread 2: Read from long-term memory (different conversation!)\n",
      "config2 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"What are my preferences?\"}]\n",
      "}, config=config2)\n",
      "# Agent can read /memories/preferences.txt from the first thread\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Pre-bound models\n",
      "content===> # No longer supported\n",
      "model_with_tools = ChatOpenAI().bind_tools([some_tool])\n",
      "agent = create_agent(model_with_tools, tools=[])\n",
      "\n",
      "# Use instead\n",
      "agent = create_agent(\"gpt-4o-mini\", tools=[some_tool])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Responding to interrupts\n",
      "content===> from langgraph.types import Command\n",
      "\n",
      "# Human-in-the-loop leverages LangGraph's persistence layer.\n",
      "# You must provide a thread ID to associate the execution with a conversation thread,\n",
      "# so the conversation can be paused and resumed (as is needed for human review).\n",
      "config = {\"configurable\": {\"thread_id\": \"some_id\"}} \n",
      "# Run the graph until the interrupt is hit.\n",
      "result = agent.invoke(\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"Delete old records from the database\",\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    config=config \n",
      ")\n",
      "\n",
      "# The interrupt contains the full HITL request with action_requests and review_configs\n",
      "print(result['__interrupt__'])\n",
      "# > [\n",
      "# >    Interrupt(\n",
      "# >       value={\n",
      "# >          'action_requests': [\n",
      "# >             {\n",
      "# >                'name': 'execute_sql',\n",
      "# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n",
      "# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n",
      "# >             }\n",
      "# >          ],\n",
      "# >          'review_configs': [\n",
      "# >             {\n",
      "# >                'action_name': 'execute_sql',\n",
      "# >                'allowed_decisions': ['approve', 'reject']\n",
      "# >             }\n",
      "# >          ]\n",
      "# >       }\n",
      "# >    )\n",
      "# > ]\n",
      "\n",
      "\n",
      "# Resume with approval decision\n",
      "agent.invoke(\n",
      "    Command( \n",
      "        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"edit\", \"reject\"\n",
      "    ), \n",
      "    config=config # Same thread ID to resume the paused conversation\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for SQL Server\n",
      "content===> from langchain_google_cloud_sql_mssql import MSSQLLoader # MSSQLEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SageMaker Tracking\n",
      "content===> pip install google-search-results sagemaker\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpointer libraries\n",
      "content===> langgraph-checkpoint-postgres\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> pip install google-cloud-discoveryengine langchain-google-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Examples\n",
      "content===> {\n",
      "  \"dependencies\": [\"langchain_openai\", \"./your_package\"],\n",
      "  \"graphs\": {\n",
      "    \"my_agent\": \"./your_package/your_file.py:agent\"\n",
      "  },\n",
      "  \"env\": \"./.env\"\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> from langchain_community.tools.azure_cognitive_services import (\n",
      "    AzureCogsFormRecognizerTool,\n",
      "    AzureCogsImageAnalysisTool,\n",
      "    AzureCogsSpeech2TextTool,\n",
      "    AzureCogsText2SpeechTool,\n",
      "    AzureCogsTextAnalyticsHealthTool,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI\n",
      "content===> projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bing Search\n",
      "content===> BING_SUBSCRIPTION_KEY\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Anthropic prompt caching\n",
      "content===> from langchain_anthropic import ChatAnthropic\n",
      "from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "LONG_PROMPT = \"\"\"\n",
      "Please be a helpful assistant.\n",
      "\n",
      "<Lots more context ...>\n",
      "\"\"\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n",
      "    system_prompt=LONG_PROMPT,\n",
      "    middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n",
      ")\n",
      "\n",
      "# cache store\n",
      "agent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]})\n",
      "\n",
      "# cache hit, system prompt is cached\n",
      "agent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 3. Define model node\n",
      "content===> from langchain.messages import SystemMessage\n",
      "\n",
      "\n",
      "def llm_call(state: dict):\n",
      "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
      "\n",
      "    return {\n",
      "        \"messages\": [\n",
      "            model_with_tools.invoke(\n",
      "                [\n",
      "                    SystemMessage(\n",
      "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
      "                    )\n",
      "                ]\n",
      "                + state[\"messages\"]\n",
      "            )\n",
      "        ],\n",
      "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
      "    }\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Debugging with interrupts\n",
      "content===> graph = builder.compile(\n",
      "    interrupt_before=[\"node_a\"],  \n",
      "    interrupt_after=[\"node_b\", \"node_c\"],  \n",
      "    checkpointer=checkpointer,\n",
      ")\n",
      "\n",
      "# Pass a thread ID to the graph\n",
      "config = {\n",
      "    \"configurable\": {\n",
      "        \"thread_id\": \"some_thread\"\n",
      "    }\n",
      "}\n",
      "\n",
      "# Run the graph until the breakpoint\n",
      "graph.invoke(inputs, config=config)  \n",
      "\n",
      "# Resume the graph\n",
      "graph.invoke(None, config=config)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face Hub Tools\n",
      "content===> load_huggingface_tool\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
      "    # Get the user id from the config\n",
      "    user_id = config[\"configurable\"][\"user_id\"]\n",
      "\n",
      "    # Namespace the memory\n",
      "    namespace = (user_id, \"memories\")\n",
      "\n",
      "    # Search based on the most recent message\n",
      "    memories = store.search(\n",
      "        namespace,\n",
      "        query=state[\"messages\"][-1].content,\n",
      "        limit=3\n",
      "    )\n",
      "    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n",
      "\n",
      "    # ... Use memories in the model call\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> for chunk in model_with_tools.stream(\n",
      "    \"What's the weather in Boston and Tokyo?\"\n",
      "):\n",
      "    # Tool call chunks arrive progressively\n",
      "    for tool_chunk in chunk.tool_call_chunks:\n",
      "        if name := tool_chunk.get(\"name\"):\n",
      "            print(f\"Tool: {name}\")\n",
      "        if id_ := tool_chunk.get(\"id\"):\n",
      "            print(f\"ID: {id_}\")\n",
      "        if args := tool_chunk.get(\"args\"):\n",
      "            print(f\"Args: {args}\")\n",
      "\n",
      "# Output:\n",
      "# Tool: get_weather\n",
      "# ID: call_SvMlU1TVIZugrFLckFE2ceRE\n",
      "# Args: {\"lo\n",
      "# Args: catio\n",
      "# Args: n\": \"B\n",
      "# Args: osto\n",
      "# Args: n\"}\n",
      "# Tool: get_weather\n",
      "# ID: call_QMZdy6qInx13oWKE7KhuhOLR\n",
      "# Args: {\"lo\n",
      "# Args: catio\n",
      "# Args: n\": \"T\n",
      "# Args: okyo\n",
      "# Args: \"}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI\n",
      "content===> export AZURE_AI_CREDENTIAL=your-api-key\n",
      "export AZURE_AI_ENDPOINT=your-endpoint\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> tool_args_match_overrides\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Getting Started\n",
      "content===> from toolbox_langchain import ToolboxClient\n",
      "\n",
      "async with ToolboxClient(\"http://127.0.0.1:5000\") as client:\n",
      "\n",
      "    tools = client.load_toolset()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> {\n",
      "  \"dependencies\": [\".\"],\n",
      "  \"graphs\": {\n",
      "    \"agent\": \"./src/agent.py:agent\"\n",
      "  },\n",
      "  \"env\": \".env\"\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining state via middleware\n",
      "content===> from langchain.agents import AgentState\n",
      "from langchain.agents.middleware import AgentMiddleware\n",
      "\n",
      "\n",
      "class CustomState(AgentState):\n",
      "    user_preferences: dict\n",
      "\n",
      "class CustomMiddleware(AgentMiddleware):\n",
      "    state_schema = CustomState\n",
      "    tools = [tool1, tool2]\n",
      "\n",
      "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
      "        ...\n",
      "\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=tools,\n",
      "    middleware=[CustomMiddleware()]\n",
      ")\n",
      "\n",
      "# The agent can now track additional state beyond messages\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
      "    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trim messages\n",
      "content===> from langchain_core.messages.utils import (  \n",
      "    trim_messages,  \n",
      "    count_tokens_approximately  \n",
      ")  \n",
      "\n",
      "def call_model(state: MessagesState):\n",
      "    messages = trim_messages(  \n",
      "        state[\"messages\"],\n",
      "        strategy=\"last\",\n",
      "        token_counter=count_tokens_approximately,\n",
      "        max_tokens=128,\n",
      "        start_on=\"human\",\n",
      "        end_on=(\"human\", \"tool\"),\n",
      "    )\n",
      "    response = model.invoke(messages)\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "builder = StateGraph(MessagesState)\n",
      "builder.add_node(call_model)\n",
      "...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add a graph as a node\n",
      "content===> {'node_1': {'foo': 'hi! foo'}}\n",
      "{'node_2': {'foo': 'hi! foobar'}}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Message prompts\n",
      "content===> from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
      "\n",
      "messages = [\n",
      "    SystemMessage(\"You are a poetry expert\"),\n",
      "    HumanMessage(\"Write a haiku about spring\"),\n",
      "    AIMessage(\"Cherry blossoms bloom...\")\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware3.after_model()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> State type restrictions\n",
      "content===> from langchain.agents import AgentState, create_agent\n",
      "\n",
      "# AgentState is a TypedDict\n",
      "class CustomAgentState(AgentState):  \n",
      "    user_id: str\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=tools,\n",
      "    state_schema=CustomAgentState  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Data\n",
      "content===> pip install azureml-fsspec, azure-ai-generative\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma on Vertex AI Model Garden\n",
      "content===> from langchain_google_vertexai.gemma import GemmaChatVertexAIModelGarden\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> ((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), {'node_2': {'foo': 'hi! foobar'}})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bedrock token usage\n",
      "content===> from langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> from langchain.messages import RemoveMessage  \n",
      "\n",
      "def delete_messages(state):\n",
      "    messages = state[\"messages\"]\n",
      "    if len(messages) > 2:\n",
      "        # remove the earliest two messages\n",
      "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> def custom_error_handler(error: Exception) -> str:\n",
      "    if isinstance(error, StructuredOutputValidationError):\n",
      "        return \"There was an issue with the format. Try again.\n",
      "    elif isinstance(error, MultipleStructuredOutputsError):\n",
      "        return \"Multiple structured outputs were returned. Pick the most relevant one.\"\n",
      "    else:\n",
      "        return f\"Error: {str(error)}\"\n",
      "\n",
      "ToolStrategy(\n",
      "    schema=ToolStrategy(Union[ContactInfo, EventDetails]),\n",
      "    handle_errors=custom_error_handler\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining state via middleware\n",
      "content===> from langchain.agents.middleware import AgentState, AgentMiddleware\n",
      "from typing_extensions import NotRequired\n",
      "from typing import Any\n",
      "\n",
      "class CustomState(AgentState):\n",
      "    model_call_count: NotRequired[int]\n",
      "\n",
      "class CallCounterMiddleware(AgentMiddleware[CustomState]):\n",
      "    state_schema = CustomState  \n",
      "\n",
      "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
      "        count = state.get(\"model_call_count\", 0)\n",
      "        if count > 10:\n",
      "            return {\"jump_to\": \"end\"}\n",
      "        return None\n",
      "\n",
      "    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
      "        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[...],\n",
      "    middleware=[CallCounterMiddleware()]  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Minimize tool sets\n",
      "content===> # ✅ Good: Focused tool set\n",
      "email_agent = {\n",
      "    \"name\": \"email-sender\",\n",
      "    \"tools\": [send_email, validate_email],  # Only email-related\n",
      "}\n",
      "\n",
      "# ❌ Bad: Too many tools\n",
      "email_agent = {\n",
      "    \"name\": \"email-sender\",\n",
      "    \"tools\": [send_email, web_search, database_query, file_upload],  # Unfocused\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Vector Search\n",
      "content===> VectorSearchVectorStore\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in subgraphs\n",
      "content===> from langgraph.graph import START, StateGraph\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from typing import TypedDict\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "# Subgraph\n",
      "\n",
      "def subgraph_node_1(state: State):\n",
      "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
      "\n",
      "subgraph_builder = StateGraph(State)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph = subgraph_builder.compile()  \n",
      "\n",
      "# Parent graph\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"node_1\", subgraph)  \n",
      "builder.add_edge(START, \"node_1\")\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "graph = builder.compile(checkpointer=checkpointer)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> StateBackend (ephemeral)\n",
      "content===> # By default we provide a StateBackend\n",
      "agent = create_deep_agent()\n",
      "\n",
      "# Under the hood, it looks like\n",
      "from deepagents.backends import StateBackend\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    backend=(lambda rt: StateBackend(rt))   # Note that the tools access State through the runtime.state\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face Text-to-Speech Model Inference.\n",
      "content===> OpenAI Text-to-Speech API\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Model Context Protocol (MCP)\n",
      "content===> langchain-mcp-adapters\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Get state history\n",
      "content===> config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "list(graph.get_state_history(config))\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Playwright URL Loader\n",
      "content===> from langchain_community.document_loaders.onenote import OneNoteLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Examples\n",
      "content===> ./your_package/your_file.py\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom tool name\n",
      "content===> @tool(\"web_search\")  # Custom name\n",
      "def search(query: str) -> str:\n",
      "    \"\"\"Search the web for information.\"\"\"\n",
      "    return f\"Results for: {query}\"\n",
      "\n",
      "print(search.name)  # web_search\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> from langgraph.graph import START, StateGraph\n",
      "from typing import TypedDict\n",
      "\n",
      "# Define subgraph\n",
      "class SubgraphState(TypedDict):\n",
      "    foo: str  # note that this key is shared with the parent graph state\n",
      "    bar: str\n",
      "\n",
      "def subgraph_node_1(state: SubgraphState):\n",
      "    return {\"bar\": \"bar\"}\n",
      "\n",
      "def subgraph_node_2(state: SubgraphState):\n",
      "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
      "\n",
      "subgraph_builder = StateGraph(SubgraphState)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_node(subgraph_node_2)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Define parent graph\n",
      "class ParentState(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "def node_1(state: ParentState):\n",
      "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
      "\n",
      "builder = StateGraph(ParentState)\n",
      "builder.add_node(\"node_1\", node_1)\n",
      "builder.add_node(\"node_2\", subgraph)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "builder.add_edge(\"node_1\", \"node_2\")\n",
      "graph = builder.compile()\n",
      "\n",
      "for chunk in graph.stream(\n",
      "    {\"foo\": \"foo\"},\n",
      "    stream_mode=\"updates\",\n",
      "    # Set subgraphs=True to stream outputs from subgraphs\n",
      "    subgraphs=True,  \n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Memorystore for Redis\n",
      "content===> from langchain_google_memorystore_redis import RedisVectorStore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trace selectively\n",
      "content===> import langsmith as ls\n",
      "\n",
      "# This WILL be traced\n",
      "with ls.tracing_context(enabled=True):\n",
      "    agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]})\n",
      "\n",
      "# This will NOT be traced (if LANGSMITH_TRACING is not set)\n",
      "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Content block reference\n",
      "content===> {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Hello world\",\n",
      "    \"annotations\": []\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Vector Search\n",
      "content===> from langchain_google_vertexai import VectorSearchVectorStoreGCS\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Memorystore for Redis\n",
      "content===> from langchain_google_memorystore_redis import MemorystoreDocumentLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tool emulator\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import LLMToolEmulator\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[get_weather, search_database, send_email],\n",
      "    middleware=[\n",
      "        # Emulate all tools by default\n",
      "        LLMToolEmulator(),\n",
      "\n",
      "        # Or emulate specific tools\n",
      "        # LLMToolEmulator(tools=[\"get_weather\", \"search_database\"]),\n",
      "\n",
      "        # Or use a custom model for emulation\n",
      "        # LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\"),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Specify a backend\n",
      "content===> FilesystemBackend(root_dir=\".\")\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke a graph from a node\n",
      "content===> from typing_extensions import TypedDict\n",
      "from langgraph.graph.state import StateGraph, START\n",
      "\n",
      "class SubgraphState(TypedDict):\n",
      "    bar: str\n",
      "\n",
      "# Subgraph\n",
      "\n",
      "def subgraph_node_1(state: SubgraphState):\n",
      "    return {\"bar\": \"hi! \" + state[\"bar\"]}\n",
      "\n",
      "subgraph_builder = StateGraph(SubgraphState)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Parent graph\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "def call_subgraph(state: State):\n",
      "    # Transform the state to the subgraph state\n",
      "    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  \n",
      "    # Transform response back to the parent state\n",
      "    return {\"foo\": subgraph_output[\"bar\"]}\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"node_1\", call_subgraph)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "graph = builder.compile()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> InMemoryStore (development)\n",
      "content===> from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "store = InMemoryStore()\n",
      "agent = create_deep_agent(store=store, use_longterm_memory=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Identify a checkpoint\n",
      "content===> # This is the state before last (states are listed in chronological order)\n",
      "selected_state = states[1]\n",
      "print(selected_state.next)\n",
      "print(selected_state.values)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Cloud\n",
      "content===> langchain-google-cloud-sql-pg\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ================================= Tool Message =================================\n",
      "Name: ToolStrategy\n",
      "\n",
      "There was an issue with the format. Try again.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ToolStrategy\n",
      "content===> from pydantic import BaseModel\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy\n",
      "\n",
      "\n",
      "class ContactInfo(BaseModel):\n",
      "    name: str\n",
      "    email: str\n",
      "    phone: str\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o-mini\",\n",
      "    tools=[search_tool],\n",
      "    response_format=ToolStrategy(ContactInfo)\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
      "})\n",
      "\n",
      "result[\"structured_response\"]\n",
      "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face Hub Tools\n",
      "content===> pip install transformers huggingface_hub\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI\n",
      "content===> from langchain_google_vertexai import ChatVertexAI\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke\n",
      "content===> from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
      "\n",
      "conversation = [\n",
      "    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n",
      "    HumanMessage(\"Translate: I love programming.\"),\n",
      "    AIMessage(\"J'adore la programmation.\"),\n",
      "    HumanMessage(\"Translate: I love building applications.\")\n",
      "]\n",
      "\n",
      "response = model.invoke(conversation)\n",
      "print(response)  # AIMessage(\"J'adore créer des applications.\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware3.before_model()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-cohere\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bring-your-own documents\n",
      "content===> langchain-elasticsearch\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Threads\n",
      "content===> {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> ls_info(path: str) -> list[FileInfo]\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Response Format\n",
      "content===> type[StructuredResponseT]\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> StoreBackend (LangGraph Store)\n",
      "content===> from deepagents.backends import StoreBackend\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    backend=(lambda rt: StoreBackend(rt))   # Note that the tools access Store through the runtime.store\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Token usage\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "\n",
      "response = model.invoke(\"Hello!\")\n",
      "response.usage_metadata\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-azure-ai pymongo\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> from typing import Any, TypedDict\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langchain.messages import AnyMessage\n",
      "from langchain_core.messages.utils import count_tokens_approximately\n",
      "from langgraph.graph import StateGraph, START, MessagesState\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langmem.short_term import SummarizationNode, RunningSummary  \n",
      "\n",
      "model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n",
      "summarization_model = model.bind(max_tokens=128)\n",
      "\n",
      "class State(MessagesState):\n",
      "    context: dict[str, RunningSummary]  \n",
      "\n",
      "class LLMInputState(TypedDict):  \n",
      "    summarized_messages: list[AnyMessage]\n",
      "    context: dict[str, RunningSummary]\n",
      "\n",
      "summarization_node = SummarizationNode(  \n",
      "    token_counter=count_tokens_approximately,\n",
      "    model=summarization_model,\n",
      "    max_tokens=256,\n",
      "    max_tokens_before_summary=256,\n",
      "    max_summary_tokens=128,\n",
      ")\n",
      "\n",
      "def call_model(state: LLMInputState):  \n",
      "    response = model.invoke(state[\"summarized_messages\"])\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(call_model)\n",
      "builder.add_node(\"summarize\", summarization_node)  \n",
      "builder.add_edge(START, \"summarize\")\n",
      "builder.add_edge(\"summarize\", \"call_model\")\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "# Invoke the graph\n",
      "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
      "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
      "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
      "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
      "\n",
      "final_response[\"messages\"][-1].pretty_print()\n",
      "print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Reasoning\n",
      "content===> for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
      "    reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n",
      "    print(reasoning_steps if reasoning_steps else chunk.text)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Wrap-style hooks\n",
      "content===> from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
      "from typing import Callable\n",
      "\n",
      "class RetryMiddleware(AgentMiddleware):\n",
      "    def __init__(self, max_retries: int = 3):\n",
      "        super().__init__()\n",
      "        self.max_retries = max_retries\n",
      "\n",
      "    def wrap_model_call(\n",
      "        self,\n",
      "        request: ModelRequest,\n",
      "        handler: Callable[[ModelRequest], ModelResponse],\n",
      "    ) -> ModelResponse:\n",
      "        for attempt in range(self.max_retries):\n",
      "            try:\n",
      "                return handler(request)\n",
      "            except Exception as e:\n",
      "                if attempt == self.max_retries - 1:\n",
      "                    raise\n",
      "                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prebuilt middleware\n",
      "content===> SummarizationMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured outputs\n",
      "content===> from pydantic import BaseModel, Field\n",
      "\n",
      "class Movie(BaseModel):\n",
      "    \"\"\"A movie with details.\"\"\"\n",
      "    title: str = Field(..., description=\"The title of the movie\")\n",
      "    year: int = Field(..., description=\"The year the movie was released\")\n",
      "    director: str = Field(..., description=\"The director of the movie\")\n",
      "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
      "\n",
      "model_with_structure = model.with_structured_output(Movie)\n",
      "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
      "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom tool description\n",
      "content===> @tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\n",
      "def calc(expression: str) -> str:\n",
      "    \"\"\"Evaluate mathematical expressions.\"\"\"\n",
      "    return str(eval(expression))\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_core.vectorstores import InMemoryVectorStore\n",
      "\n",
      "vector_store = InMemoryVectorStore(embeddings)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cosmos DB for Apache Gremlin\n",
      "content===> from langchain_community.graphs import GremlinGraph\n",
      "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Jobs\n",
      "content===> pip install google-search-results langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Mocking Chat Model\n",
      "content===> from langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n",
      "\n",
      "model = GenericFakeChatModel(messages=iter([\n",
      "    AIMessage(content=\"\", tool_calls=[ToolCall(name=\"foo\", args={\"bar\": \"baz\"}, id=\"call_1\")]),\n",
      "    \"bar\"\n",
      "]))\n",
      "\n",
      "model.invoke(\"hello\")\n",
      "# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Caching\n",
      "content===> CacheBackedEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Approve or reject\n",
      "content===> from typing import Literal\n",
      "from langgraph.types import interrupt, Command\n",
      "\n",
      "def approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\n",
      "    # Pause execution; payload shows up under result[\"__interrupt__\"]\n",
      "    is_approved = interrupt({\n",
      "        \"question\": \"Do you want to proceed with this action?\",\n",
      "        \"details\": state[\"action_details\"]\n",
      "    })\n",
      "\n",
      "    # Route based on the response\n",
      "    if is_approved:\n",
      "        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\n",
      "    else:\n",
      "        return Command(goto=\"cancel\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ================================= Tool Message =================================\n",
      "Name: ProductRating\n",
      "\n",
      "Please provide a valid rating between 1-5 and include a comment.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Model Garden\n",
      "content===> from langchain_google_vertexai import VertexAIModelGarden\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Transcripts Loader\n",
      "content===> pip install youtube-transcript-api langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Search\n",
      "content===> from langchain_community.tools import GoogleSearchRun, GoogleSearchResults\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Office 365 individual tools\n",
      "content===> O365CreateDraftMessage\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Speech-to-Text\n",
      "content===> from langchain_google_community import SpeechToTextLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> \"sync\"\n",
      "content===> graph.stream(\n",
      "    {\"input\": \"test\"},\n",
      "    durability=\"sync\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> first_model.invoke(\n",
      "    \"what's your name\",\n",
      "    config={\n",
      "        \"configurable\": {\n",
      "            \"first_model\": \"claude-sonnet-4-5-20250929\",\n",
      "            \"first_temperature\": 0.5,\n",
      "            \"first_max_tokens\": 100,\n",
      "        }\n",
      "    },\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Initialize a model\n",
      "content===> pip install -U \"langchain[openai]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Subagent not being called\n",
      "content===> # ✅ Good\n",
      "{\"name\": \"research-specialist\", \"description\": \"Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches.\"}\n",
      "\n",
      "# ❌ Bad\n",
      "{\"name\": \"helper\", \"description\": \"helps with stuff\"}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Provider strategy\n",
      "content===> from pydantic import BaseModel\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "class ContactInfo(BaseModel):\n",
      "    \"\"\"Contact information for a person.\"\"\"\n",
      "    name: str = Field(description=\"The name of the person\")\n",
      "    email: str = Field(description=\"The email address of the person\")\n",
      "    phone: str = Field(description=\"The phone number of the person\")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5\",\n",
      "    tools=tools,\n",
      "    response_format=ContactInfo  # Auto-selects ProviderStrategy\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
      "})\n",
      "\n",
      "result[\"structured_response\"]\n",
      "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prompt\n",
      "content===> from langchain.agents import create_agent\n",
      "from typing import TypedDict\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
      "\n",
      "\n",
      "class CustomContext(TypedDict):\n",
      "    user_name: str\n",
      "\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get the weather in a city.\"\"\"\n",
      "    return f\"The weather in {city} is always sunny!\"\n",
      "\n",
      "\n",
      "@dynamic_prompt\n",
      "def dynamic_system_prompt(request: ModelRequest) -> str:\n",
      "    user_name = request.runtime.context[\"user_name\"]\n",
      "    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
      "    return system_prompt\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[get_weather],\n",
      "    middleware=[dynamic_system_prompt],\n",
      "    context_schema=CustomContext,\n",
      ")\n",
      "\n",
      "result = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
      "    context=CustomContext(user_name=\"John Smith\"),\n",
      ")\n",
      "for msg in result[\"messages\"]:\n",
      "    msg.pretty_print()\n",
      "\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from qdrant_client.models import Distance, VectorParams\n",
      "from langchain_qdrant import QdrantVectorStore\n",
      "from qdrant_client import QdrantClient\n",
      "\n",
      "client = QdrantClient(\":memory:\")\n",
      "\n",
      "vector_size = len(embeddings.embed_query(\"sample text\"))\n",
      "\n",
      "if not client.collection_exists(\"test\"):\n",
      "    client.create_collection(\n",
      "        collection_name=\"test\",\n",
      "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
      "    )\n",
      "vector_store = QdrantVectorStore(\n",
      "    client=client,\n",
      "    collection_name=\"test\",\n",
      "    embedding=embeddings,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image captioning\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI callback handler\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Filter by node\n",
      "content===> stream_mode=\"messages\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trace selectively\n",
      "content===> import langsmith as ls\n",
      "\n",
      "# This WILL be traced\n",
      "with ls.tracing_context(enabled=True):\n",
      "    agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]})\n",
      "\n",
      "# This will NOT be traced (if LANGSMITH_TRACING is not set)\n",
      "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face model loader\n",
      "content===> from langchain_community.document_loaders import HuggingFaceModelLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LangSmith Integration\n",
      "content===> import pytest\n",
      "from langsmith import testing as t\n",
      "from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n",
      "\n",
      "trajectory_evaluator = create_trajectory_llm_as_judge(\n",
      "    model=\"openai:o3-mini\",\n",
      "    prompt=TRAJECTORY_ACCURACY_PROMPT,\n",
      ")\n",
      "\n",
      "@pytest.mark.langsmith\n",
      "def test_trajectory_accuracy():\n",
      "    result = agent.invoke({\n",
      "        \"messages\": [HumanMessage(content=\"What's the weather in SF?\")]\n",
      "    })\n",
      "\n",
      "    reference_trajectory = [\n",
      "        HumanMessage(content=\"What's the weather in SF?\"),\n",
      "        AIMessage(content=\"\", tool_calls=[\n",
      "            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n",
      "        ]),\n",
      "        ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_1\"),\n",
      "        AIMessage(content=\"The weather in SF is 75 degrees and sunny.\"),\n",
      "    ]\n",
      "\n",
      "    # Log inputs, outputs, and reference outputs to LangSmith\n",
      "    t.log_inputs({})\n",
      "    t.log_outputs({\"messages\": result[\"messages\"]})\n",
      "    t.log_reference_outputs({\"messages\": reference_trajectory})\n",
      "\n",
      "    trajectory_evaluator(\n",
      "        outputs=result[\"messages\"],\n",
      "        reference_outputs=reference_trajectory\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Combine multiple guardrails\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[search_tool, send_email_tool],\n",
      "    middleware=[\n",
      "        # Layer 1: Deterministic input filter (before agent)\n",
      "        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\n",
      "\n",
      "        # Layer 2: PII protection (before and after model)\n",
      "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
      "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n",
      "\n",
      "        # Layer 3: Human approval for sensitive tools\n",
      "        HumanInTheLoopMiddleware(interrupt_on={\"send_email\": True}),\n",
      "\n",
      "        # Layer 4: Model-based safety check (after agent)\n",
      "        SafetyGuardrailMiddleware(),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Vector Search\n",
      "content===> from langchain_google_vertexai import VectorSearchVectorStore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AI Message\n",
      "content===> from langchain.messages import AIMessage, SystemMessage, HumanMessage\n",
      "\n",
      "# Create an AI message manually (e.g., for conversation history)\n",
      "ai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n",
      "\n",
      "# Add to conversation history\n",
      "messages = [\n",
      "    SystemMessage(\"You are a helpful assistant\"),\n",
      "    HumanMessage(\"Can you help me?\"),\n",
      "    ai_msg,  # Insert as if it came from the model\n",
      "    HumanMessage(\"Great! What's 2+2?\")\n",
      "]\n",
      "\n",
      "response = model.invoke(messages)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AWS Glue\n",
      "content===> from langchain_community.document_loaders.glue_catalog import GlueCatalogLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> create_agent\n",
      "content===> langgraph.prebuilt.create_react_agent\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> EditResult(error, path, files_update, occurrences)\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining tools\n",
      "content===> from langchain.tools import tool\n",
      "\n",
      "@tool(parse_docstring=True)\n",
      "def search_orders(\n",
      "    user_id: str,\n",
      "    status: str,\n",
      "    limit: int = 10\n",
      ") -> str:\n",
      "    \"\"\"Search for user orders by status.\n",
      "\n",
      "    Use this when the user asks about order history or wants to check\n",
      "    order status. Always filter by the provided status.\n",
      "\n",
      "    Args:\n",
      "        user_id: Unique identifier for the user\n",
      "        status: Order status: 'pending', 'shipped', or 'delivered'\n",
      "        limit: Maximum number of results to return\n",
      "    \"\"\"\n",
      "    # Implementation here\n",
      "    pass\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware3.after_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
      "  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n",
      "\n",
      "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
      "\n",
      "embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Store is required\n",
      "content===> # ❌ This will error\n",
      "agent = create_deep_agent(use_longterm_memory=True)  # Missing store!\n",
      "\n",
      "# ✅ Correct\n",
      "agent = create_deep_agent(\n",
      "    use_longterm_memory=True,\n",
      "    store=InMemoryStore()\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_google_vertexai import VertexAIEmbeddings\n",
      "\n",
      "embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. View your agent in Studio\n",
      "content===> https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure OpenAI\n",
      "content===> import os\n",
      "\n",
      "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://<your-endpoint.openai.azure.com/\"\n",
      "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Node-style hooks\n",
      "content===> from langchain.agents.middleware import AgentMiddleware, AgentState\n",
      "from langgraph.runtime import Runtime\n",
      "from typing import Any\n",
      "\n",
      "class LoggingMiddleware(AgentMiddleware):\n",
      "    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "        print(f\"About to call model with {len(state['messages'])} messages\")\n",
      "        return None\n",
      "\n",
      "    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "        print(f\"Model returned: {state['messages'][-1].content}\")\n",
      "        return None\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Neptune\n",
      "content===> pip install langchain-aws\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install LangChain\n",
      "content===> # Installing the OpenAI integration\n",
      "pip install -U langchain-openai\n",
      "\n",
      "# Installing the Anthropic integration\n",
      "pip install -U langchain-anthropic\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/install\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-core\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> ================================ Human Message =================================\n",
      "\n",
      "Find the most popular wireless headphones right now and check if they're in stock\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> High-level API\n",
      "content===> {'__start__': <langgraph.pregel.read.PregelNode at 0x7d05e3ba1810>,\n",
      " 'write_essay': <langgraph.pregel.read.PregelNode at 0x7d05e3ba14d0>,\n",
      " 'score_essay': <langgraph.pregel.read.PregelNode at 0x7d05e3ba1710>}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> StructuredOutputValidationError\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> User preferences\n",
      "content===> agent = create_deep_agent(\n",
      "    store=store,\n",
      "    use_longterm_memory=True,\n",
      "    system_prompt=\"\"\"When users tell you their preferences, save them to\n",
      "    /memories/user_preferences.txt so you remember them in future conversations.\"\"\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Server-side tool use\n",
      "content===> [\n",
      "    {\n",
      "        \"type\": \"server_tool_call\",\n",
      "        \"name\": \"web_search\",\n",
      "        \"args\": {\n",
      "            \"query\": \"positive news stories today\",\n",
      "            \"type\": \"search\"\n",
      "        },\n",
      "        \"id\": \"ws_abc123\"\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"server_tool_result\",\n",
      "        \"tool_call_id\": \"ws_abc123\",\n",
      "        \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Here are some positive news stories from today...\",\n",
      "        \"annotations\": [\n",
      "            {\n",
      "                \"end_index\": 410,\n",
      "                \"start_index\": 337,\n",
      "                \"title\": \"article title\",\n",
      "                \"type\": \"citation\",\n",
      "                \"url\": \"...\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 5: Wire it together\n",
      "content===> from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.types import RetryPolicy\n",
      "\n",
      "# Create the graph\n",
      "workflow = StateGraph(EmailAgentState)\n",
      "\n",
      "# Add nodes with appropriate error handling\n",
      "workflow.add_node(\"read_email\", read_email)\n",
      "workflow.add_node(\"classify_intent\", classify_intent)\n",
      "\n",
      "# Add retry policy for nodes that might have transient failures\n",
      "workflow.add_node(\n",
      "    \"search_documentation\",\n",
      "    search_documentation,\n",
      "    retry_policy=RetryPolicy(max_attempts=3)\n",
      ")\n",
      "workflow.add_node(\"bug_tracking\", bug_tracking)\n",
      "workflow.add_node(\"draft_response\", draft_response)\n",
      "workflow.add_node(\"human_review\", human_review)\n",
      "workflow.add_node(\"send_reply\", send_reply)\n",
      "\n",
      "# Add only the essential edges\n",
      "workflow.add_edge(START, \"read_email\")\n",
      "workflow.add_edge(\"read_email\", \"classify_intent\")\n",
      "workflow.add_edge(\"send_reply\", END)\n",
      "\n",
      "# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\n",
      "memory = MemorySaver()\n",
      "app = workflow.compile(checkpointer=memory)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_huggingface import HuggingFaceEmbeddings\n",
      "\n",
      "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ScaNN (Local Index)\n",
      "content===> from langchain_community.vectorstores import ScaNN\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> create_react_agent→create_agent\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(  \n",
      "    model,\n",
      "    tools,\n",
      "    system_prompt=\"You are a helpful assistant.\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Filter by LLM invocation\n",
      "content===> from typing import TypedDict\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langgraph.graph import START, StateGraph\n",
      "\n",
      "# The joke_model is tagged with \"joke\"\n",
      "joke_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"joke\"])\n",
      "# The poem_model is tagged with \"poem\"\n",
      "poem_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"poem\"])\n",
      "\n",
      "\n",
      "class State(TypedDict):\n",
      "      topic: str\n",
      "      joke: str\n",
      "      poem: str\n",
      "\n",
      "\n",
      "async def call_model(state, config):\n",
      "      topic = state[\"topic\"]\n",
      "      print(\"Writing joke...\")\n",
      "      # Note: Passing the config through explicitly is required for python < 3.11\n",
      "      # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n",
      "      # The config is passed through explicitly to ensure the context vars are propagated correctly\n",
      "      # This is required for Python < 3.11 when using async code. Please see the async section for more details\n",
      "      joke_response = await joke_model.ainvoke(\n",
      "            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n",
      "            config,\n",
      "      )\n",
      "      print(\"\\n\\nWriting poem...\")\n",
      "      poem_response = await poem_model.ainvoke(\n",
      "            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n",
      "            config,\n",
      "      )\n",
      "      return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n",
      "\n",
      "\n",
      "graph = (\n",
      "      StateGraph(State)\n",
      "      .add_node(call_model)\n",
      "      .add_edge(START, \"call_model\")\n",
      "      .compile()\n",
      ")\n",
      "\n",
      "# The stream_mode is set to \"messages\" to stream LLM tokens\n",
      "# The metadata contains information about the LLM invocation, including the tags\n",
      "async for msg, metadata in graph.astream(\n",
      "      {\"topic\": \"cats\"},\n",
      "      stream_mode=\"messages\",\n",
      "):\n",
      "    if metadata[\"tags\"] == [\"joke\"]:\n",
      "        print(msg.content, end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using CompiledSubAgent\n",
      "content===> from deepagents import create_deep_agent, CompiledSubAgent\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "# Create a custom agent graph\n",
      "custom_graph = create_agent(\n",
      "    model=your_model,\n",
      "    tools=specialized_tools,\n",
      "    prompt=\"You are a specialized agent for data analysis...\"\n",
      ")\n",
      "\n",
      "# Use it as a custom subagent\n",
      "custom_subagent = CompiledSubAgent(\n",
      "    name=\"data-analyzer\",\n",
      "    description=\"Specialized agent for complex data analysis tasks\",\n",
      "    runnable=custom_graph\n",
      ")\n",
      "\n",
      "subagents = [custom_subagent]\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[internet_search],\n",
      "    system_prompt=research_instructions,\n",
      "    subagents=subagents\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Validating human input\n",
      "content===> import sqlite3\n",
      "from typing import TypedDict\n",
      "\n",
      "from langgraph.checkpoint.sqlite import SqliteSaver\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.types import Command, interrupt\n",
      "\n",
      "\n",
      "class FormState(TypedDict):\n",
      "    age: int | None\n",
      "\n",
      "\n",
      "def get_age_node(state: FormState):\n",
      "    prompt = \"What is your age?\"\n",
      "\n",
      "    while True:\n",
      "        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n",
      "\n",
      "        if isinstance(answer, int) and answer > 0:\n",
      "            return {\"age\": answer}\n",
      "\n",
      "        prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n",
      "\n",
      "\n",
      "builder = StateGraph(FormState)\n",
      "builder.add_node(\"collect_age\", get_age_node)\n",
      "builder.add_edge(START, \"collect_age\")\n",
      "builder.add_edge(\"collect_age\", END)\n",
      "\n",
      "checkpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "config = {\"configurable\": {\"thread_id\": \"form-1\"}}\n",
      "first = graph.invoke({\"age\": None}, config=config)\n",
      "print(first[\"__interrupt__\"])  # -> [Interrupt(value='What is your age?', ...)]\n",
      "\n",
      "# Provide invalid data; the node re-prompts\n",
      "retry = graph.invoke(Command(resume=\"thirty\"), config=config)\n",
      "print(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n",
      "\n",
      "# Provide valid data; loop exits and state updates\n",
      "final = graph.invoke(Command(resume=30), config=config)\n",
      "print(final[\"age\"])  # -> 30\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Memory storage\n",
      "content===> from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "\n",
      "def embed(texts: list[str]) -> list[list[float]]:\n",
      "    # Replace with an actual embedding function or LangChain embeddings object\n",
      "    return [[1.0, 2.0] * len(texts)]\n",
      "\n",
      "\n",
      "# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\n",
      "store = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) \n",
      "user_id = \"my-user\"\n",
      "application_context = \"chitchat\"\n",
      "namespace = (user_id, application_context) \n",
      "store.put( \n",
      "    namespace,\n",
      "    \"a-memory\",\n",
      "    {\n",
      "        \"rules\": [\n",
      "            \"User likes short, direct language\",\n",
      "            \"User only speaks English & python\",\n",
      "        ],\n",
      "        \"my-key\": \"my-value\",\n",
      "    },\n",
      ")\n",
      "# get the \"memory\" by ID\n",
      "item = store.get(namespace, \"a-memory\") \n",
      "# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\n",
      "items = store.search( \n",
      "    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_astradb import AstraDBVectorStore\n",
      "\n",
      "vector_store = AstraDBVectorStore(\n",
      "    embedding=embeddings,\n",
      "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
      "    collection_name=\"astra_vector_langchain\",\n",
      "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
      "    namespace=ASTRA_DB_NAMESPACE,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> Azure Cognitive Services\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Encryption\n",
      "content===> from langgraph.checkpoint.serde.encrypted import EncryptedSerializer\n",
      "from langgraph.checkpoint.postgres import PostgresSaver\n",
      "\n",
      "serde = EncryptedSerializer.from_pycryptodome_aes()\n",
      "checkpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\n",
      "checkpointer.setup()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Updated return type for chat models\n",
      "content===> def bind_tools(\n",
      "        ...\n",
      "    ) -> Runnable[LanguageModelInput, AIMessage]:\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. View your agent in Studio\n",
      "content===> http://127.0.0.1:2024\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Container Apps dynamic sessions\n",
      "content===> pip install langchain-azure-dynamic-sessions\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Messaging Services\n",
      "content===> TelegramChatFileLoader\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> System Prompt\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
      "\n",
      "@dynamic_prompt\n",
      "def state_aware_prompt(request: ModelRequest) -> str:\n",
      "    # request.messages is a shortcut for request.state[\"messages\"]\n",
      "    message_count = len(request.messages)\n",
      "\n",
      "    base = \"You are a helpful assistant.\"\n",
      "\n",
      "    if message_count > 10:\n",
      "        base += \"\\nThis is a long conversation - be extra concise.\"\n",
      "\n",
      "    return base\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[state_aware_prompt]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Default message format for OpenAI Responses API\n",
      "content===> # Enforce previous behavior with output_version flag\n",
      "model = ChatOpenAI(model=\"gpt-4o-mini\", output_version=\"v0\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceEndpoint\n",
      "content===> from langchain_huggingface import HuggingFaceEndpoint\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-huggingface\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Context editing\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[\n",
      "        ContextEditingMiddleware(\n",
      "            edits=[\n",
      "                ClearToolUsesEdit(trigger=1000),  # Clear old tool uses\n",
      "            ],\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> langchain-classic\n",
      "content===> from langchain import ...\n",
      "from langchain_classic import ...\n",
      "\n",
      "from langchain.chains import ...\n",
      "from langchain_classic.chains import ...\n",
      "\n",
      "from langchain.retrievers import ...\n",
      "from langchain_classic.retrievers import ...\n",
      "\n",
      "from langchain import hub  \n",
      "from langchain_classic import hub  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Audio Loader\n",
      "content===> from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
      "# Often used with whisper parsers:\n",
      "# from langchain_community.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Excel\n",
      "content===> UnstructuredExcelLoader\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Human-in-the-loop\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langgraph.types import Command\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[search_tool, send_email_tool, delete_database_tool],\n",
      "    middleware=[\n",
      "        HumanInTheLoopMiddleware(\n",
      "            interrupt_on={\n",
      "                # Require approval for sensitive operations\n",
      "                \"send_email\": True,\n",
      "                \"delete_database\": True,\n",
      "                # Auto-approve safe operations\n",
      "                \"search\": False,\n",
      "            }\n",
      "        ),\n",
      "    ],\n",
      "    # Persist the state across interrupts\n",
      "    checkpointer=InMemorySaver(),\n",
      ")\n",
      "\n",
      "# Human-in-the-loop requires a thread ID for persistence\n",
      "config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
      "\n",
      "# Agent will pause and wait for approval before executing sensitive tools\n",
      "result = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n",
      "    config=config\n",
      ")\n",
      "\n",
      "result = agent.invoke(\n",
      "    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
      "    config=config  # Same thread ID to resume the paused conversation\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Neptune with Cypher\n",
      "content===> from langchain_aws.graphs import NeptuneGraph\n",
      "from langchain_aws.graphs import NeptuneAnalyticsGraph\n",
      "from langchain_aws.chains import create_neptune_opencypher_qa_chain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft OneNote\n",
      "content===> from langchain_community.document_loaders.onenote import OneNoteLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 1. Install the LangGraph CLI\n",
      "content===> # Python >= 3.11 is required.\n",
      "pip install --upgrade \"langgraph-cli[inmem]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Featured providers\n",
      "content===> langchain-huggingface\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Selecting tools\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
      "from typing import Callable\n",
      "\n",
      "@wrap_model_call\n",
      "def state_based_tools(\n",
      "    request: ModelRequest,\n",
      "    handler: Callable[[ModelRequest], ModelResponse]\n",
      ") -> ModelResponse:\n",
      "    \"\"\"Filter tools based on conversation State.\"\"\"\n",
      "    # Read from State: check if user has authenticated\n",
      "    state = request.state  \n",
      "    is_authenticated = state.get(\"authenticated\", False)  \n",
      "    message_count = len(state[\"messages\"])\n",
      "\n",
      "    # Only enable sensitive tools after authentication\n",
      "    if not is_authenticated:\n",
      "        tools = [t for t in request.tools if t.name.startswith(\"public_\")]\n",
      "        request = request.override(tools=tools)  \n",
      "    elif message_count < 5:\n",
      "        # Limit tools early in conversation\n",
      "        tools = [t for t in request.tools if t.name != \"advanced_search\"]\n",
      "        request = request.override(tools=tools)  \n",
      "\n",
      "    return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[public_search, private_search, advanced_search],\n",
      "    middleware=[state_based_tools]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Decision types\n",
      "content===> interrupt_on = {\n",
      "    # Sensitive operations: allow all options\n",
      "    \"delete_file\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n",
      "\n",
      "    # Moderate risk: approval or rejection only\n",
      "    \"write_file\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
      "\n",
      "    # Must approve (no rejection allowed)\n",
      "    \"critical_operation\": {\"allowed_decisions\": [\"approve\"]},\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> View subgraph state\n",
      "content===> from langgraph.graph import START, StateGraph\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.types import interrupt, Command\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "# Subgraph\n",
      "\n",
      "def subgraph_node_1(state: State):\n",
      "    value = interrupt(\"Provide value:\")\n",
      "    return {\"foo\": state[\"foo\"] + value}\n",
      "\n",
      "subgraph_builder = StateGraph(State)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Parent graph\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"node_1\", subgraph)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "\n",
      "checkpointer = MemorySaver()\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "graph.invoke({\"foo\": \"\"}, config)\n",
      "parent_state = graph.get_state(config)\n",
      "\n",
      "# This will be available only when the subgraph is interrupted.\n",
      "# Once you resume the graph, you won't be able to access the subgraph state.\n",
      "subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state\n",
      "\n",
      "# resume the subgraph\n",
      "graph.invoke(Command(resume=\"bar\"), config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma local from Hugging Face\n",
      "content===> from langchain_google_vertexai.gemma import GemmaChatLocalHF\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> SummarizationMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Keep state raw, format prompts on-demand\n",
      "content===> from typing import TypedDict, Literal\n",
      "\n",
      "# Define the structure for email classification\n",
      "class EmailClassification(TypedDict):\n",
      "    intent: Literal[\"question\", \"bug\", \"billing\", \"feature\", \"complex\"]\n",
      "    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n",
      "    topic: str\n",
      "    summary: str\n",
      "\n",
      "class EmailAgentState(TypedDict):\n",
      "    # Raw email data\n",
      "    email_content: str\n",
      "    sender_email: str\n",
      "    email_id: str\n",
      "\n",
      "    # Classification result\n",
      "    classification: EmailClassification | None\n",
      "\n",
      "    # Raw search/API results\n",
      "    search_results: list[str] | None  # List of raw document chunks\n",
      "    customer_history: dict | None  # Raw customer data from CRM\n",
      "\n",
      "    # Generated content\n",
      "    draft_response: str | None\n",
      "    messages: list[str] | None\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> pip install -U langgraph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU \"langchain[langchain-perplexity]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configuring interrupts\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import HumanInTheLoopMiddleware \n",
      "from langgraph.checkpoint.memory import InMemorySaver \n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n",
      "    middleware=[\n",
      "        HumanInTheLoopMiddleware( \n",
      "            interrupt_on={\n",
      "                \"write_file\": True,  # All decisions (approve, edit, reject) allowed\n",
      "                \"execute_sql\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing allowed\n",
      "                # Safe operation, no approval needed\n",
      "                \"read_data\": False,\n",
      "            },\n",
      "            # Prefix for interrupt messages - combined with tool name and args to form the full message\n",
      "            # e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n",
      "            # Individual tools can override this by specifying a \"description\" in their interrupt config\n",
      "            description_prefix=\"Tool execution pending approval\",\n",
      "        ),\n",
      "    ],\n",
      "    # Human-in-the-loop requires checkpointing to handle interrupts.\n",
      "    # In production, use a persistent checkpointer like AsyncPostgresSaver.\n",
      "    checkpointer=InMemorySaver(),  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> VertexStringEvaluator\n",
      "content===> # Note: Original doc listed VertexPairWiseStringEvaluator twice. Assuming this class exists.\n",
      "from langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator # Verify class name if needed\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Server-side tool use\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-4.1-mini\")\n",
      "\n",
      "tool = {\"type\": \"web_search\"}\n",
      "model_with_tools = model.bind_tools([tool])\n",
      "\n",
      "response = model_with_tools.invoke(\"What was a positive news story from today?\")\n",
      "response.content_blocks\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bedrock\n",
      "content===> from langchain_aws import BedrockEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Import path\n",
      "content===> from langgraph.prebuilt import create_react_agent \n",
      "from langchain.agents import create_agent \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Local development\n",
      "content===> # Create a new Agent Chat UI project\n",
      "npx create-agent-chat-app --project-name my-chat-ui\n",
      "cd my-chat-ui\n",
      "\n",
      "# Install dependencies and start\n",
      "pnpm install\n",
      "pnpm dev\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Spanner\n",
      "content===> from langchain_google_spanner import SpannerLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Development environment\n",
      "content===> cd libs/core\n",
      "uv sync --all-groups\n",
      "make test  # Ensure tests pass before starting development\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 1. Install the LangGraph CLI\n",
      "content===> # Python >= 3.11 is required.\n",
      "pip install --upgrade \"langgraph-cli[inmem]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-milvus\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Long-term memory\n",
      "content===> use_longterm_memory=True\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content blocks\n",
      "content===> langchain-google-genai\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Writes\n",
      "content===> from langchain.tools import tool, ToolRuntime\n",
      "from langchain.agents import create_agent\n",
      "from langgraph.types import Command\n",
      "\n",
      "@tool\n",
      "def authenticate_user(\n",
      "    password: str,\n",
      "    runtime: ToolRuntime\n",
      ") -> Command:\n",
      "    \"\"\"Authenticate user and update State.\"\"\"\n",
      "    # Perform authentication (simplified)\n",
      "    if password == \"correct\":\n",
      "        # Write to State: mark as authenticated using Command\n",
      "        return Command(\n",
      "            update={\"authenticated\": True},\n",
      "        )\n",
      "    else:\n",
      "        return Command(update={\"authenticated\": False})\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[authenticate_user]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Metadata filtering\n",
      "content===> vector_store.similarity_search(\n",
      "  \"query\",\n",
      "  k=3,\n",
      "  filter={\"source\": \"tweets\"}\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Research projects\n",
      "content===> research_agent = create_deep_agent(\n",
      "    store=store,\n",
      "    use_longterm_memory=True,\n",
      "    system_prompt=\"\"\"You are a research assistant.\n",
      "\n",
      "    Save your research progress to /memories/research/:\n",
      "    - /memories/research/sources.txt - List of sources found\n",
      "    - /memories/research/notes.txt - Key findings and notes\n",
      "    - /memories/research/report.md - Final report draft\n",
      "\n",
      "    This allows research to continue across multiple sessions.\"\"\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a real-world agent\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\n",
      "    \"claude-sonnet-4-5-20250929\",\n",
      "    temperature=0.5,\n",
      "    timeout=10,\n",
      "    max_tokens=1000\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Review and edit state\n",
      "content===> graph.invoke(\n",
      "    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\n",
      "    config=config\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Before model\n",
      "content===> from langchain.messages import RemoveMessage\n",
      "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langchain.agents import create_agent, AgentState\n",
      "from langchain.agents.middleware import before_model\n",
      "from langgraph.runtime import Runtime\n",
      "from typing import Any\n",
      "\n",
      "\n",
      "@before_model\n",
      "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
      "    messages = state[\"messages\"]\n",
      "\n",
      "    if len(messages) <= 3:\n",
      "        return None  # No changes needed\n",
      "\n",
      "    first_msg = messages[0]\n",
      "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
      "    new_messages = [first_msg] + recent_messages\n",
      "\n",
      "    return {\n",
      "        \"messages\": [\n",
      "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
      "            *new_messages\n",
      "        ]\n",
      "    }\n",
      "\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=tools,\n",
      "    middleware=[trim_messages]\n",
      ")\n",
      "\n",
      "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
      "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
      "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
      "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
      "\n",
      "final_response[\"messages\"][-1].pretty_print()\n",
      "\"\"\"\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "Your name is Bob. You told me that earlier.\n",
      "If you'd like me to call you a nickname or use a different name, just say the word.\n",
      "\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> git checkout -b your-username/short-bugfix-name\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for PostgreSQL\n",
      "content===> pip install langchain-google-cloud-sql-pg\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> langchain-classic\n",
      "content===> # Chains\n",
      "from langchain_classic.chains import LLMChain\n",
      "\n",
      "# Retrievers\n",
      "from langchain_classic.retrievers import ...\n",
      "\n",
      "# Indexing\n",
      "from langchain_classic.indexes import ...\n",
      "\n",
      "# Hub\n",
      "from langchain_classic import hub\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Always use a checkpointer\n",
      "content===> from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "checkpointer = MemorySaver()\n",
      "agent = create_deep_agent(\n",
      "    tools=[...],\n",
      "    interrupt_on={...},\n",
      "    checkpointer=checkpointer  # Required for HITL\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face dataset\n",
      "content===> from langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Generative AI (Gemini API & AI Studio)\n",
      "content===> export GOOGLE_API_KEY=\"YOUR_API_KEY\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> [('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n",
      "[('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> mget(key: Sequence[str]) -> List[Optional[bytes]]\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multimodal\n",
      "content===> # From URL\n",
      "message = {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
      "        {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n",
      "    ]\n",
      "}\n",
      "\n",
      "# From base64 data\n",
      "message = {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
      "        {\n",
      "            \"type\": \"image\",\n",
      "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
      "            \"mime_type\": \"image/jpeg\",\n",
      "        },\n",
      "    ]\n",
      "}\n",
      "\n",
      "# From provider-managed File ID\n",
      "message = {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
      "        {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n",
      "    ]\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bring-your-own documents\n",
      "content===> AzureAISearchRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bedrock\n",
      "content===> from langchain_aws import BedrockLLM\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a real-world agent\n",
      "content===> from dataclasses import dataclass\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "@tool\n",
      "def get_weather_for_location(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    \"\"\"Custom runtime context schema.\"\"\"\n",
      "    user_id: str\n",
      "\n",
      "@tool\n",
      "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
      "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
      "    user_id = runtime.context.user_id\n",
      "    return \"Florida\" if user_id == \"1\" else \"SF\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Testing requirements\n",
      "content===> tests/integration_tests/\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Human-in-the-loop\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[read_email_tool, send_email_tool],\n",
      "    checkpointer=InMemorySaver(),\n",
      "    middleware=[\n",
      "        HumanInTheLoopMiddleware(\n",
      "            interrupt_on={\n",
      "                # Require approval, editing, or rejection for sending emails\n",
      "                \"send_email_tool\": {\n",
      "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
      "                },\n",
      "                # Auto-approve reading emails\n",
      "                \"read_email_tool\": False,\n",
      "            }\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> # Invoke the graph\n",
      "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
      "\n",
      "# Let's say hi again\n",
      "for update in graph.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n",
      "):\n",
      "    print(update)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 3. Environment variables\n",
      "content===> LANGSMITH_API_KEY=lsv2...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> View the history of the thread\n",
      "content===> config = {\n",
      "    \"configurable\": {\n",
      "        \"thread_id\": \"1\"\n",
      "    }\n",
      "}\n",
      "list(graph.get_state_history(config))  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Route to different backends\n",
      "content===> from deepagents import create_deep_agent\n",
      "from deepagents.backends import FilesystemBackend\n",
      "from deepagents.backends.composite import build_composite_state_backend\n",
      "\n",
      "composite_backend = lambda rt: CompositeBackend(\n",
      "    routes={\n",
      "        \"/memories/\": FilesystemBackend(root_dir=\"/deepagents/myagent\"),\n",
      "    },\n",
      ")\n",
      "\n",
      "agent = create_deep_agent(backend=composite_backend)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> from langchain.messages import RemoveMessage  \n",
      "\n",
      "def delete_messages(state):\n",
      "    messages = state[\"messages\"]\n",
      "    if len(messages) > 2:\n",
      "        # remove the earliest two messages\n",
      "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n",
      "\n",
      "def call_model(state: MessagesState):\n",
      "    response = model.invoke(state[\"messages\"])\n",
      "    return {\"messages\": response}\n",
      "\n",
      "builder = StateGraph(MessagesState)\n",
      "builder.add_sequence([call_model, delete_messages])\n",
      "builder.add_edge(START, \"call_model\")\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "app = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "for event in app.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
      "    config,\n",
      "    stream_mode=\"values\"\n",
      "):\n",
      "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
      "\n",
      "for event in app.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
      "    config,\n",
      "    stream_mode=\"values\"\n",
      "):\n",
      "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> State type restrictions\n",
      "content===> langchain.agents.AgentState\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Similarity metrics\n",
      "content===> import numpy as np\n",
      "\n",
      "def cosine_similarity(vec1, vec2):\n",
      "    dot = np.dot(vec1, vec2)\n",
      "    return dot / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
      "\n",
      "similarity = cosine_similarity(query_embedding, document_embedding)\n",
      "print(\"Cosine Similarity:\", similarity)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Orchestrator-worker\n",
      "content===> from typing import Annotated, List\n",
      "import operator\n",
      "\n",
      "\n",
      "# Schema for structured output to use in planning\n",
      "class Section(BaseModel):\n",
      "    name: str = Field(\n",
      "        description=\"Name for this section of the report.\",\n",
      "    )\n",
      "    description: str = Field(\n",
      "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Sections(BaseModel):\n",
      "    sections: List[Section] = Field(\n",
      "        description=\"Sections of the report.\",\n",
      "    )\n",
      "\n",
      "\n",
      "# Augment the LLM with schema for structured output\n",
      "planner = llm.with_structured_output(Sections)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Replay\n",
      "content===> config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\n",
      "graph.invoke(None, config=config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> from deepagents import create_deep_agent\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "store = InMemoryStore()  # Or any other Store object\n",
      "agent = create_deep_agent(\n",
      "    store=store,\n",
      "    use_longterm_memory=True\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Connect to your agent\n",
      "content===> http://localhost:2024\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining state viastate_schema\n",
      "content===> from langchain.tools import tool, ToolRuntime\n",
      "from langchain.agents import create_agent, AgentState  \n",
      "\n",
      "\n",
      "# Define custom state extending AgentState\n",
      "class CustomState(AgentState):\n",
      "    user_name: str\n",
      "\n",
      "@tool\n",
      "def greet(\n",
      "    runtime: ToolRuntime[CustomState]\n",
      ") -> str:\n",
      "    \"\"\"Use this to greet the user by name.\"\"\"\n",
      "    user_name = runtime.state.get(\"user_name\", \"Unknown\")  \n",
      "    return f\"Hello {user_name}!\"\n",
      "\n",
      "agent = create_agent(  \n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[greet],\n",
      "    state_schema=CustomState  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> [\n",
      "    {\n",
      "        'name': 'GetPopulation',\n",
      "        'args': {'location': 'Los Angeles, CA'},\n",
      "        'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\n",
      "        'type': 'tool_call'\n",
      "    },\n",
      "    {\n",
      "        'name': 'GetPopulation',\n",
      "        'args': {'location': 'New York City, NY'},\n",
      "        'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\n",
      "        'type': 'tool_call'\n",
      "    }\n",
      "]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Async Support\n",
      "content===> from agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n",
      "from agentevals.trajectory.match import create_async_trajectory_match_evaluator\n",
      "\n",
      "async_judge = create_async_trajectory_llm_as_judge(\n",
      "    model=\"openai:o3-mini\",\n",
      "    prompt=TRAJECTORY_ACCURACY_PROMPT,\n",
      ")\n",
      "\n",
      "async_evaluator = create_async_trajectory_match_evaluator(\n",
      "    trajectory_match_mode=\"strict\",\n",
      ")\n",
      "\n",
      "async def test_async_evaluation():\n",
      "    result = await agent.ainvoke({\n",
      "        \"messages\": [HumanMessage(content=\"What's the weather?\")]\n",
      "    })\n",
      "\n",
      "    evaluation = await async_judge(outputs=result[\"messages\"])\n",
      "    assert evaluation[\"score\"] is True\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google\n",
      "content===> langchain-google-cloud-sql-pg\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI\n",
      "content===> AzureAIChatCompletionsModel\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Lens\n",
      "content===> google-search-results\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon OpenSearch Service\n",
      "content===> Amazon OpenSearch Service\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Context still getting bloated\n",
      "content===> system_prompt=\"\"\"When you gather large amounts of data:\n",
      "1. Save raw data to /data/raw_results.txt\n",
      "2. Process and analyze the data\n",
      "3. Return only the analysis summary\n",
      "\n",
      "This keeps context clean.\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Debugging\n",
      "content===> for chunk in graph.stream(\n",
      "    {\"topic\": \"ice cream\"},\n",
      "    stream_mode=\"debug\",  \n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ToolRuntime\n",
      "content===> from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "# Access the current conversation state\n",
      "@tool\n",
      "def summarize_conversation(\n",
      "    runtime: ToolRuntime\n",
      ") -> str:\n",
      "    \"\"\"Summarize the conversation so far.\"\"\"\n",
      "    messages = runtime.state[\"messages\"]\n",
      "\n",
      "    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n",
      "    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n",
      "    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n",
      "\n",
      "    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n",
      "\n",
      "# Access custom state fields\n",
      "@tool\n",
      "def get_user_preference(\n",
      "    pref_name: str,\n",
      "    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n",
      ") -> str:\n",
      "    \"\"\"Get a user preference value.\"\"\"\n",
      "    preferences = runtime.state.get(\"user_preferences\", {})\n",
      "    return preferences.get(pref_name, \"Not set\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Customizing agent memory\n",
      "content===> from langchain.agents import create_agent, AgentState\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "\n",
      "\n",
      "class CustomAgentState(AgentState):  \n",
      "    user_id: str\n",
      "    preferences: dict\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-5\",\n",
      "    [get_user_info],\n",
      "    state_schema=CustomAgentState,  \n",
      "    checkpointer=InMemorySaver(),\n",
      ")\n",
      "\n",
      "# Custom state can be passed in invoke\n",
      "result = agent.invoke(\n",
      "    {\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
      "        \"user_id\": \"user_123\",  \n",
      "        \"preferences\": {\"theme\": \"dark\"}  \n",
      "    },\n",
      "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.tools import tool\n",
      "from langchain.messages import HumanMessage, AIMessage, ToolMessage\n",
      "from agentevals.trajectory.match import create_trajectory_match_evaluator\n",
      "\n",
      "\n",
      "@tool\n",
      "def get_weather(city: str):\n",
      "    \"\"\"Get weather information for a city.\"\"\"\n",
      "    return f\"It's 75 degrees and sunny in {city}.\"\n",
      "\n",
      "agent = create_agent(\"gpt-4o\", tools=[get_weather])\n",
      "\n",
      "evaluator = create_trajectory_match_evaluator(  \n",
      "    trajectory_match_mode=\"strict\",  \n",
      ")  \n",
      "\n",
      "def test_weather_tool_called_strict():\n",
      "    result = agent.invoke({\n",
      "        \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]\n",
      "    })\n",
      "\n",
      "    reference_trajectory = [\n",
      "        HumanMessage(content=\"What's the weather in San Francisco?\"),\n",
      "        AIMessage(content=\"\", tool_calls=[\n",
      "            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n",
      "        ]),\n",
      "        ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool_call_id=\"call_1\"),\n",
      "        AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"),\n",
      "    ]\n",
      "\n",
      "    evaluation = evaluator(\n",
      "        outputs=result[\"messages\"],\n",
      "        reference_outputs=reference_trajectory\n",
      "    )\n",
      "    # {\n",
      "    #     'key': 'trajectory_strict_match',\n",
      "    #     'score': True,\n",
      "    #     'comment': None,\n",
      "    # }\n",
      "    assert evaluation[\"score\"] is True\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add policy hooks\n",
      "content===> from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult\n",
      "from deepagents.backends.utils import FileInfo, GrepMatch\n",
      "\n",
      "class PolicyWrapper(BackendProtocol):\n",
      "    def __init__(self, inner: BackendProtocol, deny_prefixes: list[str] | None = None):\n",
      "        self.inner = inner\n",
      "        self.deny_prefixes = [p if p.endswith(\"/\") else p + \"/\" for p in (deny_prefixes or [])]\n",
      "\n",
      "    def _deny(self, path: str) -> bool:\n",
      "        return any(path.startswith(p) for p in self.deny_prefixes)\n",
      "\n",
      "    def ls_info(self, path: str) -> list[FileInfo]:\n",
      "        return self.inner.ls_info(path)\n",
      "    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:\n",
      "        return self.inner.read(file_path, offset=offset, limit=limit)\n",
      "    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:\n",
      "        return self.inner.grep_raw(pattern, path, glob)\n",
      "    def glob_info(self, pattern: str, path: str = \"/\") -> list[FileInfo]:\n",
      "        return self.inner.glob_info(pattern, path)\n",
      "    def write(self, file_path: str, content: str) -> WriteResult:\n",
      "        if self._deny(file_path):\n",
      "            return WriteResult(error=f\"Writes are not allowed under {file_path}\")\n",
      "        return self.inner.write(file_path, content)\n",
      "    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:\n",
      "        if self._deny(file_path):\n",
      "            return EditResult(error=f\"Edits are not allowed under {file_path}\")\n",
      "        return self.inner.edit(file_path, old_string, new_string, replace_all)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Define tool node\n",
      "content===> from langchain.messages import ToolMessage\n",
      "\n",
      "\n",
      "def tool_node(state: dict):\n",
      "    \"\"\"Performs the tool call\"\"\"\n",
      "\n",
      "    result = []\n",
      "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
      "        tool = tools_by_name[tool_call[\"name\"]]\n",
      "        observation = tool.invoke(tool_call[\"args\"])\n",
      "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
      "    return {\"messages\": result}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma on Vertex AI Model Garden\n",
      "content===> from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-google-vertexai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multiple structured outputs error\n",
      "content===> ================================ Human Message =================================\n",
      "\n",
      "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\n",
      "None\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  ContactInfo (call_1)\n",
      " Call ID: call_1\n",
      "  Args:\n",
      "    name: John Doe\n",
      "    email: john@email.com\n",
      "  EventDetails (call_2)\n",
      " Call ID: call_2\n",
      "  Args:\n",
      "    event_name: Tech Conference\n",
      "    date: March 15th\n",
      "================================= Tool Message =================================\n",
      "Name: ContactInfo\n",
      "\n",
      "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n",
      " Please fix your mistakes.\n",
      "================================= Tool Message =================================\n",
      "Name: EventDetails\n",
      "\n",
      "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n",
      " Please fix your mistakes.\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  ContactInfo (call_3)\n",
      " Call ID: call_3\n",
      "  Args:\n",
      "    name: John Doe\n",
      "    email: john@email.com\n",
      "================================= Tool Message =================================\n",
      "Name: ContactInfo\n",
      "\n",
      "Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tools\n",
      "content===> import os\n",
      "from typing import Literal\n",
      "from tavily import TavilyClient\n",
      "from deepagents import create_deep_agent\n",
      "\n",
      "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
      "\n",
      "def internet_search(\n",
      "    query: str,\n",
      "    max_results: int = 5,\n",
      "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
      "    include_raw_content: bool = False,\n",
      "):\n",
      "    \"\"\"Run a web search\"\"\"\n",
      "    return tavily_client.search(\n",
      "        query,\n",
      "        max_results=max_results,\n",
      "        include_raw_content=include_raw_content,\n",
      "        topic=topic,\n",
      "    )\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    tools=[internet_search]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Schema validation error\n",
      "content===> from pydantic import BaseModel, Field\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy\n",
      "\n",
      "\n",
      "class ProductRating(BaseModel):\n",
      "    rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5)\n",
      "    comment: str = Field(description=\"Review comment\")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5\",\n",
      "    tools=[],\n",
      "    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n",
      "    system_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n",
      ")\n",
      "\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> check_inventory(\"WH-1000XM5\")\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Filter by node\n",
      "content===> from typing import TypedDict\n",
      "from langgraph.graph import START, StateGraph\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "\n",
      "\n",
      "class State(TypedDict):\n",
      "      topic: str\n",
      "      joke: str\n",
      "      poem: str\n",
      "\n",
      "\n",
      "def write_joke(state: State):\n",
      "      topic = state[\"topic\"]\n",
      "      joke_response = model.invoke(\n",
      "            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n",
      "      )\n",
      "      return {\"joke\": joke_response.content}\n",
      "\n",
      "\n",
      "def write_poem(state: State):\n",
      "      topic = state[\"topic\"]\n",
      "      poem_response = model.invoke(\n",
      "            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n",
      "      )\n",
      "      return {\"poem\": poem_response.content}\n",
      "\n",
      "\n",
      "graph = (\n",
      "      StateGraph(State)\n",
      "      .add_node(write_joke)\n",
      "      .add_node(write_poem)\n",
      "      # write both the joke and the poem concurrently\n",
      "      .add_edge(START, \"write_joke\")\n",
      "      .add_edge(START, \"write_poem\")\n",
      "      .compile()\n",
      ")\n",
      "\n",
      "# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n",
      "# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n",
      "# with information about the graph node where the LLM was called and other information\n",
      "for msg, metadata in graph.stream(\n",
      "    {\"topic\": \"cats\"},\n",
      "    stream_mode=\"messages\",  \n",
      "):\n",
      "    # Filter the streamed tokens by the langgraph_node field in the metadata\n",
      "    # to only include the tokens from the write_poem node\n",
      "    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n",
      "        print(msg.content, end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured output\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class Weather(BaseModel):\n",
      "    temperature: float\n",
      "    condition: str\n",
      "\n",
      "def weather_tool(city: str) -> str:\n",
      "    \"\"\"Get the weather for a city.\"\"\"\n",
      "    return f\"it's sunny and 70 degrees in {city}\"\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-4o-mini\",\n",
      "    tools=[weather_tool],\n",
      "    response_format=ToolStrategy(Weather)\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n",
      "})\n",
      "\n",
      "print(repr(result[\"structured_response\"]))\n",
      "# results in `Weather(temperature=70.0, condition='sunny')`\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> summarize_conversation\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Channels\n",
      "content===> BinaryOperatorAggregate\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> from langchain.tools import tool\n",
      "\n",
      "@tool\n",
      "def get_weather(location: str) -> str:\n",
      "    \"\"\"Get the weather at a location.\"\"\"\n",
      "    return f\"It's sunny in {location}.\"\n",
      "\n",
      "\n",
      "model_with_tools = model.bind_tools([get_weather])  \n",
      "\n",
      "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
      "for tool_call in response.tool_calls:\n",
      "    # View tool calls made by the model\n",
      "    print(f\"Tool: {tool_call['name']}\")\n",
      "    print(f\"Args: {tool_call['args']}\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add a graph as a node\n",
      "content===> from typing_extensions import TypedDict\n",
      "from langgraph.graph.state import StateGraph, START\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "# Subgraph\n",
      "\n",
      "def subgraph_node_1(state: State):\n",
      "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
      "\n",
      "subgraph_builder = StateGraph(State)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Parent graph\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"node_1\", subgraph)  \n",
      "builder.add_edge(START, \"node_1\")\n",
      "graph = builder.compile()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Durability modes\n",
      "content===> checkpoint_during=False\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trim messages\n",
      "content===> ================================== Ai Message ==================================\n",
      "\n",
      "Your name is Bob, as you mentioned when you first introduced yourself.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> create_trajectory_match_evaluator\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining tools\n",
      "content===> from langchain.tools import tool\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "@tool\n",
      "def search(query: str) -> str:\n",
      "    \"\"\"Search for information.\"\"\"\n",
      "    return f\"Results for: {query}\"\n",
      "\n",
      "@tool\n",
      "def get_weather(location: str) -> str:\n",
      "    \"\"\"Get weather information for a location.\"\"\"\n",
      "    return f\"Weather in {location}: Sunny, 72°F\"\n",
      "\n",
      "agent = create_agent(model, tools=[search, get_weather])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLMs\n",
      "content===> from langchain_google_genai import GoogleGenerativeAI\n",
      "\n",
      "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
      "result = llm.invoke(\"Sing a ballad of LangChain.\")\n",
      "print(result)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in subgraphs\n",
      "content===> subgraph_builder = StateGraph(...)\n",
      "subgraph = subgraph_builder.compile(checkpointer=True)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use with any LLM\n",
      "content===> inputs = {\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"content\": None,\n",
      "            \"role\": \"assistant\",\n",
      "            \"tool_calls\": [\n",
      "                {\n",
      "                    \"id\": \"1\",\n",
      "                    \"function\": {\n",
      "                        \"arguments\": '{\"place\":\"bedroom\"}',\n",
      "                        \"name\": \"get_items\",\n",
      "                    },\n",
      "                    \"type\": \"function\",\n",
      "                }\n",
      "            ],\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "async for chunk in graph.astream(\n",
      "    inputs,\n",
      "    stream_mode=\"custom\",\n",
      "):\n",
      "    print(chunk[\"content\"], end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> \"Invalid regex pattern: ...\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> search_products(\"wireless headphones\")\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Development environment\n",
      "content===> cd libs/langchain\n",
      "uv sync --all-groups\n",
      "make test  # Ensure tests pass before starting development\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multimodal\n",
      "content===> \"extras\": {\"key\": value}\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> {\n",
      "    ...\n",
      "    \"store\": {\n",
      "        \"index\": {\n",
      "            \"embed\": \"openai:text-embeddings-3-small\",\n",
      "            \"dims\": 1536,\n",
      "            \"fields\": [\"$\"]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Provider strategy\n",
      "content===> response_format=ToolStrategy(ProductReview)\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Static model\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-5\",\n",
      "    tools=tools\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-google-genai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Search Tool\n",
      "content===> # Note: YouTubeSearchTool might be in langchain or langchain_community\n",
      "from langchain.tools import YouTubeSearchTool # Or langchain_community.tools\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Testing requirements\n",
      "content===> make test\n",
      "# Or directly:\n",
      "uv run --group test pytest tests/unit_tests\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "\n",
      "_set_env(\"ANTHROPIC_API_KEY\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> response_format = ToolStrategy(\n",
      "    schema=ProductRating,\n",
      "    handle_errors=False  # All errors raised\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quick start\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "def send_email(to: str, subject: str, body: str):\n",
      "    \"\"\"Send an email to a recipient.\"\"\"\n",
      "    # ... email sending logic\n",
      "    return f\"Email sent to {to}\"\n",
      "\n",
      "def search_web(query: str):\n",
      "    \"\"\"Search the web for information.\"\"\"\n",
      "    # ... web search logic\n",
      "    return f\"Search results for: {query}\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[send_email, search_web],\n",
      "    system_prompt=\"You are a helpful assistant that can send emails and search the web.\"\n",
      ")\n",
      "\n",
      "# Run the agent - all steps will be traced automatically\n",
      "response = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for the latest AI news and email a summary to john@example.com\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool error handling\n",
      "content===> [\n",
      "    ...\n",
      "    ToolMessage(\n",
      "        content=\"Tool error: Please check your input and try again. (division by zero)\",\n",
      "        tool_call_id=\"...\"\n",
      "    ),\n",
      "    ...\n",
      "]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> {\n",
      "  \"dependencies\": [\".\"],\n",
      "  \"graphs\": {\n",
      "    \"agent\": \"./src/agent.py:agent\"\n",
      "  },\n",
      "  \"env\": \".env\"\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware2.wrap_model_call()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Channels\n",
      "content===> total = BinaryOperatorAggregate(int, operator.add)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured output\n",
      "content===> 'structured_response'\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> ================================= Tool Message =================================\n",
      "\n",
      "Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 4: Create a deep agent\n",
      "content===> # System prompt to steer the agent to be an expert researcher\n",
      "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
      "\n",
      "You have access to an internet search tool as your primary means of gathering information.\n",
      "\n",
      "## `internet_search`\n",
      "\n",
      "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
      "\"\"\"\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    tools=[internet_search],\n",
      "    system_prompt=research_instructions\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> ================================== Ai Message ==================================\n",
      "\n",
      "From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n",
      "\n",
      "Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> langchain-google-genai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU \"langchain[azure]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Async with Python < 3.11\n",
      "content===> from typing import TypedDict\n",
      "from langgraph.graph import START, StateGraph\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(model=\"gpt-4o-mini\")\n",
      "\n",
      "class State(TypedDict):\n",
      "    topic: str\n",
      "    joke: str\n",
      "\n",
      "# Accept config as an argument in the async node function\n",
      "async def call_model(state, config):\n",
      "    topic = state[\"topic\"]\n",
      "    print(\"Generating joke...\")\n",
      "    # Pass config to model.ainvoke() to ensure proper context propagation\n",
      "    joke_response = await model.ainvoke(  \n",
      "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n",
      "        config,\n",
      "    )\n",
      "    return {\"joke\": joke_response.content}\n",
      "\n",
      "graph = (\n",
      "    StateGraph(State)\n",
      "    .add_node(call_model)\n",
      "    .add_edge(START, \"call_model\")\n",
      "    .compile()\n",
      ")\n",
      "\n",
      "# Set stream_mode=\"messages\" to stream LLM tokens\n",
      "async for chunk, metadata in graph.astream(\n",
      "    {\"topic\": \"ice cream\"},\n",
      "    stream_mode=\"messages\",  \n",
      "):\n",
      "    if chunk.content:\n",
      "        print(chunk.content, end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> def summarize_conversation(state: State):\n",
      "\n",
      "    # First, we get any existing summary\n",
      "    summary = state.get(\"summary\", \"\")\n",
      "\n",
      "    # Create our summarization prompt\n",
      "    if summary:\n",
      "\n",
      "        # A summary already exists\n",
      "        summary_message = (\n",
      "            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n",
      "            \"Extend the summary by taking into account the new messages above:\"\n",
      "        )\n",
      "\n",
      "    else:\n",
      "        summary_message = \"Create a summary of the conversation above:\"\n",
      "\n",
      "    # Add prompt to our history\n",
      "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
      "    response = model.invoke(messages)\n",
      "\n",
      "    # Delete all but the 2 most recent messages\n",
      "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
      "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Productivity tools\n",
      "content===> NotionDirectoryLoader\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom tool message content\n",
      "content===> ================================= Tool Message =================================\n",
      "Name: MeetingAction\n",
      "\n",
      "Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Cloud\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. Build and compile the agent\n",
      "content===> # Build workflow\n",
      "agent_builder = StateGraph(MessagesState)\n",
      "\n",
      "# Add nodes\n",
      "agent_builder.add_node(\"llm_call\", llm_call)\n",
      "agent_builder.add_node(\"tool_node\", tool_node)\n",
      "\n",
      "# Add edges to connect nodes\n",
      "agent_builder.add_edge(START, \"llm_call\")\n",
      "agent_builder.add_conditional_edges(\n",
      "    \"llm_call\",\n",
      "    should_continue,\n",
      "    [\"tool_node\", END]\n",
      ")\n",
      "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
      "\n",
      "# Compile the agent\n",
      "agent = agent_builder.compile()\n",
      "\n",
      "# Show the agent\n",
      "from IPython.display import Image, display\n",
      "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
      "\n",
      "# Invoke\n",
      "from langchain.messages import HumanMessage\n",
      "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
      "messages = agent.invoke({\"messages\": messages})\n",
      "for m in messages[\"messages\"]:\n",
      "    m.pretty_print()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Enable tracing\n",
      "content===> export LANGSMITH_TRACING=true\n",
      "export LANGSMITH_API_KEY=<your-api-key>\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text structure-based\n",
      "content===> RecursiveCharacterTextSplitter\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> parallel_tool_calls=False\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool Message\n",
      "content===> from langchain.messages import ToolMessage\n",
      "\n",
      "# Sent to model\n",
      "message_content = \"It was the best of times, it was the worst of times.\"\n",
      "\n",
      "# Artifact available downstream\n",
      "artifact = {\"document_id\": \"doc_123\", \"page\": 0}\n",
      "\n",
      "tool_message = ToolMessage(\n",
      "    content=message_content,\n",
      "    tool_call_id=\"call_123\",\n",
      "    name=\"search_books\",\n",
      "    artifact=artifact,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Log to a project\n",
      "content===> import langsmith as ls\n",
      "\n",
      "with ls.tracing_context(project_name=\"email-agent-test\", enabled=True):\n",
      "    response = agent.invoke({\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]\n",
      "    })\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream multiple modes\n",
      "content===> from langchain.agents import create_agent\n",
      "from langgraph.config import get_stream_writer\n",
      "\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    writer = get_stream_writer()\n",
      "    writer(f\"Looking up data for city: {city}\")\n",
      "    writer(f\"Acquired data for city: {city}\")\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[get_weather],\n",
      ")\n",
      "\n",
      "for stream_mode, chunk in agent.stream(  \n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
      "    stream_mode=[\"updates\", \"custom\"]\n",
      "):\n",
      "    print(f\"stream_mode: {stream_mode}\")\n",
      "    print(f\"content: {chunk}\")\n",
      "    print(\"\\n\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prerequisites\n",
      "content===> $ pip install -U pytest\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> ================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  search_products (call_abc123)\n",
      " Call ID: call_abc123\n",
      "  Args:\n",
      "    query: wireless headphones\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft SharePoint\n",
      "content===> from langchain_community.document_loaders.sharepoint import SharePointLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI\n",
      "content===> from langchain_google_vertexai import VertexAIEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Firestore (Native Mode)\n",
      "content===> from langchain_google_firestore import FirestoreVectorStore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> AzureCogsText2SpeechTool\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Selecting formats\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
      "from pydantic import BaseModel, Field\n",
      "from typing import Callable\n",
      "\n",
      "class SimpleResponse(BaseModel):\n",
      "    \"\"\"Simple response for early conversation.\"\"\"\n",
      "    answer: str = Field(description=\"A brief answer\")\n",
      "\n",
      "class DetailedResponse(BaseModel):\n",
      "    \"\"\"Detailed response for established conversation.\"\"\"\n",
      "    answer: str = Field(description=\"A detailed answer\")\n",
      "    reasoning: str = Field(description=\"Explanation of reasoning\")\n",
      "    confidence: float = Field(description=\"Confidence score 0-1\")\n",
      "\n",
      "@wrap_model_call\n",
      "def state_based_output(\n",
      "    request: ModelRequest,\n",
      "    handler: Callable[[ModelRequest], ModelResponse]\n",
      ") -> ModelResponse:\n",
      "    \"\"\"Select output format based on State.\"\"\"\n",
      "    # request.messages is a shortcut for request.state[\"messages\"]\n",
      "    message_count = len(request.messages)  \n",
      "\n",
      "    if message_count < 3:\n",
      "        # Early conversation - use simple format\n",
      "        request = request.override(response_format=SimpleResponse)  \n",
      "    else:\n",
      "        # Established conversation - use detailed format\n",
      "        request = request.override(response_format=DetailedResponse)  \n",
      "\n",
      "    return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[state_based_output]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> AzureCogsSpeech2TextTool\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Search\n",
      "content===> from langchain_community.vectorstores.azuresearch import AzureSearch\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Scholar\n",
      "content===> pip install google-search-results langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Semantic Search\n",
      "content===> # Find memories about food preferences\n",
      "# (This can be done after putting memories into the store)\n",
      "memories = store.search(\n",
      "    namespace_for_memory,\n",
      "    query=\"What does the user like to eat?\",\n",
      "    limit=3  # Return top 3 matches\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Speech-to-Text\n",
      "content===> pip install langchain-google-community[speech]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Semantic Search\n",
      "content===> from langchain.embeddings import init_embeddings\n",
      "\n",
      "store = InMemoryStore(\n",
      "    index={\n",
      "        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n",
      "        \"dims\": 1536,                              # Embedding dimensions\n",
      "        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n",
      "    }\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use a virtual filesystem\n",
      "content===> files(path text primary key, content text, created_at timestamptz, modified_at timestamptz)\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use semantic search\n",
      "content===> from langchain.embeddings import init_embeddings\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "# Create store with semantic search enabled\n",
      "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
      "store = InMemoryStore(\n",
      "    index={\n",
      "        \"embed\": embeddings,\n",
      "        \"dims\": 1536,\n",
      "    }\n",
      ")\n",
      "\n",
      "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
      "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n",
      "\n",
      "items = store.search(\n",
      "    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SubAgent (Dictionary-based)\n",
      "content===> \"provider:model-name\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install\n",
      "content===> pip install -U langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic tool definition\n",
      "content===> from langchain.tools import tool\n",
      "\n",
      "@tool\n",
      "def search_database(query: str, limit: int = 10) -> str:\n",
      "    \"\"\"Search the customer database for records matching the query.\n",
      "\n",
      "    Args:\n",
      "        query: Search terms to look for\n",
      "        limit: Maximum number of results to return\n",
      "    \"\"\"\n",
      "    return f\"Found {limit} results for '{query}'\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool retry\n",
      "content===> initial_delay * (backoff_factor ** retry_number)\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling strategy\n",
      "content===> tuple[type[Exception], ...]\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Token usage\n",
      "content===> {\n",
      "    'gpt-4o-mini-2024-07-18': {\n",
      "        'input_tokens': 8,\n",
      "        'output_tokens': 10,\n",
      "        'total_tokens': 18,\n",
      "        'input_token_details': {'audio': 0, 'cache_read': 0},\n",
      "        'output_token_details': {'audio': 0, 'reasoning': 0}\n",
      "    },\n",
      "    'claude-haiku-4-5-20251001': {\n",
      "        'input_tokens': 8,\n",
      "        'output_tokens': 21,\n",
      "        'total_tokens': 29,\n",
      "        'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n",
      "    }\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Drive\n",
      "content===> from langchain_googledrive.retrievers import GoogleDriveRetriever\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Featured providers\n",
      "content===> ChatGoogleGenerativeAI\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Scholar\n",
      "content===> google-search-results\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Repository structure\n",
      "content===> langchain-text-splitters\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware2.after_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ToolStrategy(\n",
      "    schema=ProductRating,\n",
      "    handle_errors=ValueError  # Only retry on ValueError, raise others\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> MultipleStructuredOutputsError\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> read(file_path: str, offset: int = 0, limit: int = 2000) -> str\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI Warehouse\n",
      "content===> pip install langchain-google-community # Add specific docai dependencies if needed\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_chroma import Chroma\n",
      "\n",
      "vector_store = Chroma(\n",
      "    collection_name=\"example_collection\",\n",
      "    embedding_function=embeddings,\n",
      "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Translate\n",
      "content===> GoogleTranslateTransformer\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Batch\n",
      "content===> for response in model.batch_as_completed([\n",
      "    \"Why do parrots have colorful feathers?\",\n",
      "    \"How do airplanes fly?\",\n",
      "    \"What is quantum computing?\"\n",
      "]):\n",
      "    print(response)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"VOYAGE_API_KEY\"):\n",
      "  os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n",
      "\n",
      "from langchain-voyageai import VoyageAIEmbeddings\n",
      "\n",
      "embeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tools\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[check_weather, search_web]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quickstart\n",
      "content===> agent = create_deep_agent(backend=lambda rt: StoreBackend(rt))\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-nvidia-ai-endpoints\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI\n",
      "content===> from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n",
      "\n",
      "embed_model = AzureAIEmbeddingsModel(\n",
      "    model_name=\"text-embedding-ada-002\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SageMaker Endpoint\n",
      "content===> from langchain_aws import SagemakerEndpoint\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Chat models\n",
      "content===> data:image/png;base64,...\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> my-app/\n",
      "├── src\n",
      "│   └── agent.py\n",
      "├── .env\n",
      "└── langgraph.json\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> embed_documents(texts: List[str]) → List[List[float]]\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Finance\n",
      "content===> pip install google-search-results langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Access\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_name: str\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[...],\n",
      "    context_schema=Context  \n",
      ")\n",
      "\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
      "    context=Context(user_name=\"John Smith\")  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Token usage\n",
      "content===> {'input_tokens': 8,\n",
      " 'output_tokens': 304,\n",
      " 'total_tokens': 312,\n",
      " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
      " 'output_token_details': {'audio': 0, 'reasoning': 256}}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LangSmith Integration\n",
      "content===> pytest test_trajectory.py --langsmith-output\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_core.embeddings import DeterministicFakeEmbedding\n",
      "\n",
      "embeddings = DeterministicFakeEmbedding(size=4096)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> model_with_tools = model.bind_tools([get_weather])\n",
      "\n",
      "response = model_with_tools.invoke(\n",
      "    \"What's the weather in Boston and Tokyo?\"\n",
      ")\n",
      "\n",
      "\n",
      "# The model may generate multiple tool calls\n",
      "print(response.tool_calls)\n",
      "# [\n",
      "#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n",
      "#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n",
      "# ]\n",
      "\n",
      "\n",
      "# Execute all tools (can be done in parallel with async)\n",
      "results = []\n",
      "for tool_call in response.tool_calls:\n",
      "    if tool_call['name'] == 'get_weather':\n",
      "        result = get_weather.invoke(tool_call)\n",
      "    ...\n",
      "    results.append(result)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Vector Search\n",
      "content===> from langchain_google_vertexai import VectorSearchVectorStoreDatastore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> from langgraph.checkpoint.memory import InMemorySaver\n",
      "\n",
      "# We need this because we want to enable threads (conversations)\n",
      "checkpointer = InMemorySaver()\n",
      "\n",
      "# ... Define the graph ...\n",
      "\n",
      "# Compile the graph with the checkpointer and store\n",
      "graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware1.before_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image editor\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> langchain-classic\n",
      "content===> CacheBackedEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Self-improving instructions\n",
      "content===> agent = create_deep_agent(\n",
      "    store=store,\n",
      "    use_longterm_memory=True,\n",
      "    system_prompt=\"\"\"You have a file at /memories/instructions.txt with additional\n",
      "    instructions and preferences.\n",
      "\n",
      "    Read this file at the start of conversations to understand user preferences.\n",
      "\n",
      "    When users provide feedback like \"please always do X\" or \"I prefer Y\",\n",
      "    update /memories/instructions.txt using the edit_file tool.\"\"\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Transcripts Loader\n",
      "content===> from langchain_community.document_loaders import YoutubeLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SageMaker Endpoint\n",
      "content===> from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
      "from langchain_community.llms.sagemaker_endpoint import ContentHandlerBase\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Finance\n",
      "content===> google-search-results\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Get state\n",
      "content===> graph.get_state(config)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> \"Error: File '/x' not found\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content blocks\n",
      "content===> from langchain.messages import AIMessage\n",
      "\n",
      "message = AIMessage(\n",
      "    content=[\n",
      "        {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n",
      "        {\"type\": \"text\", \"text\": \"...\"},\n",
      "    ],\n",
      "    response_metadata={\"model_provider\": \"anthropic\"}\n",
      ")\n",
      "message.content_blocks\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Streaming and chunks\n",
      "content===> chunks = []\n",
      "full_message = None\n",
      "for chunk in model.stream(\"Hi\"):\n",
      "    chunks.append(chunk)\n",
      "    print(chunk.text)\n",
      "    full_message = chunk if full_message is None else full_message + chunk\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> values\n",
      "content===> graph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 5. Define end logic\n",
      "content===> from typing import Literal\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "\n",
      "\n",
      "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
      "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
      "\n",
      "    messages = state[\"messages\"]\n",
      "    last_message = messages[-1]\n",
      "\n",
      "    # If the LLM makes a tool call, then perform an action\n",
      "    if last_message.tool_calls:\n",
      "        return \"tool_node\"\n",
      "\n",
      "    # Otherwise, we stop (reply to the user)\n",
      "    return END\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-unstructured\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Schema validation error\n",
      "content===> ================================ Human Message =================================\n",
      "\n",
      "Parse this: Amazing product, 10/10!\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  ProductRating (call_1)\n",
      " Call ID: call_1\n",
      "  Args:\n",
      "    rating: 10\n",
      "    comment: Amazing product\n",
      "================================= Tool Message =================================\n",
      "Name: ProductRating\n",
      "\n",
      "Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating\n",
      "  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n",
      " Please fix your mistakes.\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  ProductRating (call_2)\n",
      " Call ID: call_2\n",
      "  Args:\n",
      "    rating: 5\n",
      "    comment: Amazing product\n",
      "================================= Tool Message =================================\n",
      "Name: ProductRating\n",
      "\n",
      "Returning structured response: {'rating': 5, 'comment': 'Amazing product'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Do not return complex values ininterruptcalls\n",
      "content===> def node_a(state: State):\n",
      "    # ✅ Good: passing simple types that are serializable\n",
      "    name = interrupt(\"What's your name?\")\n",
      "    count = interrupt(42)\n",
      "    approved = interrupt(True)\n",
      "\n",
      "    return {\"name\": name, \"count\": count, \"approved\": approved}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling strategy\n",
      "content===> from pydantic import BaseModel, Field\n",
      "from typing import Literal\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy\n",
      "\n",
      "\n",
      "class ProductReview(BaseModel):\n",
      "    \"\"\"Analysis of a product review.\"\"\"\n",
      "    rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n",
      "    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n",
      "    key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5\",\n",
      "    tools=tools,\n",
      "    response_format=ToolStrategy(ProductReview)\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n",
      "})\n",
      "result[\"structured_response\"]\n",
      "# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Installation\n",
      "content===> toolbox --tools-file \"tools.yaml\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Search\n",
      "content===> from langchain_community.agent_toolkits.load_tools import load_tools\n",
      "tools = load_tools([\"google-search\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use anonymizers to prevent logging of sensitive data in traces\n",
      "content===> from langchain_core.tracers.langchain import LangChainTracer\n",
      "from langgraph.graph import StateGraph, MessagesState\n",
      "from langsmith import Client\n",
      "from langsmith.anonymizer import create_anonymizer\n",
      "\n",
      "anonymizer = create_anonymizer([\n",
      "    # Matches SSNs\n",
      "    { \"pattern\": r\"\\b\\d{3}-?\\d{2}-?\\d{4}\\b\", \"replace\": \"<ssn>\" }\n",
      "])\n",
      "\n",
      "tracer_client = Client(anonymizer=anonymizer)\n",
      "tracer = LangChainTracer(client=tracer_client)\n",
      "# Define the graph\n",
      "graph = (\n",
      "    StateGraph(MessagesState)\n",
      "    ...\n",
      "    .compile()\n",
      "    .with_config({'callbacks': [tracer]})\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> After agent guardrails\n",
      "content===> from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n",
      "from langgraph.runtime import Runtime\n",
      "from langchain_core.messages import AIMessage\n",
      "from langchain.chat_models import init_chat_model\n",
      "from typing import Any\n",
      "\n",
      "class SafetyGuardrailMiddleware(AgentMiddleware):\n",
      "    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.safety_model = init_chat_model(\"gpt-4o-mini\")\n",
      "\n",
      "    @hook_config(can_jump_to=[\"end\"])\n",
      "    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "        # Get the final AI response\n",
      "        if not state[\"messages\"]:\n",
      "            return None\n",
      "\n",
      "        last_message = state[\"messages\"][-1]\n",
      "        if not isinstance(last_message, AIMessage):\n",
      "            return None\n",
      "\n",
      "        # Use a model to evaluate safety\n",
      "        safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n",
      "        Respond with only 'SAFE' or 'UNSAFE'.\n",
      "\n",
      "        Response: {last_message.content}\"\"\"\n",
      "\n",
      "        result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n",
      "\n",
      "        if \"UNSAFE\" in result.content:\n",
      "            return {\n",
      "                \"messages\": [{\n",
      "                    \"role\": \"assistant\",\n",
      "                    \"content\": \"I cannot provide that response. Please rephrase your request.\"\n",
      "                }],\n",
      "                \"jump_to\": \"end\"\n",
      "            }\n",
      "\n",
      "        return None\n",
      "\n",
      "# Use the safety guardrail\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[search_tool, calculator_tool],\n",
      "    middleware=[SafetyGuardrailMiddleware()],\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Kendra\n",
      "content===> pip install langchain-aws\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Embedding Models\n",
      "content===> GoogleGenerativeAIEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Length-based\n",
      "content===> from langchain_text_splitters import CharacterTextSplitter\n",
      "\n",
      "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
      "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
      ")\n",
      "texts = text_splitter.split_text(document)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Resume execution from the checkpoint\n",
      "content===> {'topic': 'chickens',\n",
      " 'joke': 'Why did the chicken join a band?\\n\\nBecause it had excellent drumsticks!'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
      "  os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n",
      "\n",
      "from langchain_openai import AzureOpenAIEmbeddings\n",
      "\n",
      "embeddings = AzureOpenAIEmbeddings(\n",
      "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
      "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
      "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> # Inside your repo, install dependencies\n",
      "uv sync --all-groups\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Storage\n",
      "content===> pip install langchain-google-community[gcs]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware3.wrap_model_call()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft OneNote\n",
      "content===> pip install bs4 msal\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> AzureCosmosDBMongoVCoreVectorStore\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> model_with_tools.invoke(\n",
      "    \"what's bigger in 2024 LA or NYC\",\n",
      "    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},\n",
      ").tool_calls\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> memories[-1].dict()\n",
      "{'value': {'food_preference': 'I like pizza'},\n",
      " 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n",
      " 'namespace': ['1', 'memories'],\n",
      " 'created_at': '2024-10-02T17:22:31.590602+00:00',\n",
      " 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-ollama\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Search\n",
      "content===> pip install langchain-google-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Do not wrapinterruptcalls in try/except\n",
      "content===> def node_a(state: State):\n",
      "    # ✅ Good: interrupting first, then handling\n",
      "    # error conditions separately\n",
      "    interrupt(\"What's your name?\")\n",
      "    try:\n",
      "        fetch_data()  # This can fail\n",
      "    except Exception as e:\n",
      "        print(e)\n",
      "    return state\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-azure-ai azure-cosmos\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke\n",
      "content===> response = model.invoke(\"Why do parrots have colorful feathers?\")\n",
      "print(response)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Get state history\n",
      "content===> graph.get_state_history(config)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Content block reference\n",
      "content===> \"server_tool_call_chunk\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Installation\n",
      "content===> pip install toolbox-langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> AzureCogsTextAnalyticsHealthTool\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 3: Create a search tool\n",
      "content===> import os\n",
      "from typing import Literal\n",
      "from tavily import TavilyClient\n",
      "from deepagents import create_deep_agent\n",
      "\n",
      "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
      "\n",
      "def internet_search(\n",
      "    query: str,\n",
      "    max_results: int = 5,\n",
      "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
      "    include_raw_content: bool = False,\n",
      "):\n",
      "    \"\"\"Run a web search\"\"\"\n",
      "    return tavily_client.search(\n",
      "        query,\n",
      "        max_results=max_results,\n",
      "        include_raw_content=include_raw_content,\n",
      "        topic=topic,\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Translate\n",
      "content===> from langchain_google_community import GoogleTranslateTransformer\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Resuming interrupts\n",
      "content===> from langgraph.types import Command\n",
      "\n",
      "# Initial run - hits the interrupt and pauses\n",
      "# thread_id is the persistent pointer (stores a stable ID in production)\n",
      "config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
      "result = graph.invoke({\"input\": \"data\"}, config=config)\n",
      "\n",
      "# Check what was interrupted\n",
      "# __interrupt__ contains the payload that was passed to interrupt()\n",
      "print(result[\"__interrupt__\"])\n",
      "# > [Interrupt(value='Do you approve this action?')]\n",
      "\n",
      "# Resume with the human's response\n",
      "# The resume payload becomes the return value of interrupt() inside the node\n",
      "graph.invoke(Command(resume=True), config=config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> WriteResult(error, path, files_update)\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> BigQuery Vector Search\n",
      "content===> # Note: BigQueryVectorSearch might be in langchain or langchain_community depending on version\n",
      "# Check imports in the usage example.\n",
      "from langchain.vectorstores import BigQueryVectorSearch # Or langchain_community.vectorstores\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agent jumps\n",
      "content===> class EarlyExitMiddleware(AgentMiddleware):\n",
      "    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
      "        # Check some condition\n",
      "        if should_exit(state):\n",
      "            return {\n",
      "                \"messages\": [AIMessage(\"Exiting early due to condition.\")],\n",
      "                \"jump_to\": \"end\"\n",
      "            }\n",
      "        return None\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image generator\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> # Bind (potentially multiple) tools to the model\n",
      "model_with_tools = model.bind_tools([get_weather])\n",
      "\n",
      "# Step 1: Model generates tool calls\n",
      "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
      "ai_msg = model_with_tools.invoke(messages)\n",
      "messages.append(ai_msg)\n",
      "\n",
      "# Step 2: Execute tools and collect results\n",
      "for tool_call in ai_msg.tool_calls:\n",
      "    # Execute the tool with the generated arguments\n",
      "    tool_result = get_weather.invoke(tool_call)\n",
      "    messages.append(tool_result)\n",
      "\n",
      "# Step 3: Pass results back to model for final response\n",
      "final_response = model_with_tools.invoke(messages)\n",
      "print(final_response.text)\n",
      "# \"The current weather in Boston is 72°F and sunny.\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceEmbeddings\n",
      "content===> from langchain_huggingface import HuggingFaceEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tokens\n",
      "content===> stream_mode=\"messages\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Similarity search\n",
      "content===> similar_docs = vector_store.similarity_search(\"your query here\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_milvus import Milvus\n",
      "\n",
      "URI = \"./milvus_example.db\"\n",
      "\n",
      "vector_store = Milvus(\n",
      "    embedding_function=embeddings,\n",
      "    connection_args={\"uri\": URI},\n",
      "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Jobs\n",
      "content===> google-search-results\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> git clone https://github.com/your-username/name-of-forked-repo.git\n",
      "\n",
      "# For instance, for LangChain:\n",
      "git clone https://github.com/parrot123/langchain.git\n",
      "\n",
      "# For LangGraph:\n",
      "git clone https://github.com/parrot123/langgraph.git\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AWS Lambda\n",
      "content===> from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> View subgraph state\n",
      "content===> graph.get_state(config, subgraphs=True)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tokens\n",
      "content===> node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: tools\n",
      "content: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: []\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': 'Here'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ''s'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' what'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' I'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' got'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ':'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' \"'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': \"It's\"}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' always'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' sunny'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' in'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' San'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': ' Francisco'}]\n",
      "\n",
      "\n",
      "node: model\n",
      "content: [{'type': 'text', 'text': '!\"\\n\\n'}]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Blob Storage\n",
      "content===> from langchain_azure_storage.document_loaders import AzureBlobStorageLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> BigQuery\n",
      "content===> pip install langchain-google-community[bigquery]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invocation config\n",
      "content===> response = model.invoke(\n",
      "    \"Tell me a joke\",\n",
      "    config={\n",
      "        \"run_name\": \"joke_generation\",      # Custom name for this run\n",
      "        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n",
      "        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n",
      "        \"callbacks\": [my_callback_handler], # Callback handlers\n",
      "    }\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> pip install -U langgraph langgraph-checkpoint-redis\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceInstructEmbeddings\n",
      "content===> HuggingFaceInstructEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 7. Test the API\n",
      "content===> pip install langgraph-sdk\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Spanner\n",
      "content===> pip install langchain-google-spanner\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Static prompt rename\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[check_weather],\n",
      "    system_prompt=\"You are a helpful assistant\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-nomic\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agent progress\n",
      "content===> step: model\n",
      "content: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\n",
      "\n",
      "step: tools\n",
      "content: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n",
      "\n",
      "step: model\n",
      "content: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom updates\n",
      "content===> Looking up data for city: San Francisco\n",
      "Acquired data for city: San Francisco\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_azure_ai.vectorstores.azure_cosmos_db_mongo_vcore import (\n",
      "    AzureCosmosDBMongoVCoreVectorSearch,\n",
      ")\n",
      "\n",
      "vectorstore = AzureCosmosDBMongoVCoreVectorSearch.from_documents(\n",
      "    docs,\n",
      "    openai_embeddings,\n",
      "    collection=collection,\n",
      "    index_name=INDEX_NAME,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 1: Install dependencies\n",
      "content===> pip install deepagents tavily-python\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Chat Completions API\n",
      "content===> model = ChatDeepSeek(\n",
      "    model=\"...\",\n",
      "    api_key=\"...\",\n",
      "    api_base=\"https://openrouter.ai/api/v1\",\n",
      "    extra_body={\"reasoning\": {\"enabled\": True}},\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for MySQL\n",
      "content===> from langchain_google_cloud_sql_mysql import MySQLVectorStore # MySQLEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma local from Kaggle\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cosmos DB for MongoDB (vCore)\n",
      "content===> from langchain_community.vectorstores import AzureCosmosDBVectorSearch\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Featured providers\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deprecations\n",
      "content===> langchain.agents.AgentState\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ChatHuggingFace\n",
      "content===> from langchain_huggingface import ChatHuggingFace\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon MemoryDB\n",
      "content===> from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore\n",
      "\n",
      "vds = InMemoryVectorStore.from_documents(\n",
      "            chunks,\n",
      "            embeddings,\n",
      "            redis_url=\"rediss://cluster_endpoint:6379/ssl=True ssl_cert_reqs=none\",\n",
      "            vector_schema=vector_schema,\n",
      "            index_name=INDEX_NAME,\n",
      "        )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamic runtime context\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
      "from langchain.agents import AgentState\n",
      "\n",
      "\n",
      "class CustomState(AgentState):  \n",
      "    user_name: str\n",
      "\n",
      "@dynamic_prompt\n",
      "def personalized_prompt(request: ModelRequest) -> str:  \n",
      "    user_name = request.state.get(\"user_name\", \"User\")\n",
      "    return f\"You are a helpful assistant. User's name is {user_name}\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[...],\n",
      "    state_schema=CustomState,  \n",
      "    middleware=[personalized_prompt],  \n",
      ")\n",
      "\n",
      "agent.invoke({\n",
      "    \"messages\": \"hi!\",\n",
      "    \"user_name\": \"John Smith\"\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Read standardized content\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "response = model.invoke(\"Explain AI\")\n",
      "\n",
      "for block in response.content_blocks:\n",
      "    if block[\"type\"] == \"reasoning\":\n",
      "        print(block.get(\"reasoning\"))\n",
      "    elif block[\"type\"] == \"text\":\n",
      "        print(block.get(\"text\"))\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> InMemorySaver Checkpointer\n",
      "content===> from langgraph.checkpoint.memory import InMemorySaver\n",
      "\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=[],\n",
      "    checkpointer=InMemorySaver()\n",
      ")\n",
      "\n",
      "# First invocation\n",
      "agent.invoke(HumanMessage(content=\"I live in Sydney, Australia.\"))\n",
      "\n",
      "# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time\n",
      "agent.invoke(HumanMessage(content=\"What's my local time?\"))\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI\n",
      "content===> pip install langchain-google-community[docai]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face Hub Tools\n",
      "content===> from langchain_community.agent_toolkits.load_tools import load_huggingface_tool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom state schema\n",
      "content===> agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    middleware=[CallCounterMiddleware()],\n",
      "    tools=[...],\n",
      ")\n",
      "\n",
      "# Invoke with custom state\n",
      "result = agent.invoke({\n",
      "    \"messages\": [HumanMessage(\"Hello\")],\n",
      "    \"model_call_count\": 0,\n",
      "    \"user_id\": \"user-123\",\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use with any LLM\n",
      "content===> from langgraph.config import get_stream_writer\n",
      "\n",
      "def call_arbitrary_model(state):\n",
      "    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n",
      "    # Get the stream writer to send custom data\n",
      "    writer = get_stream_writer()  \n",
      "    # Assume you have a streaming client that yields chunks\n",
      "    # Generate LLM tokens using your custom streaming client\n",
      "    for chunk in your_custom_streaming_client(state[\"topic\"]):\n",
      "        # Use the writer to send custom data to the stream\n",
      "        writer({\"custom_llm_chunk\": chunk})  \n",
      "    return {\"result\": \"completed\"}\n",
      "\n",
      "graph = (\n",
      "    StateGraph(State)\n",
      "    .add_node(call_arbitrary_model)\n",
      "    # Add other nodes and edges as needed\n",
      "    .compile()\n",
      ")\n",
      "# Set stream_mode=\"custom\" to receive the custom data in the stream\n",
      "for chunk in graph.stream(\n",
      "    {\"topic\": \"cats\"},\n",
      "    stream_mode=\"custom\",  \n",
      "\n",
      "):\n",
      "    # The chunk will contain the custom data streamed from the llm\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cosmos DB NoSQL\n",
      "content===> pip install azure-cosmos\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Generative AI (Gemini API & AI Studio)\n",
      "content===> pip install -U langchain-google-genai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> ================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  check_inventory (call_def456)\n",
      " Call ID: call_def456\n",
      "  Args:\n",
      "    product_id: WH-1000XM5\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PII detection\n",
      "content===> apply_to_tool_results\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Do not return complex values ininterruptcalls\n",
      "content===> def validate_input(value):\n",
      "    return len(value) > 0\n",
      "\n",
      "def node_a(state: State):\n",
      "    # ❌ Bad: passing a function to interrupt\n",
      "    # The function cannot be serialized\n",
      "    response = interrupt({\n",
      "        \"question\": \"What's your name?\",\n",
      "        \"validator\": validate_input  # This will fail\n",
      "    })\n",
      "    return {\"name\": response}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream Writer\n",
      "content===> from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "@tool\n",
      "def get_weather(city: str, runtime: ToolRuntime) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    writer = runtime.stream_writer\n",
      "\n",
      "    # Stream custom updates as the tool executes\n",
      "    writer(f\"Looking up data for city: {city}\")\n",
      "    writer(f\"Acquired data for city: {city}\")\n",
      "\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware3.before_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Resume execution from the checkpoint\n",
      "content===> graph.invoke(None, new_config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add short-term memory\n",
      "content===> from langgraph.checkpoint.memory import InMemorySaver  \n",
      "from langgraph.graph import StateGraph\n",
      "\n",
      "checkpointer = InMemorySaver()  \n",
      "\n",
      "builder = StateGraph(...)\n",
      "graph = builder.compile(checkpointer=checkpointer)  \n",
      "\n",
      "graph.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n",
      "    {\"configurable\": {\"thread_id\": \"1\"}},  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image generator\n",
      "content===> from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Search\n",
      "content===> Azure Cognitive Search\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpoints\n",
      "content===> {'foo': '', 'bar': []}\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU \"langchain[langchain-deepseek]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Read short-term memory in a tool\n",
      "content===> from langchain.agents import create_agent, AgentState\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "\n",
      "class CustomState(AgentState):\n",
      "    user_id: str\n",
      "\n",
      "@tool\n",
      "def get_user_info(\n",
      "    runtime: ToolRuntime\n",
      ") -> str:\n",
      "    \"\"\"Look up user info.\"\"\"\n",
      "    user_id = runtime.state[\"user_id\"]\n",
      "    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[get_user_info],\n",
      "    state_schema=CustomState,\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": \"look up user information\",\n",
      "    \"user_id\": \"user_123\"\n",
      "})\n",
      "print(result[\"messages\"][-1].content)\n",
      "# > User is John Smith.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpoints\n",
      "content===> from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from typing import Annotated\n",
      "from typing_extensions import TypedDict\n",
      "from operator import add\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: str\n",
      "    bar: Annotated[list[str], add]\n",
      "\n",
      "def node_a(state: State):\n",
      "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
      "\n",
      "def node_b(state: State):\n",
      "    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n",
      "\n",
      "\n",
      "workflow = StateGraph(State)\n",
      "workflow.add_node(node_a)\n",
      "workflow.add_node(node_b)\n",
      "workflow.add_edge(START, \"node_a\")\n",
      "workflow.add_edge(\"node_a\", \"node_b\")\n",
      "workflow.add_edge(\"node_b\", END)\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "graph = workflow.compile(checkpointer=checkpointer)\n",
      "\n",
      "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "graph.invoke({\"foo\": \"\"}, config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cosmos DB for Apache Gremlin\n",
      "content===> pip install gremlinpython\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure OpenAI\n",
      "content===> from langchain_openai import AzureChatOpenAI\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Embedding Models\n",
      "content===> from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
      "\n",
      "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
      "vector = embeddings.embed_query(\"What are embeddings?\")\n",
      "print(vector[:5])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multiple structured outputs error\n",
      "content===> from pydantic import BaseModel, Field\n",
      "from typing import Union\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy\n",
      "\n",
      "\n",
      "class ContactInfo(BaseModel):\n",
      "    name: str = Field(description=\"Person's name\")\n",
      "    email: str = Field(description=\"Email address\")\n",
      "\n",
      "class EventDetails(BaseModel):\n",
      "    event_name: str = Field(description=\"Name of the event\")\n",
      "    date: str = Field(description=\"Event date\")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5\",\n",
      "    tools=[],\n",
      "    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True\n",
      ")\n",
      "\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bring-your-own documents\n",
      "content===> ElasticsearchRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text property\n",
      "content===> # Property access\n",
      "text = response.text\n",
      "\n",
      "# Deprecated method call\n",
      "text = response.text()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LangSmith Integration\n",
      "content===> export LANGSMITH_API_KEY=\"your_langsmith_api_key\"\n",
      "export LANGSMITH_TRACING=\"true\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamic prompts\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
      "from langgraph.runtime import Runtime\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class Context:  \n",
      "    user_role: str = \"user\"\n",
      "\n",
      "@dynamic_prompt\n",
      "def dynamic_prompt(request: ModelRequest) -> str:  \n",
      "    user_role = request.runtime.context.user_role\n",
      "    base_prompt = \"You are a helpful assistant.\"\n",
      "\n",
      "    if user_role == \"expert\":\n",
      "        prompt = (\n",
      "            f\"{base_prompt} Provide detailed technical responses.\"\n",
      "        )\n",
      "    elif user_role == \"beginner\":\n",
      "        prompt = (\n",
      "            f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
      "        )\n",
      "    else:\n",
      "        prompt = base_prompt\n",
      "\n",
      "    return prompt  \n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=tools,\n",
      "    middleware=[dynamic_prompt],  \n",
      "    context_schema=Context\n",
      ")\n",
      "\n",
      "# Use with context\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n",
      "    context=Context(user_role=\"expert\")\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Runtime context\n",
      "content===> config[\"configurable\"]\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google\n",
      "content===> langchain-google-genai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Content block reference\n",
      "content===> {\n",
      "    \"type\": \"tool_call\",\n",
      "    \"name\": \"search\",\n",
      "    \"args\": {\"query\": \"weather\"},\n",
      "    \"id\": \"call_123\"\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ToolRuntime\n",
      "content===> from langgraph.types import Command\n",
      "from langchain.messages import RemoveMessage\n",
      "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "# Update the conversation history by removing all messages\n",
      "@tool\n",
      "def clear_conversation() -> Command:\n",
      "    \"\"\"Clear the conversation history.\"\"\"\n",
      "\n",
      "    return Command(\n",
      "        update={\n",
      "            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n",
      "        }\n",
      "    )\n",
      "\n",
      "# Update the user_name in the agent state\n",
      "@tool\n",
      "def update_user_name(\n",
      "    new_name: str,\n",
      "    runtime: ToolRuntime\n",
      ") -> Command:\n",
      "    \"\"\"Update the user's name.\"\"\"\n",
      "    return Command(update={\"user_name\": new_name})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Match decision order to actions\n",
      "content===> if result.get(\"__interrupt__\"):\n",
      "    interrupts = result[\"__interrupt__\"][0].value\n",
      "    action_requests = interrupts[\"action_requests\"]\n",
      "\n",
      "    # Create one decision per action, in order\n",
      "    decisions = []\n",
      "    for action in action_requests:\n",
      "        decision = get_user_decision(action)  # Your logic\n",
      "        decisions.append(decision)\n",
      "\n",
      "    result = agent.invoke(\n",
      "        Command(resume={\"decisions\": decisions}),\n",
      "        config=config\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Co-locate Python and JavaScript/TypeScript content\n",
      "content===> :::python\n",
      "Python-specific content. In real docs, the preceding backslash (before `python`) is omitted.\n",
      ":::\n",
      "\n",
      ":::js\n",
      "JavaScript/TypeScript-specific content. In real docs, the preceding backslash (before `js`) is omitted.\n",
      ":::\n",
      "\n",
      "Content for both languages (not wrapped)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Prepare your agent\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "def send_email(to: str, subject: str, body: str):\n",
      "    \"\"\"Send an email\"\"\"\n",
      "    email = {\n",
      "        \"to\": to,\n",
      "        \"subject\": subject,\n",
      "        \"body\": body\n",
      "    }\n",
      "    # ... email sending logic\n",
      "\n",
      "    return f\"Email sent to {to}\"\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-4o\",\n",
      "    tools=[send_email],\n",
      "    system_prompt=\"You are an email assistant. Always use the send_email tool.\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AlloyDB for PostgreSQL\n",
      "content===> from langchain_google_alloydb_pg import AlloyDBLoader # AlloyDBEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Context\n",
      "content===> from dataclasses import dataclass\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain.agents import create_agent\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "\n",
      "USER_DATABASE = {\n",
      "    \"user123\": {\n",
      "        \"name\": \"Alice Johnson\",\n",
      "        \"account_type\": \"Premium\",\n",
      "        \"balance\": 5000,\n",
      "        \"email\": \"alice@example.com\"\n",
      "    },\n",
      "    \"user456\": {\n",
      "        \"name\": \"Bob Smith\",\n",
      "        \"account_type\": \"Standard\",\n",
      "        \"balance\": 1200,\n",
      "        \"email\": \"bob@example.com\"\n",
      "    }\n",
      "}\n",
      "\n",
      "@dataclass\n",
      "class UserContext:\n",
      "    user_id: str\n",
      "\n",
      "@tool\n",
      "def get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n",
      "    \"\"\"Get the current user's account information.\"\"\"\n",
      "    user_id = runtime.context.user_id\n",
      "\n",
      "    if user_id in USER_DATABASE:\n",
      "        user = USER_DATABASE[user_id]\n",
      "        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n",
      "    return \"User not found\"\n",
      "\n",
      "model = ChatOpenAI(model=\"gpt-4o\")\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=[get_account_info],\n",
      "    context_schema=UserContext,\n",
      "    system_prompt=\"You are a financial assistant.\"\n",
      ")\n",
      "\n",
      "result = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n",
      "    context=UserContext(user_id=\"user123\")\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> config: RunnableConfig\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-chroma\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool Message\n",
      "content===> # After a model makes a tool call\n",
      "ai_message = AIMessage(\n",
      "    content=[],\n",
      "    tool_calls=[{\n",
      "        \"name\": \"get_weather\",\n",
      "        \"args\": {\"location\": \"San Francisco\"},\n",
      "        \"id\": \"call_123\"\n",
      "    }]\n",
      ")\n",
      "\n",
      "# Execute tool and create result message\n",
      "weather_result = \"Sunny, 72°F\"\n",
      "tool_message = ToolMessage(\n",
      "    content=weather_result,\n",
      "    tool_call_id=\"call_123\"  # Must match the call ID\n",
      ")\n",
      "\n",
      "# Continue conversation\n",
      "messages = [\n",
      "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
      "    ai_message,  # Model's tool call\n",
      "    tool_message,  # Tool execution result\n",
      "]\n",
      "response = model.invoke(messages)  # Model processes the result\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarization\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import SummarizationMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[weather_tool, calculator_tool],\n",
      "    middleware=[\n",
      "        SummarizationMiddleware(\n",
      "            model=\"gpt-4o-mini\",\n",
      "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
      "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
      "            summary_prompt=\"Custom prompt for summarization...\",  # Optional\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AI Message\n",
      "content===> response = model.invoke(\"Explain AI\")\n",
      "print(type(response))  # <class 'langchain_core.messages.AIMessage'>\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Installing AgentEvals\n",
      "content===> pip install agentevals\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware2.after_model()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Initialization\n",
      "content===> from langchain_core.vectorstores import InMemoryVectorStore\n",
      "vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> from langchain_google_community import VertexAISearchSummaryTool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bring-your-own documents\n",
      "content===> VertexAISearchRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI\n",
      "content===> from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
      "\n",
      "llm = AzureAIChatCompletionsModel(\n",
      "    model_name=\"gpt-4o\",\n",
      "    api_version=\"2024-05-01-preview\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Node-style hooks\n",
      "content===> from langchain.agents.middleware import AgentMiddleware, AgentState\n",
      "from langchain.messages import AIMessage\n",
      "from langgraph.runtime import Runtime\n",
      "from typing import Any\n",
      "\n",
      "class MessageLimitMiddleware(AgentMiddleware):\n",
      "    def __init__(self, max_messages: int = 50):\n",
      "        super().__init__()\n",
      "        self.max_messages = max_messages\n",
      "\n",
      "    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "        if len(state[\"messages\"]) == self.max_messages:\n",
      "            return {\n",
      "                \"messages\": [AIMessage(\"Conversation limit reached.\")],\n",
      "                \"jump_to\": \"end\"\n",
      "            }\n",
      "        return None\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-elasticsearch\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> values\n",
      "content===> {\"foo\": 1, \"bar\": [\"a\"]}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Create a LangGraph app 🌱\n",
      "content===> new-langgraph-project-python\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Example: Summarization\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import SummarizationMiddleware\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[\n",
      "        SummarizationMiddleware(\n",
      "            model=\"gpt-4o-mini\",\n",
      "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
      "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 2: Set up your API keys\n",
      "content===> export ANTHROPIC_API_KEY=\"your-api-key\"\n",
      "export TAVILY_API_KEY=\"your-tavily-api-key\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Inside middleware\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "from langchain.messages import AnyMessage\n",
      "from langchain.agents import create_agent, AgentState\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\n",
      "from langgraph.runtime import Runtime\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_name: str\n",
      "\n",
      "# Dynamic prompts\n",
      "@dynamic_prompt\n",
      "def dynamic_system_prompt(request: ModelRequest) -> str:\n",
      "    user_name = request.runtime.context.user_name  \n",
      "    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
      "    return system_prompt\n",
      "\n",
      "# Before model hook\n",
      "@before_model\n",
      "def log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  \n",
      "    print(f\"Processing request for user: {runtime.context.user_name}\")  \n",
      "    return None\n",
      "\n",
      "# After model hook\n",
      "@after_model\n",
      "def log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  \n",
      "    print(f\"Completed request for user: {runtime.context.user_name}\")  \n",
      "    return None\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[...],\n",
      "    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  \n",
      "    context_schema=Context\n",
      ")\n",
      "\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
      "    context=Context(user_name=\"John Smith\")\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Development environment\n",
      "content===> cd libs/partners/langchain-{partner}\n",
      "uv sync --all-groups\n",
      "make test  # Ensure tests pass before starting development\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Mistral on Vertex AI Model Garden\n",
      "content===> from langchain_google_vertexai.model_garden_maas.mistral import VertexModelGardenMistral\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Semantic Search\n",
      "content===> # Store with specific fields to embed\n",
      "store.put(\n",
      "    namespace_for_memory,\n",
      "    str(uuid.uuid4()),\n",
      "    {\n",
      "        \"food_preference\": \"I love Italian cuisine\",\n",
      "        \"context\": \"Discussing dinner plans\"\n",
      "    },\n",
      "    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n",
      ")\n",
      "\n",
      "# Store without embedding (still retrievable, but not searchable)\n",
      "store.put(\n",
      "    namespace_for_memory,\n",
      "    str(uuid.uuid4()),\n",
      "    {\"system_info\": \"Last updated: 2024-01-01\"},\n",
      "    index=False\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware2.before_model()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agents\n",
      "content===> from langchain.tools import tool\n",
      "\n",
      "\n",
      "# Define tools\n",
      "@tool\n",
      "def multiply(a: int, b: int) -> int:\n",
      "    \"\"\"Multiply `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "    \"\"\"\n",
      "    return a * b\n",
      "\n",
      "\n",
      "@tool\n",
      "def add(a: int, b: int) -> int:\n",
      "    \"\"\"Adds `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "    \"\"\"\n",
      "    return a + b\n",
      "\n",
      "\n",
      "@tool\n",
      "def divide(a: int, b: int) -> float:\n",
      "    \"\"\"Divide `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "    \"\"\"\n",
      "    return a / b\n",
      "\n",
      "\n",
      "# Augment the LLM with tools\n",
      "tools = [add, multiply, divide]\n",
      "tools_by_name = {tool.name: tool for tool in tools}\n",
      "llm_with_tools = llm.bind_tools(tools)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Firestore (Datastore Mode)\n",
      "content===> pip install langchain-google-datastore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom middleware\n",
      "content===> from dataclasses import dataclass\n",
      "from typing import Callable\n",
      "\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "from langchain.agents.middleware import (\n",
      "    AgentMiddleware,\n",
      "    ModelRequest\n",
      ")\n",
      "from langchain.agents.middleware.types import ModelResponse\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_expertise: str = \"beginner\"\n",
      "\n",
      "class ExpertiseBasedToolMiddleware(AgentMiddleware):\n",
      "    def wrap_model_call(\n",
      "        self,\n",
      "        request: ModelRequest,\n",
      "        handler: Callable[[ModelRequest], ModelResponse]\n",
      "    ) -> ModelResponse:\n",
      "        user_level = request.runtime.context.user_expertise\n",
      "\n",
      "        if user_level == \"expert\":\n",
      "            # More powerful model\n",
      "            model = ChatOpenAI(model=\"gpt-5\")\n",
      "            tools = [advanced_search, data_analysis]\n",
      "        else:\n",
      "            # Less powerful model\n",
      "            model = ChatOpenAI(model=\"gpt-5-nano\")\n",
      "            tools = [simple_search, basic_calculator]\n",
      "\n",
      "        request.model = model\n",
      "        request.tools = tools\n",
      "        return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[\n",
      "        simple_search,\n",
      "        advanced_search,\n",
      "        basic_calculator,\n",
      "        data_analysis\n",
      "    ],\n",
      "    middleware=[ExpertiseBasedToolMiddleware()],\n",
      "    context_schema=Context\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-google-genai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install LangGraph\n",
      "content===> pip install -U langgraph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/install\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install LangChain\n",
      "content===> pip install -U langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/install\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> values\n",
      "content===> from typing import Annotated\n",
      "from typing_extensions import TypedDict\n",
      "from operator import add\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: int\n",
      "    bar: Annotated[list[str], add]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic usage example\n",
      "content===> {'refineTopic': {'topic': 'ice cream and cats'}}\n",
      "{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> yield_keys(prefix: Optional[str] = None) -> Iterator[str]\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AlloyDB for PostgreSQL\n",
      "content===> pip install langchain-google-alloydb-pg\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_postgres import PGVector\n",
      "\n",
      "vector_store = PGVector(\n",
      "    embeddings=embeddings,\n",
      "    collection_name=\"my_docs\",\n",
      "    connection=\"postgresql+psycopg://...\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream multiple modes\n",
      "content===> stream_mode=[\"updates\", \"custom\"]\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Implementing our email agent nodes\n",
      "content===> def search_documentation(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n",
      "    \"\"\"Search knowledge base for relevant information\"\"\"\n",
      "\n",
      "    # Build search query from classification\n",
      "    classification = state.get('classification', {})\n",
      "    query = f\"{classification.get('intent', '')} {classification.get('topic', '')}\"\n",
      "\n",
      "    try:\n",
      "        # Implement your search logic here\n",
      "        # Store raw search results, not formatted text\n",
      "        search_results = [\n",
      "            \"Reset password via Settings > Security > Change Password\",\n",
      "            \"Password must be at least 12 characters\",\n",
      "            \"Include uppercase, lowercase, numbers, and symbols\"\n",
      "        ]\n",
      "    except SearchAPIError as e:\n",
      "        # For recoverable search errors, store error and continue\n",
      "        search_results = [f\"Search temporarily unavailable: {str(e)}\"]\n",
      "\n",
      "    return Command(\n",
      "        update={\"search_results\": search_results},  # Store raw results or error\n",
      "        goto=\"draft_response\"\n",
      "    )\n",
      "\n",
      "def bug_tracking(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n",
      "    \"\"\"Create or update bug tracking ticket\"\"\"\n",
      "\n",
      "    # Create ticket in your bug tracking system\n",
      "    ticket_id = \"BUG-12345\"  # Would be created via API\n",
      "\n",
      "    return Command(\n",
      "        update={\n",
      "            \"search_results\": [f\"Bug ticket {ticket_id} created\"],\n",
      "            \"current_step\": \"bug_tracked\"\n",
      "        },\n",
      "        goto=\"draft_response\"\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Best practices\n",
      "content===> SummarizationMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use persistent stores in production\n",
      "content===> # ❌ Development only - data lost on restart\n",
      "store = InMemoryStore()\n",
      "\n",
      "# ✅ Production - data persists\n",
      "from langgraph.store.postgres import PostgresStore\n",
      "store = PostgresStore(connection_string=os.environ[\"DATABASE_URL\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bigtable\n",
      "content===> from langchain_google_bigtable import BigtableLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Write long-term memory from tools\n",
      "content===> from dataclasses import dataclass\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "\n",
      "# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\n",
      "store = InMemoryStore() \n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_id: str\n",
      "\n",
      "# TypedDict defines the structure of user information for the LLM\n",
      "class UserInfo(TypedDict):\n",
      "    name: str\n",
      "\n",
      "# Tool that allows agent to update user information (useful for chat applications)\n",
      "@tool\n",
      "def save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n",
      "    \"\"\"Save user info.\"\"\"\n",
      "    # Access the store - same as that provided to `create_agent`\n",
      "    store = runtime.store \n",
      "    user_id = runtime.context.user_id \n",
      "    # Store data in the store (namespace, key, data)\n",
      "    store.put((\"users\",), user_id, user_info) \n",
      "    return \"Successfully saved user info.\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[save_user_info],\n",
      "    store=store, \n",
      "    context_schema=Context\n",
      ")\n",
      "\n",
      "# Run the agent\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n",
      "    # user_id passed in context to identify whose information is being updated\n",
      "    context=Context(user_id=\"user_123\") \n",
      ")\n",
      "\n",
      "# You can access the store directly to get the value\n",
      "store.get((\"users\",), \"user_123\").value\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic usage example\n",
      "content===> for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bing Search API\n",
      "content===> from langchain_community.utilities import BingSearchAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> BigQuery Vector Search\n",
      "content===> pip install google-cloud-bigquery\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Read long-term memory in tools\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from langchain.agents import create_agent\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_id: str\n",
      "\n",
      "# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\n",
      "store = InMemoryStore() \n",
      "\n",
      "# Write sample data to the store using the put method\n",
      "store.put( \n",
      "    (\"users\",),  # Namespace to group related data together (users namespace for user data)\n",
      "    \"user_123\",  # Key within the namespace (user ID as key)\n",
      "    {\n",
      "        \"name\": \"John Smith\",\n",
      "        \"language\": \"English\",\n",
      "    }  # Data to store for the given user\n",
      ")\n",
      "\n",
      "@tool\n",
      "def get_user_info(runtime: ToolRuntime[Context]) -> str:\n",
      "    \"\"\"Look up user info.\"\"\"\n",
      "    # Access the store - same as that provided to `create_agent`\n",
      "    store = runtime.store \n",
      "    user_id = runtime.context.user_id\n",
      "    # Retrieve data from store - returns StoreValue object with value and metadata\n",
      "    user_info = store.get((\"users\",), user_id) \n",
      "    return str(user_info.value) if user_info else \"Unknown user\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_user_info],\n",
      "    # Pass store to agent - enables agent to access store when running tools\n",
      "    store=store, \n",
      "    context_schema=Context\n",
      ")\n",
      "\n",
      "# Run the agent\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n",
      "    context=Context(user_id=\"user_123\") \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Specify a backend\n",
      "content===> lambda rt: StateBackend(rt)\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ================================= Tool Message =================================\n",
      "Name: ToolStrategy\n",
      "\n",
      "Error: <error message>\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI Warehouse\n",
      "content===> DocumentAIWarehouseRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool call limit\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
      "\n",
      "\n",
      "# Limit all tool calls\n",
      "global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n",
      "\n",
      "# Limit specific tool\n",
      "search_limiter = ToolCallLimitMiddleware(\n",
      "    tool_name=\"search\",\n",
      "    thread_limit=5,\n",
      "    run_limit=3,\n",
      ")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[global_limiter, search_limiter],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add metadata to traces\n",
      "content===> with ls.tracing_context(\n",
      "    project_name=\"email-agent-test\",\n",
      "    enabled=True,\n",
      "    tags=[\"production\", \"email-assistant\", \"v1.0\"],\n",
      "    metadata={\"user_id\": \"user_123\", \"session_id\": \"session_456\", \"environment\": \"production\"}):\n",
      "    response = agent.invoke(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-mongodb\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> from langchain_google_community import VertexAIMultiTurnSearchRetriever\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream graph state\n",
      "content===> for chunk in graph.stream(\n",
      "    {\"topic\": \"ice cream\"},\n",
      "    stream_mode=\"updates\",  \n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Lens\n",
      "content===> from langchain_community.tools.google_lens import GoogleLensQueryRun\n",
      "from langchain_community.utilities.google_lens import GoogleLensAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text-to-Speech\n",
      "content===> pip install google-cloud-text-to-speech langchain-google-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
      "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
      "\n",
      "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
      "\n",
      "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Code examples\n",
      "content===> def filter_unknown_users(users: list[str], known_users: set[str]) -> list[str]:\n",
      "    \"\"\"Filter out users that are not in the known users set.\n",
      "\n",
      "    Args:\n",
      "        users: List of user identifiers to filter.\n",
      "        known_users: Set of known/valid user identifiers.\n",
      "\n",
      "    Returns:\n",
      "        List of users that are not in the known_users set.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If users list contains invalid identifiers.\n",
      "    \"\"\"\n",
      "    return [user for user in users if user not in known_users]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream\n",
      "content===> full = None  # None | AIMessageChunk\n",
      "for chunk in model.stream(\"What color is the sky?\"):\n",
      "    full = chunk if full is None else full + chunk\n",
      "    print(full.text)\n",
      "\n",
      "# The\n",
      "# The sky\n",
      "# The sky is\n",
      "# The sky is typically\n",
      "# The sky is typically blue\n",
      "# ...\n",
      "\n",
      "print(full.content_blocks)\n",
      "# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Where to customize\n",
      "content===> \"subagent1_description\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Storage\n",
      "content===> from langchain_google_community import GCSFileLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> ((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), {'node_2': {'foo': 'hi! foobar'}})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceInferenceAPIEmbeddings\n",
      "content===> from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI\n",
      "content===> pip install -U langchain-azure-ai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpoints\n",
      "content===> {'foo': 'b', 'bar': ['a', 'b']}\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. Test your application in Studio\n",
      "content===> >    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 1. Run the graph\n",
      "content===> How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!\n",
      "\n",
      "# The Secret Life of Socks in the Dryer\n",
      "\n",
      "I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.\n",
      "\n",
      "My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Procedural memory\n",
      "content===> # Node that *uses* the instructions\n",
      "def call_model(state: State, store: BaseStore):\n",
      "    namespace = (\"agent_instructions\", )\n",
      "    instructions = store.get(namespace, key=\"agent_a\")[0]\n",
      "    # Application logic\n",
      "    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n",
      "    ...\n",
      "\n",
      "# Node that updates instructions\n",
      "def update_instructions(state: State, store: BaseStore):\n",
      "    namespace = (\"instructions\",)\n",
      "    instructions = store.search(namespace)[0]\n",
      "    # Memory logic\n",
      "    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n",
      "    output = llm.invoke(prompt)\n",
      "    new_instructions = output['new_instructions']\n",
      "    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n",
      "    ...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Data\n",
      "content===> Azure Data Lake gen 2\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> High-level API\n",
      "content===> {'topic': <langgraph.channels.last_value.LastValue at 0x7d05e3294d80>,\n",
      " 'content': <langgraph.channels.last_value.LastValue at 0x7d05e3295040>,\n",
      " 'score': <langgraph.channels.last_value.LastValue at 0x7d05e3295980>,\n",
      " '__start__': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3297e00>,\n",
      " 'write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32960c0>,\n",
      " 'score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ab80>,\n",
      " 'branch:__start__:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32941c0>,\n",
      " 'branch:__start__:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d88800>,\n",
      " 'branch:write_essay:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3295ec0>,\n",
      " 'branch:write_essay:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ac00>,\n",
      " 'branch:score_essay:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d89700>,\n",
      " 'branch:score_essay:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b400>,\n",
      " 'start:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b280>}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install\n",
      "content===> langchain-mcp-adapters\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services individual tools\n",
      "content===> AzureCogsFormRecognizerTool\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> El Carro for Oracle Workloads\n",
      "content===> pip install langchain-google-el-carro\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_pinecone import PineconeVectorStore\n",
      "from pinecone import Pinecone\n",
      "\n",
      "pc = Pinecone(api_key=...)\n",
      "index = pc.Index(index_name)\n",
      "\n",
      "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> from langchain.messages import RemoveMessage\n",
      "from langchain.agents import create_agent, AgentState\n",
      "from langchain.agents.middleware import after_model\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langgraph.runtime import Runtime\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "\n",
      "\n",
      "@after_model\n",
      "def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n",
      "    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n",
      "    messages = state[\"messages\"]\n",
      "    if len(messages) > 2:\n",
      "        # remove the earliest two messages\n",
      "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
      "    return None\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-5-nano\",\n",
      "    tools=[],\n",
      "    system_prompt=\"Please be concise and to the point.\",\n",
      "    middleware=[delete_old_messages],\n",
      "    checkpointer=InMemorySaver(),\n",
      ")\n",
      "\n",
      "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "for event in agent.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
      "    config,\n",
      "    stream_mode=\"values\",\n",
      "):\n",
      "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
      "\n",
      "for event in agent.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
      "    config,\n",
      "    stream_mode=\"values\",\n",
      "):\n",
      "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> The /memories/ path convention\n",
      "content===> # Transient file (lost after thread ends)\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Write draft to /draft.txt\"}]\n",
      "})\n",
      "\n",
      "# Persistent file (survives across threads)\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Save final report to /memories/report.txt\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Drive\n",
      "content===> from langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper\n",
      "from langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Batch\n",
      "content===> responses = model.batch([\n",
      "    \"Why do parrots have colorful feathers?\",\n",
      "    \"How do airplanes fly?\",\n",
      "    \"What is quantum computing?\"\n",
      "])\n",
      "for response in responses:\n",
      "    print(response)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bedrock Chat\n",
      "content===> from langchain_aws import ChatBedrock\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Encryption\n",
      "content===> import sqlite3\n",
      "\n",
      "from langgraph.checkpoint.serde.encrypted import EncryptedSerializer\n",
      "from langgraph.checkpoint.sqlite import SqliteSaver\n",
      "\n",
      "serde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\n",
      "checkpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AlloyDB for PostgreSQL\n",
      "content===> from langchain_google_alloydb_pg import AlloyDBVectorStore # AlloyDBEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content blocks\n",
      "content===> ReasoningContentBlock\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Best practices\n",
      "content===> LLMToolSelectorMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> import os\n",
      "import getpass\n",
      "\n",
      "from langchain_anthropic import ChatAnthropic\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "\n",
      "_set_env(\"ANTHROPIC_API_KEY\")\n",
      "\n",
      "llm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured outputs\n",
      "content===> from pydantic import BaseModel, Field\n",
      "\n",
      "class Actor(BaseModel):\n",
      "    name: str\n",
      "    role: str\n",
      "\n",
      "class MovieDetails(BaseModel):\n",
      "    title: str\n",
      "    year: int\n",
      "    cast: list[Actor]\n",
      "    genres: list[str]\n",
      "    budget: float | None = Field(None, description=\"Budget in millions USD\")\n",
      "\n",
      "model_with_structure = model.with_structured_output(MovieDetails)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic usage example\n",
      "content===> from typing import TypedDict\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "\n",
      "class State(TypedDict):\n",
      "    topic: str\n",
      "    joke: str\n",
      "\n",
      "def refine_topic(state: State):\n",
      "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
      "\n",
      "def generate_joke(state: State):\n",
      "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
      "\n",
      "graph = (\n",
      "    StateGraph(State)\n",
      "    .add_node(refine_topic)\n",
      "    .add_node(generate_joke)\n",
      "    .add_edge(START, \"refine_topic\")\n",
      "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
      "    .add_edge(\"generate_joke\", END)\n",
      "    .compile()\n",
      ")\n",
      "\n",
      "# The stream() method returns an iterator that yields streamed outputs\n",
      "for chunk in graph.stream(  \n",
      "    {\"topic\": \"ice cream\"},\n",
      "    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n",
      "    # Other stream modes are also available. See supported stream modes for details\n",
      "    stream_mode=\"updates\",  \n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.tools import tool\n",
      "from langchain.messages import HumanMessage, AIMessage, ToolMessage\n",
      "from agentevals.trajectory.match import create_trajectory_match_evaluator\n",
      "\n",
      "\n",
      "@tool\n",
      "def get_weather(city: str):\n",
      "    \"\"\"Get weather information for a city.\"\"\"\n",
      "    return f\"It's 75 degrees and sunny in {city}.\"\n",
      "\n",
      "@tool\n",
      "def get_events(city: str):\n",
      "    \"\"\"Get events happening in a city.\"\"\"\n",
      "    return f\"Concert at the park in {city} tonight.\"\n",
      "\n",
      "agent = create_agent(\"gpt-4o\", tools=[get_weather, get_events])\n",
      "\n",
      "evaluator = create_trajectory_match_evaluator(  \n",
      "    trajectory_match_mode=\"unordered\",  \n",
      ")  \n",
      "\n",
      "def test_multiple_tools_any_order():\n",
      "    result = agent.invoke({\n",
      "        \"messages\": [HumanMessage(content=\"What's happening in SF today?\")]\n",
      "    })\n",
      "\n",
      "    # Reference shows tools called in different order than actual execution\n",
      "    reference_trajectory = [\n",
      "        HumanMessage(content=\"What's happening in SF today?\"),\n",
      "        AIMessage(content=\"\", tool_calls=[\n",
      "            {\"id\": \"call_1\", \"name\": \"get_events\", \"args\": {\"city\": \"SF\"}},\n",
      "            {\"id\": \"call_2\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n",
      "        ]),\n",
      "        ToolMessage(content=\"Concert at the park in SF tonight.\", tool_call_id=\"call_1\"),\n",
      "        ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_2\"),\n",
      "        AIMessage(content=\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n",
      "    ]\n",
      "\n",
      "    evaluation = evaluator(\n",
      "        outputs=result[\"messages\"],\n",
      "        reference_outputs=reference_trajectory,\n",
      "    )\n",
      "    # {\n",
      "    #     'key': 'trajectory_unordered_match',\n",
      "    #     'score': True,\n",
      "    # }\n",
      "    assert evaluation[\"score\"] is True\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Disable streaming for specific chat models\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\n",
      "    \"claude-sonnet-4-5-20250929\",\n",
      "    # Set disable_streaming=True to disable streaming for the chat model\n",
      "    disable_streaming=True\n",
      "\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> View thread state\n",
      "content===> StateSnapshot(\n",
      "    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n",
      "    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n",
      "    metadata={\n",
      "        'source': 'loop',\n",
      "        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n",
      "        'step': 4,\n",
      "        'parents': {},\n",
      "        'thread_id': '1'\n",
      "    },\n",
      "    created_at='2025-05-05T16:01:24.680462+00:00',\n",
      "    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n",
      "    tasks=(),\n",
      "    interrupts=()\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Vector Search\n",
      "content===> Vertex AI Matching Engine\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for SQL Server\n",
      "content===> pip install langchain-google-cloud-sql-mssql\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> from langgraph.store.postgres import PostgresStore\n",
      "\n",
      "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
      "with PostgresStore.from_conn_string(DB_URI) as store:  \n",
      "    builder = StateGraph(...)\n",
      "    graph = builder.compile(store=store)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bing Search\n",
      "content===> from langchain_community.tools.bing_search import BingSearchResults\n",
      "from langchain_community.utilities import BingSearchAPIWrapper\n",
      "\n",
      "api_wrapper = BingSearchAPIWrapper()\n",
      "tool = BingSearchResults(api_wrapper=api_wrapper)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Write short-term memory from tools\n",
      "content===> from langchain.tools import tool, ToolRuntime\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from langchain.messages import ToolMessage\n",
      "from langchain.agents import create_agent, AgentState\n",
      "from langgraph.types import Command\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class CustomState(AgentState):  \n",
      "    user_name: str\n",
      "\n",
      "class CustomContext(BaseModel):\n",
      "    user_id: str\n",
      "\n",
      "@tool\n",
      "def update_user_info(\n",
      "    runtime: ToolRuntime[CustomContext, CustomState],\n",
      ") -> Command:\n",
      "    \"\"\"Look up and update user info.\"\"\"\n",
      "    user_id = runtime.context.user_id  \n",
      "    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
      "    return Command(update={\n",
      "        \"user_name\": name,\n",
      "        # update the message history\n",
      "        \"messages\": [\n",
      "            ToolMessage(\n",
      "                \"Successfully looked up user information\",\n",
      "                tool_call_id=runtime.tool_call_id\n",
      "            )\n",
      "        ]\n",
      "    })\n",
      "\n",
      "@tool\n",
      "def greet(\n",
      "    runtime: ToolRuntime[CustomContext, CustomState]\n",
      ") -> str:\n",
      "    \"\"\"Use this to greet the user once you found their info.\"\"\"\n",
      "    user_name = runtime.state[\"user_name\"]\n",
      "    return f\"Hello {user_name}!\"\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[update_user_info, greet],\n",
      "    state_schema=CustomState,\n",
      "    context_schema=CustomContext,  \n",
      ")\n",
      "\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n",
      "    context=CustomContext(user_id=\"user_123\"),\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add a graph as a node\n",
      "content===> from typing_extensions import TypedDict\n",
      "from langgraph.graph.state import StateGraph, START\n",
      "\n",
      "# Define subgraph\n",
      "class SubgraphState(TypedDict):\n",
      "    foo: str  # shared with parent graph state\n",
      "    bar: str  # private to SubgraphState\n",
      "\n",
      "def subgraph_node_1(state: SubgraphState):\n",
      "    return {\"bar\": \"bar\"}\n",
      "\n",
      "def subgraph_node_2(state: SubgraphState):\n",
      "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
      "    # and is sending update on the shared state key ('foo')\n",
      "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
      "\n",
      "subgraph_builder = StateGraph(SubgraphState)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_node(subgraph_node_2)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Define parent graph\n",
      "class ParentState(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "def node_1(state: ParentState):\n",
      "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
      "\n",
      "builder = StateGraph(ParentState)\n",
      "builder.add_node(\"node_1\", node_1)\n",
      "builder.add_node(\"node_2\", subgraph)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "builder.add_edge(\"node_1\", \"node_2\")\n",
      "graph = builder.compile()\n",
      "\n",
      "for chunk in graph.stream({\"foo\": \"foo\"}):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invoke a graph from a node\n",
      "content===> ((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n",
      "(('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_2': {'bar': 'hi! foobaz'}})\n",
      "((), {'node_2': {'foo': 'hi! foobaz'}})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> grep_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Chat models\n",
      "content===> ChatGoogleGenerativeAI\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using SubAgent\n",
      "content===> import os\n",
      "from typing import Literal\n",
      "from tavily import TavilyClient\n",
      "from deepagents import create_deep_agent\n",
      "\n",
      "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
      "\n",
      "def internet_search(\n",
      "    query: str,\n",
      "    max_results: int = 5,\n",
      "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
      "    include_raw_content: bool = False,\n",
      "):\n",
      "    \"\"\"Run a web search\"\"\"\n",
      "    return tavily_client.search(\n",
      "        query,\n",
      "        max_results=max_results,\n",
      "        include_raw_content=include_raw_content,\n",
      "        topic=topic,\n",
      "    )\n",
      "\n",
      "research_subagent = {\n",
      "    \"name\": \"research-agent\",\n",
      "    \"description\": \"Used to research more in depth questions\",\n",
      "    \"system_prompt\": \"You are a great researcher\",\n",
      "    \"tools\": [internet_search],\n",
      "    \"model\": \"openai:gpt-4o\",  # Optional override, defaults to main agent model\n",
      "}\n",
      "subagents = [research_subagent]\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    subagents=subagents\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Base URL or proxy\n",
      "content===> model = init_chat_model(\n",
      "    model=\"MODEL_NAME\",\n",
      "    model_provider=\"openai\",\n",
      "    base_url=\"BASE_URL\",\n",
      "    api_key=\"YOUR_API_KEY\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Short-term vs. long-term filesystem\n",
      "content===> from langchain.agents import create_agent\n",
      "from deepagents.middleware import FilesystemMiddleware\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "store = InMemoryStore()\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    store=store,\n",
      "    middleware=[\n",
      "        FilesystemMiddleware(\n",
      "            long_term_memory=True,\n",
      "            custom_tool_descriptions={\n",
      "                \"ls\": \"Use the ls tool when...\",\n",
      "                \"read_file\": \"Use the read_file tool to...\"\n",
      "            }  # Optional: Custom descriptions for filesystem tools\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> @pytest.mark.vcr()\n",
      "def test_agent_trajectory():\n",
      "    # ...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamic model\n",
      "content===> from langchain_openai import ChatOpenAI\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
      "\n",
      "\n",
      "basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "advanced_model = ChatOpenAI(model=\"gpt-4o\")\n",
      "\n",
      "@wrap_model_call\n",
      "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
      "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
      "    message_count = len(request.state[\"messages\"])\n",
      "\n",
      "    if message_count > 10:\n",
      "        # Use an advanced model for longer conversations\n",
      "        model = advanced_model\n",
      "    else:\n",
      "        model = basic_model\n",
      "\n",
      "    request.model = model\n",
      "    return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=basic_model,  # Default model\n",
      "    tools=tools,\n",
      "    middleware=[dynamic_model_selection]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PII detection\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import PIIMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[\n",
      "        # Redact emails in user input\n",
      "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
      "        # Mask credit cards (show last 4 digits)\n",
      "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
      "        # Custom PII type with regex\n",
      "        PIIMiddleware(\n",
      "            \"api_key\",\n",
      "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
      "            strategy=\"block\",  # Raise error if detected\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deleting documents\n",
      "content===> vector_store.delete(ids=[\"id1\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> [\n",
      "    {\n",
      "        'name': 'GetPopulation',\n",
      "        'args': {'location': 'Los Angeles, CA'},\n",
      "        'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\n",
      "        'type': 'tool_call'\n",
      "    },\n",
      "    {\n",
      "        'name': 'GetPopulation',\n",
      "        'args': {'location': 'New York, NY'},\n",
      "        'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n",
      "        'type': 'tool_call'\n",
      "    }\n",
      "]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Control the output from the subagent\n",
      "content===> from typing import Annotated\n",
      "from langchain.agents import AgentState\n",
      "from langchain.tools import InjectedToolCallId\n",
      "from langgraph.types import Command\n",
      "\n",
      "\n",
      "@tool(\n",
      "    \"subagent1_name\",\n",
      "    description=\"subagent1_description\"\n",
      ")\n",
      "# We need to pass the `tool_call_id` to the sub agent so it can use it to respond with the tool call result\n",
      "def call_subagent1(\n",
      "    query: str,\n",
      "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
      "# You need to return a `Command` object to include more than just a final tool call\n",
      ") -> Command:\n",
      "    result = subagent1.invoke({\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
      "    })\n",
      "    return Command(update={\n",
      "        # This is the example state key we are passing back\n",
      "        \"example_state_key\": result[\"example_state_key\"],\n",
      "        \"messages\": [\n",
      "            ToolMessage(\n",
      "                content=result[\"messages\"][-1].content,\n",
      "                # We need to include the tool call id so it matches up with the right tool call\n",
      "                tool_call_id=tool_call_id\n",
      "            )\n",
      "        ]\n",
      "    })\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> ================================= Tool Message =================================\n",
      "\n",
      "Product WH-1000XM5: 10 units in stock\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Inside tools\n",
      "content===> from dataclasses import dataclass\n",
      "from langchain.tools import tool, ToolRuntime  \n",
      "\n",
      "@dataclass\n",
      "class Context:\n",
      "    user_id: str\n",
      "\n",
      "@tool\n",
      "def fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  \n",
      "    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n",
      "    user_id = runtime.context.user_id  \n",
      "\n",
      "    preferences: str = \"The user prefers you to write a brief and polite email.\"\n",
      "    if runtime.store:  \n",
      "        if memory := runtime.store.get((\"users\",), user_id):  \n",
      "            preferences = memory.value[\"preferences\"]\n",
      "\n",
      "    return preferences\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"PPLX_API_KEY\"):\n",
      "  os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\"Enter API key for Perplexity: \")\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"llama-3.1-sonar-small-128k-online\", model_provider=\"perplexity\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Identify a checkpoint\n",
      "content===> ('write_joke',)\n",
      "{'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFacePipeline\n",
      "content===> from langchain_huggingface import HuggingFacePipeline\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream graph state\n",
      "content===> from typing import TypedDict\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "\n",
      "\n",
      "class State(TypedDict):\n",
      "  topic: str\n",
      "  joke: str\n",
      "\n",
      "\n",
      "def refine_topic(state: State):\n",
      "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
      "\n",
      "\n",
      "def generate_joke(state: State):\n",
      "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
      "\n",
      "graph = (\n",
      "  StateGraph(State)\n",
      "  .add_node(refine_topic)\n",
      "  .add_node(generate_joke)\n",
      "  .add_edge(START, \"refine_topic\")\n",
      "  .add_edge(\"refine_topic\", \"generate_joke\")\n",
      "  .add_edge(\"generate_joke\", END)\n",
      "  .compile()\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> In production\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "from langgraph.checkpoint.postgres import PostgresSaver  \n",
      "\n",
      "\n",
      "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
      "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
      "    checkpointer.setup() # auto create tables in PostgresSql\n",
      "    agent = create_agent(\n",
      "        \"gpt-5\",\n",
      "        [get_user_info],\n",
      "        checkpointer=checkpointer,  \n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> After model\n",
      "content===> from langchain.messages import RemoveMessage\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langchain.agents import create_agent, AgentState\n",
      "from langchain.agents.middleware import after_model\n",
      "from langgraph.runtime import Runtime\n",
      "\n",
      "\n",
      "@after_model\n",
      "def validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n",
      "    \"\"\"Remove messages containing sensitive words.\"\"\"\n",
      "    STOP_WORDS = [\"password\", \"secret\"]\n",
      "    last_message = state[\"messages\"][-1]\n",
      "    if any(word in last_message.content for word in STOP_WORDS):\n",
      "        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n",
      "    return None\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[],\n",
      "    middleware=[validate_response],\n",
      "    checkpointer=InMemorySaver(),\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Search\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware2.before_agent()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> AWS S3 Directory and File\n",
      "content===> from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Validating human input\n",
      "content===> from langgraph.types import interrupt\n",
      "\n",
      "def get_age_node(state: State):\n",
      "    prompt = \"What is your age?\"\n",
      "\n",
      "    while True:\n",
      "        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n",
      "\n",
      "        # Validate the input\n",
      "        if isinstance(answer, int) and answer > 0:\n",
      "            # Valid input - continue\n",
      "            break\n",
      "        else:\n",
      "            # Invalid input - ask again with a more specific prompt\n",
      "            prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n",
      "\n",
      "    return {\"age\": answer}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Example: Summarization\n",
      "content===> SummarizationMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.tools import tool\n",
      "from langchain.messages import HumanMessage, AIMessage, ToolMessage\n",
      "from agentevals.trajectory.match import create_trajectory_match_evaluator\n",
      "\n",
      "\n",
      "@tool\n",
      "def get_weather(city: str):\n",
      "    \"\"\"Get weather information for a city.\"\"\"\n",
      "    return f\"It's 75 degrees and sunny in {city}.\"\n",
      "\n",
      "@tool\n",
      "def get_detailed_forecast(city: str):\n",
      "    \"\"\"Get detailed weather forecast for a city.\"\"\"\n",
      "    return f\"Detailed forecast for {city}: sunny all week.\"\n",
      "\n",
      "agent = create_agent(\"gpt-4o\", tools=[get_weather, get_detailed_forecast])\n",
      "\n",
      "evaluator = create_trajectory_match_evaluator(  \n",
      "    trajectory_match_mode=\"superset\",  \n",
      ")  \n",
      "\n",
      "def test_agent_calls_required_tools_plus_extra():\n",
      "    result = agent.invoke({\n",
      "        \"messages\": [HumanMessage(content=\"What's the weather in Boston?\")]\n",
      "    })\n",
      "\n",
      "    # Reference only requires get_weather, but agent may call additional tools\n",
      "    reference_trajectory = [\n",
      "        HumanMessage(content=\"What's the weather in Boston?\"),\n",
      "        AIMessage(content=\"\", tool_calls=[\n",
      "            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"Boston\"}},\n",
      "        ]),\n",
      "        ToolMessage(content=\"It's 75 degrees and sunny in Boston.\", tool_call_id=\"call_1\"),\n",
      "        AIMessage(content=\"The weather in Boston is 75 degrees and sunny.\"),\n",
      "    ]\n",
      "\n",
      "    evaluation = evaluator(\n",
      "        outputs=result[\"messages\"],\n",
      "        reference_outputs=reference_trajectory,\n",
      "    )\n",
      "    # {\n",
      "    #     'key': 'trajectory_superset_match',\n",
      "    #     'score': True,\n",
      "    #     'comment': None,\n",
      "    # }\n",
      "    assert evaluation[\"score\"] is True\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cosmos DB NoSQL\n",
      "content===> from langchain_community.vectorstores import AzureCosmosDBNoSQLVectorSearch\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Disable streaming for specific chat models\n",
      "content===> disable_streaming=True\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ================================= Tool Message =================================\n",
      "Name: ToolStrategy\n",
      "\n",
      "Multiple structured outputs were returned. Pick the most relevant one.\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Filesystem middleware\n",
      "content===> from langchain.agents import create_agent\n",
      "from deepagents.middleware.filesystem import FilesystemMiddleware\n",
      "\n",
      "# FilesystemMiddleware is included by default in create_deep_agent\n",
      "# You can customize it if building a custom agent\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    middleware=[\n",
      "        FilesystemMiddleware(\n",
      "            long_term_memory=False,  # Enables access to long-term memory, defaults to False. You must attach a store to use long-term memory.\n",
      "            system_prompt=\"Write to the filesystem when...\",  # Optional custom addition to the system prompt\n",
      "            custom_tool_descriptions={\n",
      "                \"ls\": \"Use the ls tool when...\",\n",
      "                \"read_file\": \"Use the read_file tool to...\"\n",
      "            }  # Optional: Custom descriptions for filesystem tools\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Mocking Chat Model\n",
      "content===> model.invoke(\"hello, again!\")\n",
      "# AIMessage(content='bar', ...)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete all checkpoints for a thread\n",
      "content===> thread_id = \"1\"\n",
      "checkpointer.delete_thread(thread_id)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Knowledge base\n",
      "content===> # Conversation 1: Learn about a project\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"We're building a web app with React. Save project notes.\"}]\n",
      "})\n",
      "\n",
      "# Conversation 2: Use that knowledge\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"What framework are we using?\"}]\n",
      "})\n",
      "# Agent reads /memories/project_notes.txt from previous conversation\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> for chunk in graph.stream(\n",
      "    {\"foo\": \"foo\"},\n",
      "    # Set subgraphs=True to stream outputs from subgraphs\n",
      "    subgraphs=True,  \n",
      "    stream_mode=\"updates\",\n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_ollama import OllamaEmbeddings\n",
      "\n",
      "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Define state\n",
      "content===> from langchain.messages import AnyMessage\n",
      "from typing_extensions import TypedDict, Annotated\n",
      "import operator\n",
      "\n",
      "\n",
      "class MessagesState(TypedDict):\n",
      "    messages: Annotated[list[AnyMessage], operator.add]\n",
      "    llm_calls: int\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 1. Define tools and model\n",
      "content===> from langchain.tools import tool\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "\n",
      "model = init_chat_model(\n",
      "    \"claude-sonnet-4-5-20250929\",\n",
      "    temperature=0\n",
      ")\n",
      "\n",
      "\n",
      "# Define tools\n",
      "@tool\n",
      "def multiply(a: int, b: int) -> int:\n",
      "    \"\"\"Multiply `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "    \"\"\"\n",
      "    return a * b\n",
      "\n",
      "\n",
      "@tool\n",
      "def add(a: int, b: int) -> int:\n",
      "    \"\"\"Adds `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "    \"\"\"\n",
      "    return a + b\n",
      "\n",
      "\n",
      "@tool\n",
      "def divide(a: int, b: int) -> float:\n",
      "    \"\"\"Divide `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "    \"\"\"\n",
      "    return a / b\n",
      "\n",
      "\n",
      "# Augment the LLM with tools\n",
      "tools = [add, multiply, divide]\n",
      "tools_by_name = {tool.name: tool for tool in tools}\n",
      "model_with_tools = model.bind_tools(tools)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gmail\n",
      "content===> from langchain_google_community import GMailLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-qdrant\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add long-term memory\n",
      "content===> from langgraph.store.memory import InMemoryStore  \n",
      "from langgraph.graph import StateGraph\n",
      "\n",
      "store = InMemoryStore()  \n",
      "\n",
      "builder = StateGraph(...)\n",
      "graph = builder.compile(store=store)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Create a.envfile\n",
      "content===> LANGSMITH_API_KEY=lsv2...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content\n",
      "content===> message.content_blocks\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Example block shapes\n",
      "content===> # Text block\n",
      "text_block = {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Hello world\",\n",
      "}\n",
      "\n",
      "# Image block\n",
      "image_block = {\n",
      "    \"type\": \"image\",\n",
      "    \"url\": \"https://example.com/image.png\",\n",
      "    \"mime_type\": \"image/png\",\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Wrap-style hooks\n",
      "content===> from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
      "from langchain.chat_models import init_chat_model\n",
      "from typing import Callable\n",
      "\n",
      "class DynamicModelMiddleware(AgentMiddleware):\n",
      "    def wrap_model_call(\n",
      "        self,\n",
      "        request: ModelRequest,\n",
      "        handler: Callable[[ModelRequest], ModelResponse],\n",
      "    ) -> ModelResponse:\n",
      "        # Use different model based on conversation length\n",
      "        if len(request.messages) > 10:\n",
      "            request.model = init_chat_model(\"gpt-4o\")\n",
      "        else:\n",
      "            request.model = init_chat_model(\"gpt-4o-mini\")\n",
      "\n",
      "        return handler(request)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add metadata to traces\n",
      "content===> response = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]},\n",
      "    config={\n",
      "        \"tags\": [\"production\", \"email-assistant\", \"v1.0\"],\n",
      "        \"metadata\": {\n",
      "            \"user_id\": \"user_123\",\n",
      "            \"session_id\": \"session_456\",\n",
      "            \"environment\": \"production\"\n",
      "        }\n",
      "    }\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI callback handler\n",
      "content===> from langchain_google_vertexai.callbacks import VertexAICallbackHandler\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Search\n",
      "content===> from langchain_community.retrievers import AzureAISearchRetriever\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Adding documents\n",
      "content===> vector_store.add_documents(documents=[doc1, doc2], ids=[\"id1\", \"id2\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI visual QnA\n",
      "content===> from langchain_google_vertexai.vision_models import VertexAIVisualQnAChat\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-aws\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI Warehouse\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Enable tracing\n",
      "content===> export LANGSMITH_TRACING=true\n",
      "export LANGSMITH_API_KEY=<your-api-key>\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a basic agent\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_weather],\n",
      "    system_prompt=\"You are a helpful assistant\",\n",
      ")\n",
      "\n",
      "# Run the agent\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Seamless with LangChain v1\n",
      "content===> pip install -U langgraph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Document Intelligence\n",
      "content===> Azure Form Recognizer\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Side effects called beforeinterruptmust be idempotent\n",
      "content===> def node_a(state: State):\n",
      "    # ✅ Good: using upsert operation which is idempotent\n",
      "    # Running this multiple times will have the same result\n",
      "    db.upsert_user(\n",
      "        user_id=state[\"user_id\"],\n",
      "        status=\"pending_approval\"\n",
      "    )\n",
      "\n",
      "    approved = interrupt(\"Approve this change?\")\n",
      "\n",
      "    return {\"approved\": approved}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add persistence\n",
      "content===> from langgraph.graph import START, StateGraph\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "class State(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "# Subgraph\n",
      "\n",
      "def subgraph_node_1(state: State):\n",
      "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
      "\n",
      "subgraph_builder = StateGraph(State)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Parent graph\n",
      "\n",
      "builder = StateGraph(State)\n",
      "builder.add_node(\"node_1\", subgraph)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "\n",
      "checkpointer = MemorySaver()\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Caching\n",
      "content===> query_embedding_cache\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Chat models\n",
      "content===> from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "from langchain.messages import HumanMessage\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
      "\n",
      "# Simple text invocation\n",
      "result = llm.invoke(\"Sing a ballad of LangChain.\")\n",
      "print(result.content)\n",
      "\n",
      "# Multimodal invocation with gemini-pro-vision\n",
      "message = HumanMessage(\n",
      "    content=[\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"What's in this image?\",\n",
      "        },\n",
      "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
      "    ]\n",
      ")\n",
      "result = llm.invoke([message])\n",
      "print(result.content)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Implementation\n",
      "content===> from langchain.tools import tool\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "subagent1 = create_agent(model=\"...\", tools=[...])\n",
      "\n",
      "@tool(\n",
      "    \"subagent1_name\",\n",
      "    description=\"subagent1_description\"\n",
      ")\n",
      "def call_subagent1(query: str):\n",
      "    result = subagent1.invoke({\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
      "    })\n",
      "    return result[\"messages\"][-1].content\n",
      "\n",
      "agent = create_agent(model=\"...\", tools=[call_subagent1])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Model\n",
      "content===> \"claude-sonnet-4-5-20250929\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "from langgraph.checkpoint.mongodb import MongoDBSaver  \n",
      "\n",
      "model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
      "\n",
      "DB_URI = \"localhost:27017\"\n",
      "with MongoDBSaver.from_conn_string(DB_URI) as checkpointer:  \n",
      "\n",
      "    def call_model(state: MessagesState):\n",
      "        response = model.invoke(state[\"messages\"])\n",
      "        return {\"messages\": response}\n",
      "\n",
      "    builder = StateGraph(MessagesState)\n",
      "    builder.add_node(call_model)\n",
      "    builder.add_edge(START, \"call_model\")\n",
      "\n",
      "    graph = builder.compile(checkpointer=checkpointer)  \n",
      "\n",
      "    config = {\n",
      "        \"configurable\": {\n",
      "            \"thread_id\": \"1\"\n",
      "        }\n",
      "    }\n",
      "\n",
      "    for chunk in graph.stream(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
      "        config,  \n",
      "        stream_mode=\"values\"\n",
      "    ):\n",
      "        chunk[\"messages\"][-1].pretty_print()\n",
      "\n",
      "    for chunk in graph.stream(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
      "        config,  \n",
      "        stream_mode=\"values\"\n",
      "    ):\n",
      "        chunk[\"messages\"][-1].pretty_print()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpointer interface\n",
      "content===> graph.get_state_history()\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon API Gateway\n",
      "content===> from langchain_community.llms import AmazonAPIGateway\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom tool message content\n",
      "content===> ================================ Human Message =================================\n",
      "\n",
      "From our meeting: Sarah needs to update the project timeline as soon as possible\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  MeetingAction (call_1)\n",
      " Call ID: call_1\n",
      "  Args:\n",
      "    task: Update the project timeline\n",
      "    assignee: Sarah\n",
      "    priority: high\n",
      "================================= Tool Message =================================\n",
      "Name: MeetingAction\n",
      "\n",
      "Action item captured and added to meeting notes!\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure SQL Database\n",
      "content===> from langchain_sqlserver import SQLServer_VectorStore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Providers\n",
      "content===> TencentCOSDirectoryLoader\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prompt\n",
      "content===> ================================ Human Message =================================\n",
      "\n",
      "What is the weather in SF?\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)\n",
      " Call ID: call_WFQlOGn4b2yoJrv7cih342FG\n",
      "  Args:\n",
      "    city: San Francisco\n",
      "================================= Tool Message =================================\n",
      "Name: get_weather\n",
      "\n",
      "The weather in San Francisco is always sunny!\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "Hi John Smith, the weather in San Francisco is always sunny!\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma local from Kaggle\n",
      "content===> from langchain_google_vertexai.gemma import GemmaLocalKaggle\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for PostgreSQL\n",
      "content===> from langchain_google_cloud_sql_pg import PostgresVectorStore # PostgresEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool and provider strategies\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy, ProviderStrategy\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class OutputSchema(BaseModel):\n",
      "    summary: str\n",
      "    sentiment: str\n",
      "\n",
      "# Using ToolStrategy\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o-mini\",\n",
      "    tools=tools,\n",
      "    # explicitly using tool strategy\n",
      "    response_format=ToolStrategy(OutputSchema)  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Try out your agent\n",
      "content===> # Test with an urgent billing issue\n",
      "initial_state = {\n",
      "    \"email_content\": \"I was charged twice for my subscription! This is urgent!\",\n",
      "    \"sender_email\": \"customer@example.com\",\n",
      "    \"email_id\": \"email_123\",\n",
      "    \"messages\": []\n",
      "}\n",
      "\n",
      "# Run with a thread_id for persistence\n",
      "config = {\"configurable\": {\"thread_id\": \"customer_123\"}}\n",
      "result = app.invoke(initial_state, config)\n",
      "# The graph will pause at human_review\n",
      "print(f\"Draft ready for review: {result['draft_response'][:100]}...\")\n",
      "\n",
      "# When ready, provide human input to resume\n",
      "from langgraph.types import Command\n",
      "\n",
      "human_response = Command(\n",
      "    resume={\n",
      "        \"approved\": True,\n",
      "        \"edited_response\": \"We sincerely apologize for the double charge. I've initiated an immediate refund...\"\n",
      "    }\n",
      ")\n",
      "\n",
      "# Resume execution\n",
      "final_result = app.invoke(human_response, config)\n",
      "print(f\"Email sent successfully!\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for MySQL\n",
      "content===> pip install langchain-google-cloud-sql-mysql\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamic system prompt\n",
      "content===> from typing import TypedDict\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
      "\n",
      "\n",
      "class Context(TypedDict):\n",
      "    user_role: str\n",
      "\n",
      "@dynamic_prompt\n",
      "def user_role_prompt(request: ModelRequest) -> str:\n",
      "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
      "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
      "    base_prompt = \"You are a helpful assistant.\"\n",
      "\n",
      "    if user_role == \"expert\":\n",
      "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
      "    elif user_role == \"beginner\":\n",
      "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
      "\n",
      "    return base_prompt\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[web_search],\n",
      "    middleware=[user_role_prompt],\n",
      "    context_schema=Context\n",
      ")\n",
      "\n",
      "# The system prompt will be set dynamically based on context\n",
      "result = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
      "    context={\"user_role\": \"expert\"}\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Subagent not being called\n",
      "content===> agent = create_deep_agent(\n",
      "    system_prompt=\"\"\"...your instructions...\n",
      "\n",
      "    IMPORTANT: For complex tasks, delegate to your subagents using the task() tool.\n",
      "    This keeps your context clean and improves results.\"\"\",\n",
      "    subagents=[...]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 1. Run the graph\n",
      "content===> config = {\n",
      "    \"configurable\": {\n",
      "        \"thread_id\": uuid.uuid4(),\n",
      "    }\n",
      "}\n",
      "state = graph.invoke({}, config)\n",
      "\n",
      "print(state[\"topic\"])\n",
      "print()\n",
      "print(state[\"joke\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Vision loader\n",
      "content===> pip install langchain-google-community[vision]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Office 365 email and calendar\n",
      "content===> from langchain_community.agent_toolkits import O365Toolkit\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bring-your-own documents\n",
      "content===> AmazonKnowledgeBasesRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document what gets persisted\n",
      "content===> system_prompt=\"\"\"You have access to two types of storage:\n",
      "\n",
      "SHORT-TERM (paths without /memories/):\n",
      "- Current conversation notes\n",
      "- Temporary scratch work\n",
      "- Draft documents\n",
      "\n",
      "LONG-TERM (paths starting with /memories/):\n",
      "- User preferences and settings\n",
      "- Completed reports and documents\n",
      "- Knowledge that should persist across conversations\n",
      "- Project state and progress\n",
      "\n",
      "Always use /memories/ for information that should survive beyond this conversation.\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Trends\n",
      "content===> from langchain_community.tools.google_trends import GoogleTrendsQueryRun\n",
      "from langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> create_agent\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[search_web, analyze_data, send_email],\n",
      "    system_prompt=\"You are a helpful research assistant.\"\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [\n",
      "        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n",
      "    ]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Batch\n",
      "content===> model.batch(\n",
      "    list_of_inputs,\n",
      "    config={\n",
      "        'max_concurrency': 5,  # Limit to 5 parallel calls\n",
      "    }\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> %%capture --no-stderr\n",
      "pip install --quiet -U langgraph langchain_anthropic\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Model Garden\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic usage\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "\n",
      "system_msg = SystemMessage(\"You are a helpful assistant.\")\n",
      "human_msg = HumanMessage(\"Hello, how are you?\")\n",
      "\n",
      "# Use with chat models\n",
      "messages = [system_msg, human_msg]\n",
      "response = model.invoke(messages)  # Returns AIMessage\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon OpenSearch Service\n",
      "content===> from langchain_community.vectorstores import OpenSearchVectorSearch\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text prompts\n",
      "content===> response = model.invoke(\"Write a haiku about spring\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Side effects called beforeinterruptmust be idempotent\n",
      "content===> def node_a(state: State):\n",
      "    # ❌ Bad: creating a new record before interrupt\n",
      "    # This will create duplicate records on each resume\n",
      "    audit_id = db.create_audit_log({\n",
      "        \"user_id\": state[\"user_id\"],\n",
      "        \"action\": \"pending_approval\",\n",
      "        \"timestamp\": datetime.now()\n",
      "    })\n",
      "\n",
      "    approved = interrupt(\"Approve this change?\")\n",
      "\n",
      "    return {\"approved\": approved, \"audit_id\": audit_id}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    middleware=[middleware1, middleware2, middleware3],\n",
      "    tools=[...],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Cloud\n",
      "content===> pip install langchain-google-vertexai\n",
      "# pip install langchain-google-community[...] # For other services\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SageMaker Tracking\n",
      "content===> from langchain_community.callbacks import SageMakerCallbackHandler\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Migrate tocreate_agent\n",
      "content===> config[\"configurable\"]\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 5: Wire it together\n",
      "content===> Command[Literal[\"node1\", \"node2\"]]\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Overview\n",
      "content===> \"Fetched 10/100 records\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Cloud\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
      "\n",
      "def delete_messages(state):\n",
      "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Planning\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import TodoListMiddleware\n",
      "from langchain.messages import HumanMessage\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[TodoListMiddleware()],\n",
      ")\n",
      "\n",
      "result = agent.invoke({\"messages\": [HumanMessage(\"Help me refactor my codebase\")]})\n",
      "print(result[\"todos\"])  # Array of todo items with status tracking\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tokens\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langgraph.graph import StateGraph, START\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class MyState:\n",
      "    topic: str\n",
      "    joke: str = \"\"\n",
      "\n",
      "\n",
      "model = init_chat_model(model=\"gpt-4o-mini\")\n",
      "\n",
      "def call_model(state: MyState):\n",
      "    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n",
      "    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n",
      "    model_response = model.invoke(  \n",
      "        [\n",
      "            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n",
      "        ]\n",
      "    )\n",
      "    return {\"joke\": model_response.content}\n",
      "\n",
      "graph = (\n",
      "    StateGraph(MyState)\n",
      "    .add_node(call_model)\n",
      "    .add_edge(START, \"call_model\")\n",
      "    .compile()\n",
      ")\n",
      "\n",
      "# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n",
      "# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n",
      "# with information about the graph node where the LLM was called and other information\n",
      "for message_chunk, metadata in graph.stream(\n",
      "    {\"topic\": \"ice cream\"},\n",
      "    stream_mode=\"messages\",  \n",
      "):\n",
      "    if message_chunk.content:\n",
      "        print(message_chunk.content, end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Firestore (Native Mode)\n",
      "content===> pip install langchain-google-firestore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon OpenSearch Service\n",
      "content===> pip install boto3 requests requests-aws4auth\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interrupts\n",
      "content===> config={\"configurable\": {\"thread_id\": ...}}\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Textract\n",
      "content===> from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic Usage\n",
      "content===> memories = in_memory_store.search(namespace_for_memory)\n",
      "memories[-1].dict()\n",
      "{'value': {'food_preference': 'I like pizza'},\n",
      " 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n",
      " 'namespace': ['1', 'memories'],\n",
      " 'created_at': '2024-10-02T17:22:31.590602+00:00',\n",
      " 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Page structure\n",
      "content===> ---\n",
      "title: \"Clear, specific title\"\n",
      "---\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Message metadata\n",
      "content===> human_msg = HumanMessage(\n",
      "    content=\"Hello!\",\n",
      "    name=\"alice\",  # Optional: identify different users\n",
      "    id=\"msg_123\",  # Optional: unique identifier for tracing\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PlayWright Browser Toolkit\n",
      "content===> from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceInferenceAPIEmbeddings\n",
      "content===> HuggingFaceInferenceAPIEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Presidio\n",
      "content===> pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\n",
      "python -m spacy download en_core_web_lg\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Rate limiting\n",
      "content===> from langchain_core.rate_limiters import InMemoryRateLimiter\n",
      "\n",
      "rate_limiter = InMemoryRateLimiter(\n",
      "    requests_per_second=0.1,  # 1 request every 10s\n",
      "    check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n",
      "    max_bucket_size=10,  # Controls the maximum burst size.\n",
      ")\n",
      "\n",
      "model = init_chat_model(\n",
      "    model=\"gpt-5\",\n",
      "    model_provider=\"openai\",\n",
      "    rate_limiter=rate_limiter  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> my-app/\n",
      "├── src\n",
      "│   └── agent.py\n",
      "├── .env\n",
      "└── langgraph.json\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Testing individual nodes and edges\n",
      "content===> import pytest\n",
      "\n",
      "from typing_extensions import TypedDict\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "def create_graph() -> StateGraph:\n",
      "    class MyState(TypedDict):\n",
      "        my_key: str\n",
      "\n",
      "    graph = StateGraph(MyState)\n",
      "    graph.add_node(\"node1\", lambda state: {\"my_key\": \"hello from node1\"})\n",
      "    graph.add_node(\"node2\", lambda state: {\"my_key\": \"hello from node2\"})\n",
      "    graph.add_edge(START, \"node1\")\n",
      "    graph.add_edge(\"node1\", \"node2\")\n",
      "    graph.add_edge(\"node2\", END)\n",
      "    return graph\n",
      "\n",
      "def test_individual_node_execution() -> None:\n",
      "    # Will be ignored in this example\n",
      "    checkpointer = MemorySaver()\n",
      "    graph = create_graph()\n",
      "    compiled_graph = graph.compile(checkpointer=checkpointer)\n",
      "    # Only invoke node 1\n",
      "    result = compiled_graph.nodes[\"node1\"].invoke(\n",
      "        {\"my_key\": \"initial_value\"},\n",
      "    )\n",
      "    assert result[\"my_key\"] == \"hello from node1\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
      "\n",
      "def delete_messages(state):\n",
      "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trim messages\n",
      "content===> from langchain_core.messages.utils import (\n",
      "    trim_messages,  \n",
      "    count_tokens_approximately  \n",
      ")\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langgraph.graph import StateGraph, START, MessagesState\n",
      "\n",
      "model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n",
      "summarization_model = model.bind(max_tokens=128)\n",
      "\n",
      "def call_model(state: MessagesState):\n",
      "    messages = trim_messages(  \n",
      "        state[\"messages\"],\n",
      "        strategy=\"last\",\n",
      "        token_counter=count_tokens_approximately,\n",
      "        max_tokens=128,\n",
      "        start_on=\"human\",\n",
      "        end_on=(\"human\", \"tool\"),\n",
      "    )\n",
      "    response = model.invoke(messages)\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "builder = StateGraph(MessagesState)\n",
      "builder.add_node(call_model)\n",
      "builder.add_edge(START, \"call_model\")\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
      "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
      "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
      "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
      "\n",
      "final_response[\"messages\"][-1].pretty_print()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install\n",
      "content===> pip install -U langgraph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/overview\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prebuilt middleware\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import (\n",
      "    PIIMiddleware,\n",
      "    SummarizationMiddleware,\n",
      "    HumanInTheLoopMiddleware\n",
      ")\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[read_email, send_email],\n",
      "    middleware=[\n",
      "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
      "        PIIMiddleware(\n",
      "            \"phone_number\",\n",
      "            detector=(\n",
      "                r\"(?:\\+?\\d{1,3}[\\s.-]?)?\"\n",
      "                r\"(?:\\(?\\d{2,4}\\)?[\\s.-]?)?\"\n",
      "                r\"\\d{3,4}[\\s.-]?\\d{4}\"\n",
      "\t\t\t),\n",
      "\t\t\tstrategy=\"block\"\n",
      "        ),\n",
      "        SummarizationMiddleware(\n",
      "            model=\"claude-sonnet-4-5-20250929\",\n",
      "            max_tokens_before_summary=500\n",
      "        ),\n",
      "        HumanInTheLoopMiddleware(\n",
      "            interrupt_on={\n",
      "                \"send_email\": {\n",
      "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]\n",
      "                }\n",
      "            }\n",
      "        ),\n",
      "    ]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-ibm\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Namespace\n",
      "content===> langchain.chat_models\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tailor configurations by risk\n",
      "content===> interrupt_on = {\n",
      "    # High risk: full control (approve, edit, reject)\n",
      "    \"delete_file\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n",
      "    \"send_email\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n",
      "\n",
      "    # Medium risk: no editing allowed\n",
      "    \"write_file\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
      "\n",
      "    # Low risk: no interrupts\n",
      "    \"read_file\": False,\n",
      "    \"list_files\": False,\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Usage\n",
      "content===> from langchain.agents import create_agent\n",
      "from langgraph.checkpoint.memory import InMemorySaver  \n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-5\",\n",
      "    [get_user_info],\n",
      "    checkpointer=InMemorySaver(),  \n",
      ")\n",
      "\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
      "    {\"configurable\": {\"thread_id\": \"1\"}},  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Local development\n",
      "content===> # Create a new Agent Chat UI project\n",
      "npx create-agent-chat-app --project-name my-chat-ui\n",
      "cd my-chat-ui\n",
      "\n",
      "# Install dependencies and start\n",
      "pnpm install\n",
      "pnpm dev\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multimodal\n",
      "content===> response = model.invoke(\"Create a picture of a cat\")\n",
      "print(response.content_blocks)\n",
      "# [\n",
      "#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n",
      "#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n",
      "# ]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Parameters\n",
      "content===> model = init_chat_model(\n",
      "    \"claude-sonnet-4-5-20250929\",\n",
      "    # Kwargs passed to the model:\n",
      "    temperature=0.7,\n",
      "    timeout=30,\n",
      "    max_tokens=1000,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Decorator-based middleware\n",
      "content===> from langchain.agents.middleware import before_model, after_model, wrap_model_call\n",
      "from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse, dynamic_prompt\n",
      "from langchain.messages import AIMessage\n",
      "from langchain.agents import create_agent\n",
      "from langgraph.runtime import Runtime\n",
      "from typing import Any, Callable\n",
      "\n",
      "\n",
      "# Node-style: logging before model calls\n",
      "@before_model\n",
      "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "    print(f\"About to call model with {len(state['messages'])} messages\")\n",
      "    return None\n",
      "\n",
      "# Node-style: validation after model calls\n",
      "@after_model(can_jump_to=[\"end\"])\n",
      "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "    last_message = state[\"messages\"][-1]\n",
      "    if \"BLOCKED\" in last_message.content:\n",
      "        return {\n",
      "            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n",
      "            \"jump_to\": \"end\"\n",
      "        }\n",
      "    return None\n",
      "\n",
      "# Wrap-style: retry logic\n",
      "@wrap_model_call\n",
      "def retry_model(\n",
      "    request: ModelRequest,\n",
      "    handler: Callable[[ModelRequest], ModelResponse],\n",
      ") -> ModelResponse:\n",
      "    for attempt in range(3):\n",
      "        try:\n",
      "            return handler(request)\n",
      "        except Exception as e:\n",
      "            if attempt == 2:\n",
      "                raise\n",
      "            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n",
      "\n",
      "# Wrap-style: dynamic prompts\n",
      "@dynamic_prompt\n",
      "def personalized_prompt(request: ModelRequest) -> str:\n",
      "    user_id = request.runtime.context.get(\"user_id\", \"guest\")\n",
      "    return f\"You are a helpful assistant for user {user_id}. Be concise and friendly.\"\n",
      "\n",
      "# Use decorators in agent\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],\n",
      "    tools=[...],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 7. Test the API\n",
      "content===> from langgraph_sdk import get_client\n",
      "import asyncio\n",
      "\n",
      "client = get_client(url=\"http://localhost:2024\")\n",
      "\n",
      "async def main():\n",
      "    async for chunk in client.runs.stream(\n",
      "        None,  # Threadless run\n",
      "        \"agent\", # Name of assistant. Defined in langgraph.json.\n",
      "        input={\n",
      "        \"messages\": [{\n",
      "            \"role\": \"human\",\n",
      "            \"content\": \"What is LangGraph?\",\n",
      "            }],\n",
      "        },\n",
      "    ):\n",
      "        print(f\"Receiving new event of type: {chunk.event}...\")\n",
      "        print(chunk.data)\n",
      "        print(\"\\n\\n\")\n",
      "\n",
      "asyncio.run(main())\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> langchain-classic\n",
      "content===> pip install langchain-classic\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Advanced schema definition\n",
      "content===> from pydantic import BaseModel, Field\n",
      "from typing import Literal\n",
      "\n",
      "class WeatherInput(BaseModel):\n",
      "    \"\"\"Input for weather queries.\"\"\"\n",
      "    location: str = Field(description=\"City name or coordinates\")\n",
      "    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n",
      "        default=\"celsius\",\n",
      "        description=\"Temperature unit preference\"\n",
      "    )\n",
      "    include_forecast: bool = Field(\n",
      "        default=False,\n",
      "        description=\"Include 5-day forecast\"\n",
      "    )\n",
      "\n",
      "@tool(args_schema=WeatherInput)\n",
      "def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n",
      "    \"\"\"Get current weather and optional forecast.\"\"\"\n",
      "    temp = 22 if units == \"celsius\" else 72\n",
      "    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n",
      "    if include_forecast:\n",
      "        result += \"\\nNext 5 days: Sunny\"\n",
      "    return result\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Getting started\n",
      "content===> import pytest\n",
      "\n",
      "from typing_extensions import TypedDict\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "def create_graph() -> StateGraph:\n",
      "    class MyState(TypedDict):\n",
      "        my_key: str\n",
      "\n",
      "    graph = StateGraph(MyState)\n",
      "    graph.add_node(\"node1\", lambda state: {\"my_key\": \"hello from node1\"})\n",
      "    graph.add_node(\"node2\", lambda state: {\"my_key\": \"hello from node2\"})\n",
      "    graph.add_edge(START, \"node1\")\n",
      "    graph.add_edge(\"node1\", \"node2\")\n",
      "    graph.add_edge(\"node2\", END)\n",
      "    return graph\n",
      "\n",
      "def test_basic_agent_execution() -> None:\n",
      "    checkpointer = MemorySaver()\n",
      "    graph = create_graph()\n",
      "    compiled_graph = graph.compile(checkpointer=checkpointer)\n",
      "    result = compiled_graph.invoke(\n",
      "        {\"my_key\": \"initial_value\"},\n",
      "        config={\"configurable\": {\"thread_id\": \"1\"}}\n",
      "    )\n",
      "    assert result[\"my_key\"] == \"hello from node2\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Document Intelligence\n",
      "content===> pip install azure-ai-documentintelligence\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Image captions\n",
      "content===> from langchain_community.document_loaders import ImageCaptionLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Log to a project\n",
      "content===> import langsmith as ls\n",
      "\n",
      "with ls.tracing_context(project_name=\"email-agent-test\", enabled=True):\n",
      "    response = agent.invoke({\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]\n",
      "    })\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Serialize standard content\n",
      "content===> export LC_OUTPUT_VERSION=v1\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> [pytest]\n",
      "markers =\n",
      "    vcr: record/replay HTTP via VCR\n",
      "addopts = --record-mode=once\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-openai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Get state\n",
      "content===> # get the latest state snapshot\n",
      "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "graph.get_state(config)\n",
      "\n",
      "# get a state snapshot for a specific checkpoint_id\n",
      "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\n",
      "graph.get_state(config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Migrate tocreate_agent\n",
      "content===> langchain.agents.create_agent\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceBgeEmbeddings\n",
      "content===> from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Container Apps dynamic sessions\n",
      "content===> from langchain_azure_dynamic_sessions import SessionsPythonREPLTool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LangSmith Integration\n",
      "content===> from langsmith import Client\n",
      "from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n",
      "\n",
      "client = Client()\n",
      "\n",
      "trajectory_evaluator = create_trajectory_llm_as_judge(\n",
      "    model=\"openai:o3-mini\",\n",
      "    prompt=TRAJECTORY_ACCURACY_PROMPT,\n",
      ")\n",
      "\n",
      "def run_agent(inputs):\n",
      "    \"\"\"Your agent function that returns trajectory messages.\"\"\"\n",
      "    return agent.invoke(inputs)[\"messages\"]\n",
      "\n",
      "experiment_results = client.evaluate(\n",
      "    run_agent,\n",
      "    data=\"your_dataset_name\",\n",
      "    evaluators=[trajectory_evaluator]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> FilesystemBackend (local disk)\n",
      "content===> from deepagents.backends import FilesystemBackend\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    backend=FilesystemBackend(root_dir=\"/Users/nh/Desktop/\")\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> (\"parent_node:<task_id>\", \"child_node:<task_id>\")\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
      "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
      "\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "\n",
      "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Partial execution\n",
      "content===> import pytest\n",
      "\n",
      "from typing_extensions import TypedDict\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "def create_graph() -> StateGraph:\n",
      "    class MyState(TypedDict):\n",
      "        my_key: str\n",
      "\n",
      "    graph = StateGraph(MyState)\n",
      "    graph.add_node(\"node1\", lambda state: {\"my_key\": \"hello from node1\"})\n",
      "    graph.add_node(\"node2\", lambda state: {\"my_key\": \"hello from node2\"})\n",
      "    graph.add_node(\"node3\", lambda state: {\"my_key\": \"hello from node3\"})\n",
      "    graph.add_node(\"node4\", lambda state: {\"my_key\": \"hello from node4\"})\n",
      "    graph.add_edge(START, \"node1\")\n",
      "    graph.add_edge(\"node1\", \"node2\")\n",
      "    graph.add_edge(\"node2\", \"node3\")\n",
      "    graph.add_edge(\"node3\", \"node4\")\n",
      "    graph.add_edge(\"node4\", END)\n",
      "    return graph\n",
      "\n",
      "def test_partial_execution_from_node2_to_node3() -> None:\n",
      "    checkpointer = MemorySaver()\n",
      "    graph = create_graph()\n",
      "    compiled_graph = graph.compile(checkpointer=checkpointer)\n",
      "    compiled_graph.update_state(\n",
      "        config={\n",
      "          \"configurable\": {\n",
      "            \"thread_id\": \"1\"\n",
      "          }\n",
      "        },\n",
      "        # The state passed into node 2 - simulating the state at\n",
      "        # the end of node 1\n",
      "        values={\"my_key\": \"initial_value\"},\n",
      "        # Update saved state as if it came from node 1\n",
      "        # Execution will resume at node 2\n",
      "        as_node=\"node1\",\n",
      "    )\n",
      "    result = compiled_graph.invoke(\n",
      "        # Resume execution by passing None\n",
      "        None,\n",
      "        config={\"configurable\": {\"thread_id\": \"1\"}},\n",
      "        # Stop after node 3 so that node 4 doesn't run\n",
      "        interrupt_after=\"node3\",\n",
      "    )\n",
      "    assert result[\"my_key\"] == \"hello from node3\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpoints\n",
      "content===> {'foo': 'a', 'bar': ['a']}\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cognitive Services\n",
      "content===> from langchain_community.agent_toolkits import AzureCognitiveServicesToolkit\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic configuration\n",
      "content===> {\"allowed_decisions\": [...]}\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Translate\n",
      "content===> langchain-google-community\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream subgraph outputs\n",
      "content===> from typing_extensions import TypedDict\n",
      "from langgraph.graph.state import StateGraph, START\n",
      "\n",
      "# Define subgraph\n",
      "class SubgraphState(TypedDict):\n",
      "    foo: str\n",
      "    bar: str\n",
      "\n",
      "def subgraph_node_1(state: SubgraphState):\n",
      "    return {\"bar\": \"bar\"}\n",
      "\n",
      "def subgraph_node_2(state: SubgraphState):\n",
      "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
      "    # and is sending update on the shared state key ('foo')\n",
      "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
      "\n",
      "subgraph_builder = StateGraph(SubgraphState)\n",
      "subgraph_builder.add_node(subgraph_node_1)\n",
      "subgraph_builder.add_node(subgraph_node_2)\n",
      "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
      "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
      "subgraph = subgraph_builder.compile()\n",
      "\n",
      "# Define parent graph\n",
      "class ParentState(TypedDict):\n",
      "    foo: str\n",
      "\n",
      "def node_1(state: ParentState):\n",
      "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
      "\n",
      "builder = StateGraph(ParentState)\n",
      "builder.add_node(\"node_1\", node_1)\n",
      "builder.add_node(\"node_2\", subgraph)\n",
      "builder.add_edge(START, \"node_1\")\n",
      "builder.add_edge(\"node_1\", \"node_2\")\n",
      "graph = builder.compile()\n",
      "\n",
      "for chunk in graph.stream(\n",
      "    {\"foo\": \"foo\"},\n",
      "    stream_mode=\"updates\",\n",
      "    subgraphs=True, \n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured outputs\n",
      "content===> from pydantic import BaseModel, Field\n",
      "\n",
      "class Movie(BaseModel):\n",
      "    \"\"\"A movie with details.\"\"\"\n",
      "    title: str = Field(..., description=\"The title of the movie\")\n",
      "    year: int = Field(..., description=\"The year the movie was released\")\n",
      "    director: str = Field(..., description=\"The director of the movie\")\n",
      "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
      "\n",
      "model_with_structure = model.with_structured_output(Movie, include_raw=True)  \n",
      "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
      "response\n",
      "# {\n",
      "#     \"raw\": AIMessage(...),\n",
      "#     \"parsed\": Movie(title=..., year=..., ...),\n",
      "#     \"parsing_error\": None,\n",
      "# }\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom MCP servers\n",
      "content===> from mcp.server.fastmcp import FastMCP\n",
      "\n",
      "mcp = FastMCP(\"Weather\")\n",
      "\n",
      "@mcp.tool()\n",
      "async def get_weather(location: str) -> str:\n",
      "    \"\"\"Get weather for location.\"\"\"\n",
      "    return \"It's always sunny in New York\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    mcp.run(transport=\"streamable-http\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Vector Search\n",
      "content===> pip install langchain-google-vertexai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Planning middleware\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import TodoListMiddleware\n",
      "\n",
      "# TodoListMiddleware is included by default in create_deep_agent\n",
      "# You can customize it if building a custom agent\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    # Custom planning instructions can be added via middleware\n",
      "    middleware=[\n",
      "        TodoListMiddleware(\n",
      "            system_prompt=\"Use the write_todos tool to...\"  # Optional: Custom addition to the system prompt\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PlayWright Browser individual tools\n",
      "content===> from langchain_community.tools.playwright import ClickTool\n",
      "from langchain_community.tools.playwright import CurrentWebPageTool\n",
      "from langchain_community.tools.playwright import ExtractHyperlinksTool\n",
      "from langchain_community.tools.playwright import ExtractTextTool\n",
      "from langchain_community.tools.playwright import GetElementsTool\n",
      "from langchain_community.tools.playwright import NavigateTool\n",
      "from langchain_community.tools.playwright import NavigateBackTool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Create multimodal messages\n",
      "content===> from langchain.messages import HumanMessage\n",
      "\n",
      "message = HumanMessage(content_blocks=[\n",
      "    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
      "    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n",
      "])\n",
      "res = model.invoke([message])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Audio Loader\n",
      "content===> pip install yt_dlp pydub librosa langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_mongodb import MongoDBAtlasVectorSearch\n",
      "\n",
      "vector_store = MongoDBAtlasVectorSearch(\n",
      "    embedding=embeddings,\n",
      "    collection=MONGODB_COLLECTION,\n",
      "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
      "    relevance_score_fn=\"cosine\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Short-term vs. long-term filesystem\n",
      "content===> use_longterm_memory=True\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_azure_ai.vectorstores.azure_cosmos_db_no_sql import (\n",
      "    AzureCosmosDBNoSqlVectorSearch,\n",
      ")\n",
      "vector_search = AzureCosmosDBNoSqlVectorSearch.from_documents(\n",
      "    documents=docs,\n",
      "    embedding=openai_embeddings,\n",
      "    cosmos_client=cosmos_client,\n",
      "    database_name=database_name,\n",
      "    container_name=container_name,\n",
      "    vector_embedding_policy=vector_embedding_policy,\n",
      "    full_text_policy=full_text_policy,\n",
      "    indexing_policy=indexing_policy,\n",
      "    cosmos_container_properties=cosmos_container_properties,\n",
      "    cosmos_database_properties={},\n",
      "    full_text_search_enabled=True,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Initialize a model\n",
      "content===> import os\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
      "\n",
      "model = init_chat_model(\"gpt-4.1\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> View subgraph state\n",
      "content===> graph.get_state(config)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dictionary format\n",
      "content===> messages = [\n",
      "    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n",
      "    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n",
      "    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Additional resources\n",
      "content===> langchain-mcp-adapters\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Document Intelligence\n",
      "content===> from langchain.document_loaders import AzureAIDocumentIntelligenceLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.tools import tool\n",
      "from langchain.messages import HumanMessage, AIMessage, ToolMessage\n",
      "from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n",
      "\n",
      "\n",
      "@tool\n",
      "def get_weather(city: str):\n",
      "    \"\"\"Get weather information for a city.\"\"\"\n",
      "    return f\"It's 75 degrees and sunny in {city}.\"\n",
      "\n",
      "agent = create_agent(\"gpt-4o\", tools=[get_weather])\n",
      "\n",
      "evaluator = create_trajectory_llm_as_judge(  \n",
      "    model=\"openai:o3-mini\",  \n",
      "    prompt=TRAJECTORY_ACCURACY_PROMPT,  \n",
      ")  \n",
      "\n",
      "def test_trajectory_quality():\n",
      "    result = agent.invoke({\n",
      "        \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n",
      "    })\n",
      "\n",
      "    evaluation = evaluator(\n",
      "        outputs=result[\"messages\"],\n",
      "    )\n",
      "    # {\n",
      "    #     'key': 'trajectory_accuracy',\n",
      "    #     'score': True,\n",
      "    #     'comment': 'The provided agent trajectory is reasonable...'\n",
      "    # }\n",
      "    assert evaluation[\"score\"] is True\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install\n",
      "content===> pip install langchain-mcp-adapters\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> edit(file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Listing files\n",
      "content===> agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"List all files\"}]\n",
      "})\n",
      "\n",
      "# Example output:\n",
      "# Transient files:\n",
      "# - /draft.txt\n",
      "# - /temp_notes.txt\n",
      "#\n",
      "# Long-term files:\n",
      "# - /memories/user_preferences.txt\n",
      "# - /memories/project_status.txt\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Provider strategy\n",
      "content===> class ProviderStrategy(Generic[SchemaT]):\n",
      "    schema: type[SchemaT]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "configurable_model = init_chat_model(temperature=0)\n",
      "\n",
      "configurable_model.invoke(\n",
      "    \"what's your name\",\n",
      "    config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n",
      ")\n",
      "configurable_model.invoke(\n",
      "    \"what's your name\",\n",
      "    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},  # Run with Claude\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Spanner\n",
      "content===> from langchain_google_spanner import SpannerVectorStore\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Async with Python < 3.11\n",
      "content===> from typing import TypedDict\n",
      "from langgraph.types import StreamWriter\n",
      "\n",
      "class State(TypedDict):\n",
      "      topic: str\n",
      "      joke: str\n",
      "\n",
      "# Add writer as an argument in the function signature of the async node or tool\n",
      "# LangGraph will automatically pass the stream writer to the function\n",
      "async def generate_joke(state: State, writer: StreamWriter):  \n",
      "      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n",
      "      return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
      "\n",
      "graph = (\n",
      "      StateGraph(State)\n",
      "      .add_node(generate_joke)\n",
      "      .add_edge(START, \"generate_joke\")\n",
      "      .compile()\n",
      ")\n",
      "\n",
      "# Set stream_mode=\"custom\" to receive the custom data in the stream  #\n",
      "async for chunk in graph.astream(\n",
      "      {\"topic\": \"ice cream\"},\n",
      "      stream_mode=\"custom\",\n",
      "):\n",
      "      print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Search Tool\n",
      "content===> pip install youtube_search langchain # Requires base langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Pre-model hook\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import SummarizationMiddleware\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=tools,\n",
      "    middleware=[\n",
      "        SummarizationMiddleware(  \n",
      "            model=\"claude-sonnet-4-5-20250929\",  \n",
      "            max_tokens_before_summary=1000\n",
      "        )  \n",
      "    ]  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream multiple modes\n",
      "content===> for mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> glob_info(pattern: str, path: str = \"/\") -> list[FileInfo]\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud SQL for PostgreSQL\n",
      "content===> from langchain_google_cloud_sql_pg import PostgresLoader # PostgresEngine also available\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face Text-to-Speech Model Inference.\n",
      "content===> from langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI Warehouse\n",
      "content===> from langchain_google_community.documentai_warehouse import DocumentAIWarehouseRetriever\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining state viastate_schema\n",
      "content===> from langchain.agents import AgentState\n",
      "\n",
      "\n",
      "class CustomState(AgentState):\n",
      "    user_preferences: dict\n",
      "\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=[tool1, tool2],\n",
      "    state_schema=CustomState\n",
      ")\n",
      "# The agent can now track additional state beyond messages\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
      "    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Implementing our email agent nodes\n",
      "content===> from typing import Literal\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.types import interrupt, Command, RetryPolicy\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.messages import HumanMessage\n",
      "\n",
      "llm = ChatOpenAI(model=\"gpt-4\")\n",
      "\n",
      "def read_email(state: EmailAgentState) -> dict:\n",
      "    \"\"\"Extract and parse email content\"\"\"\n",
      "    # In production, this would connect to your email service\n",
      "    return {\n",
      "        \"messages\": [HumanMessage(content=f\"Processing email: {state['email_content']}\")]\n",
      "    }\n",
      "\n",
      "def classify_intent(state: EmailAgentState) -> Command[Literal[\"search_documentation\", \"human_review\", \"draft_response\", \"bug_tracking\"]]:\n",
      "    \"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\"\n",
      "\n",
      "    # Create structured LLM that returns EmailClassification dict\n",
      "    structured_llm = llm.with_structured_output(EmailClassification)\n",
      "\n",
      "    # Format the prompt on-demand, not stored in state\n",
      "    classification_prompt = f\"\"\"\n",
      "    Analyze this customer email and classify it:\n",
      "\n",
      "    Email: {state['email_content']}\n",
      "    From: {state['sender_email']}\n",
      "\n",
      "    Provide classification including intent, urgency, topic, and summary.\n",
      "    \"\"\"\n",
      "\n",
      "    # Get structured response directly as dict\n",
      "    classification = structured_llm.invoke(classification_prompt)\n",
      "\n",
      "    # Determine next node based on classification\n",
      "    if classification['intent'] == 'billing' or classification['urgency'] == 'critical':\n",
      "        goto = \"human_review\"\n",
      "    elif classification['intent'] in ['question', 'feature']:\n",
      "        goto = \"search_documentation\"\n",
      "    elif classification['intent'] == 'bug':\n",
      "        goto = \"bug_tracking\"\n",
      "    else:\n",
      "        goto = \"draft_response\"\n",
      "\n",
      "    # Store classification as a single dict in state\n",
      "    return Command(\n",
      "        update={\"classification\": classification},\n",
      "        goto=goto\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
      "\n",
      "    # Get the user id from the config\n",
      "    user_id = config[\"configurable\"][\"user_id\"]\n",
      "\n",
      "    # Namespace the memory\n",
      "    namespace = (user_id, \"memories\")\n",
      "\n",
      "    # ... Analyze conversation and create a new memory\n",
      "\n",
      "    # Create a new memory ID\n",
      "    memory_id = str(uuid.uuid4())\n",
      "\n",
      "    # We create a new memory\n",
      "    store.put(namespace, memory_id, {\"memory\": memory})\n",
      "\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Wrap-style hooks\n",
      "content===> from langchain.tools.tool_node import ToolCallRequest\n",
      "from langchain.agents.middleware import AgentMiddleware\n",
      "from langchain_core.messages import ToolMessage\n",
      "from langgraph.types import Command\n",
      "from typing import Callable\n",
      "\n",
      "class ToolMonitoringMiddleware(AgentMiddleware):\n",
      "    def wrap_tool_call(\n",
      "        self,\n",
      "        request: ToolCallRequest,\n",
      "        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n",
      "    ) -> ToolMessage | Command:\n",
      "        print(f\"Executing tool: {request.tool_call['name']}\")\n",
      "        print(f\"Arguments: {request.tool_call['args']}\")\n",
      "\n",
      "        try:\n",
      "            result = handler(request)\n",
      "            print(f\"Tool completed successfully\")\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"Tool failed: {e}\")\n",
      "            raise\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM tokens\n",
      "content===> (message_chunk, metadata)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Development environment\n",
      "content===> cd libs/community/langchain_community/path/to/integration\n",
      "uv sync --all-groups\n",
      "make test  # Ensure tests pass before starting development\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Places\n",
      "content===> # Note: GooglePlacesTool might be in langchain or langchain_community depending on version\n",
      "from langchain.tools import GooglePlacesTool # Or langchain_community.tools\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> values\n",
      "content===> {\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Provider strategy\n",
      "content===> response_format=ProductReview\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 5. Launch LangGraph server 🚀\n",
      "content===> >    Ready!\n",
      ">\n",
      ">    - API: [http://localhost:2024](http://localhost:2024/)\n",
      ">\n",
      ">    - Docs: http://localhost:2024/docs\n",
      ">\n",
      ">    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 3. Update the state\n",
      "content===> new_config = graph.update_state(selected_state.config, values={\"topic\": \"chickens\"})\n",
      "print(new_config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using in LangGraph\n",
      "content===> # Invoke the graph\n",
      "user_id = \"1\"\n",
      "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n",
      "\n",
      "# First let's just say hi to the AI\n",
      "for update in graph.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n",
      "):\n",
      "    print(update)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Bedrock (Knowledge Bases)\n",
      "content===> from langchain_aws import AmazonKnowledgeBasesRetriever\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PII detection\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import PIIMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[customer_service_tool, email_tool],\n",
      "    middleware=[\n",
      "        # Redact emails in user input before sending to model\n",
      "        PIIMiddleware(\n",
      "            \"email\",\n",
      "            strategy=\"redact\",\n",
      "            apply_to_input=True,\n",
      "        ),\n",
      "        # Mask credit cards in user input\n",
      "        PIIMiddleware(\n",
      "            \"credit_card\",\n",
      "            strategy=\"mask\",\n",
      "            apply_to_input=True,\n",
      "        ),\n",
      "        # Block API keys - raise error if detected\n",
      "        PIIMiddleware(\n",
      "            \"api_key\",\n",
      "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
      "            strategy=\"block\",\n",
      "            apply_to_input=True,\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "# When user provides PII, it will be handled according to the strategy\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 4532-1234-5678-9010\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Prompt caching\n",
      "content===> AnthropicPromptCachingMiddleware\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Blob Storage\n",
      "content===> pip install langchain-azure-storage\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Decision types\n",
      "content===> agent.invoke(\n",
      "    Command(\n",
      "        # Decisions are provided as a list, one per action under review.\n",
      "        # The order of decisions must match the order of actions\n",
      "        # listed in the `__interrupt__` request.\n",
      "        resume={\n",
      "            \"decisions\": [\n",
      "                {\n",
      "                    \"type\": \"approve\",\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ),\n",
      "    config=config  # Same thread ID to resume the paused conversation\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_postgres import PGEngine, PGVectorStore\n",
      "\n",
      "$engine = PGEngine.from_connection_string(\n",
      "    url=\"postgresql+psycopg://...\"\n",
      ")\n",
      "\n",
      "vector_store = PGVectorStore.create_sync(\n",
      "    engine=pg_engine,\n",
      "    table_name='test_table',\n",
      "    embedding_service=embedding\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Return concise results\n",
      "content===> data_analyst = {\n",
      "    \"system_prompt\": \"\"\"Analyze the data and return:\n",
      "    1. Key insights (3-5 bullet points)\n",
      "    2. Overall confidence score\n",
      "    3. Recommended next actions\n",
      "\n",
      "    Do NOT include:\n",
      "    - Raw data\n",
      "    - Intermediate calculations\n",
      "    - Detailed tool outputs\n",
      "\n",
      "    Keep response under 300 words.\"\"\"\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom tool message content\n",
      "content===> from pydantic import BaseModel, Field\n",
      "from typing import Literal\n",
      "from langchain.agents import create_agent\n",
      "from langchain.agents.structured_output import ToolStrategy\n",
      "\n",
      "\n",
      "class MeetingAction(BaseModel):\n",
      "    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n",
      "    task: str = Field(description=\"The specific task to be completed\")\n",
      "    assignee: str = Field(description=\"Person responsible for the task\")\n",
      "    priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\")\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5\",\n",
      "    tools=[],\n",
      "    response_format=ToolStrategy(\n",
      "        schema=MeetingAction,\n",
      "        tool_message_content=\"Action item captured and added to meeting notes!\"\n",
      "    )\n",
      ")\n",
      "\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure OpenAI\n",
      "content===> pip install langchain-openai\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Model\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "from deepagents import create_deep_agent\n",
      "\n",
      "model = init_chat_model(\n",
      "    model=\"gpt-5\",\n",
      ")\n",
      "agent = create_deep_agent(\n",
      "    model=model,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> GoogleVertexAISearchRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-huggingface\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multiple specialized subagents\n",
      "content===> from deepagents import create_deep_agent\n",
      "\n",
      "subagents = [\n",
      "    {\n",
      "        \"name\": \"data-collector\",\n",
      "        \"description\": \"Gathers raw data from various sources\",\n",
      "        \"system_prompt\": \"Collect comprehensive data on the topic\",\n",
      "        \"tools\": [web_search, api_call, database_query],\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"data-analyzer\",\n",
      "        \"description\": \"Analyzes collected data for insights\",\n",
      "        \"system_prompt\": \"Analyze data and extract key insights\",\n",
      "        \"tools\": [statistical_analysis],\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"report-writer\",\n",
      "        \"description\": \"Writes polished reports from analysis\",\n",
      "        \"system_prompt\": \"Create professional reports from insights\",\n",
      "        \"tools\": [format_document],\n",
      "    },\n",
      "]\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    system_prompt=\"You coordinate data analysis and reporting. Use subagents for specialized tasks.\",\n",
      "    subagents=subagents\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Services\n",
      "content===> from langchain_community.agent_toolkits import azure_ai_services\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Custom updates\n",
      "content===> from langchain.agents import create_agent\n",
      "from langgraph.config import get_stream_writer  \n",
      "\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    writer = get_stream_writer()  \n",
      "    # stream any arbitrary data\n",
      "    writer(f\"Looking up data for city: {city}\")\n",
      "    writer(f\"Acquired data for city: {city}\")\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_weather],\n",
      ")\n",
      "\n",
      "for chunk in agent.stream(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
      "    stream_mode=\"custom\"\n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Identify a checkpoint\n",
      "content===> ()\n",
      "1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e\n",
      "\n",
      "('write_joke',)\n",
      "1f02ac4a-ce2a-6494-8001-cb2e2d651227\n",
      "\n",
      "('generate_topic',)\n",
      "1f02ac4a-a4e0-630d-8000-b73c254ba748\n",
      "\n",
      "('__start__',)\n",
      "1f02ac4a-a4dd-665e-bfff-e6c8c44315d9\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Write clear descriptions\n",
      "content===> \"Analyzes financial data and generates investment insights with confidence scores\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Trim messages\n",
      "content===> from langchain.messages import RemoveMessage\n",
      "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langchain.agents import create_agent, AgentState\n",
      "from langchain.agents.middleware import before_model\n",
      "from langgraph.runtime import Runtime\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from typing import Any\n",
      "\n",
      "\n",
      "@before_model\n",
      "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
      "    messages = state[\"messages\"]\n",
      "\n",
      "    if len(messages) <= 3:\n",
      "        return None  # No changes needed\n",
      "\n",
      "    first_msg = messages[0]\n",
      "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
      "    new_messages = [first_msg] + recent_messages\n",
      "\n",
      "    return {\n",
      "        \"messages\": [\n",
      "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
      "            *new_messages\n",
      "        ]\n",
      "    }\n",
      "\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=tools,\n",
      "    middleware=[trim_messages],\n",
      "    checkpointer=InMemorySaver(),\n",
      ")\n",
      "\n",
      "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
      "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
      "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
      "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
      "\n",
      "final_response[\"messages\"][-1].pretty_print()\n",
      "\"\"\"\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "Your name is Bob. You told me that earlier.\n",
      "If you'd like me to call you a nickname or use a different name, just say the word.\n",
      "\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Filter by LLM invocation\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "# model_1 is tagged with \"joke\"\n",
      "model_1 = init_chat_model(model=\"gpt-4o-mini\", tags=['joke'])\n",
      "# model_2 is tagged with \"poem\"\n",
      "model_2 = init_chat_model(model=\"gpt-4o-mini\", tags=['poem'])\n",
      "\n",
      "graph = ... # define a graph that uses these LLMs\n",
      "\n",
      "# The stream_mode is set to \"messages\" to stream LLM tokens\n",
      "# The metadata contains information about the LLM invocation, including the tags\n",
      "async for msg, metadata in graph.astream(\n",
      "    {\"topic\": \"cats\"},\n",
      "    stream_mode=\"messages\",  \n",
      "):\n",
      "    # Filter the streamed tokens by the tags field in the metadata to only include\n",
      "    # the tokens from the LLM invocation with the \"joke\" tag\n",
      "    if metadata[\"tags\"] == [\"joke\"]:\n",
      "        print(msg.content, end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Evaluators\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream\n",
      "content===> for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
      "    print(chunk.text, end=\"|\", flush=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Model\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
      "from langchain.chat_models import init_chat_model\n",
      "from typing import Callable\n",
      "\n",
      "# Initialize models once outside the middleware\n",
      "large_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n",
      "standard_model = init_chat_model(\"gpt-4o\")\n",
      "efficient_model = init_chat_model(\"gpt-4o-mini\")\n",
      "\n",
      "@wrap_model_call\n",
      "def state_based_model(\n",
      "    request: ModelRequest,\n",
      "    handler: Callable[[ModelRequest], ModelResponse]\n",
      ") -> ModelResponse:\n",
      "    \"\"\"Select model based on State conversation length.\"\"\"\n",
      "    # request.messages is a shortcut for request.state[\"messages\"]\n",
      "    message_count = len(request.messages)  \n",
      "\n",
      "    if message_count > 20:\n",
      "        # Long conversation - use model with larger context window\n",
      "        model = large_model\n",
      "    elif message_count > 10:\n",
      "        # Medium conversation\n",
      "        model = standard_model\n",
      "    else:\n",
      "        # Short conversation - use efficient model\n",
      "        model = efficient_model\n",
      "\n",
      "    request = request.override(model=model)  \n",
      "\n",
      "    return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o-mini\",\n",
      "    tools=[...],\n",
      "    middleware=[state_based_model]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Review and edit state\n",
      "content===> import sqlite3\n",
      "from typing import TypedDict\n",
      "\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.types import Command, interrupt\n",
      "\n",
      "\n",
      "class ReviewState(TypedDict):\n",
      "    generated_text: str\n",
      "\n",
      "\n",
      "def review_node(state: ReviewState):\n",
      "    # Ask a reviewer to edit the generated content\n",
      "    updated = interrupt({\n",
      "        \"instruction\": \"Review and edit this content\",\n",
      "        \"content\": state[\"generated_text\"],\n",
      "    })\n",
      "    return {\"generated_text\": updated}\n",
      "\n",
      "\n",
      "builder = StateGraph(ReviewState)\n",
      "builder.add_node(\"review\", review_node)\n",
      "builder.add_edge(START, \"review\")\n",
      "builder.add_edge(\"review\", END)\n",
      "\n",
      "checkpointer = MemorySaver()\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "config = {\"configurable\": {\"thread_id\": \"review-42\"}}\n",
      "initial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\n",
      "print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]\n",
      "\n",
      "# Resume with the edited text from the reviewer\n",
      "final_state = graph.invoke(\n",
      "    Command(resume=\"Improved draft after review\"),\n",
      "    config=config,\n",
      ")\n",
      "print(final_state[\"generated_text\"])  # -> \"Improved draft after review\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> System Message\n",
      "content===> system_msg = SystemMessage(\"You are a helpful coding assistant.\")\n",
      "\n",
      "messages = [\n",
      "    system_msg,\n",
      "    HumanMessage(\"How do I create a REST API?\")\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Anthropic on Vertex AI Model Garden\n",
      "content===> from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"XAI_API_KEY\"):\n",
      "  os.environ[\"XAI_API_KEY\"] = getpass.getpass(\"Enter API key for xAI: \")\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"grok-2\", model_provider=\"xai\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Caching\n",
      "content===> import time\n",
      "from langchain_classic.embeddings import CacheBackedEmbeddings  \n",
      "from langchain_classic.storage import LocalFileStore \n",
      "from langchain_core.vectorstores import InMemoryVectorStore\n",
      "\n",
      "# Create your underlying embeddings model\n",
      "underlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.\n",
      "\n",
      "# Store persists embeddings to the local filesystem\n",
      "# This isn't for production use, but is useful for local\n",
      "store = LocalFileStore(\"./cache/\") \n",
      "\n",
      "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
      "    underlying_embeddings,\n",
      "    store,\n",
      "    namespace=underlying_embeddings.model\n",
      ")\n",
      "\n",
      "# Example: caching a query embedding\n",
      "tic = time.time()\n",
      "print(cached_embedder.embed_query(\"Hello, world!\"))\n",
      "print(f\"First call took: {time.time() - tic:.2f} seconds\")\n",
      "\n",
      "# Subsequent calls use the cache\n",
      "tic = time.time()\n",
      "print(cached_embedder.embed_query(\"Hello, world!\"))\n",
      "print(f\"Second call took: {time.time() - tic:.2f} seconds\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> ================================== Ai Message ==================================\n",
      "\n",
      "I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image captioning\n",
      "content===> from langchain_google_vertexai.vision_models import VertexAIImageCaptioning\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Approve or reject\n",
      "content===> import sqlite3\n",
      "from typing import Literal, Optional, TypedDict\n",
      "\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.types import Command, interrupt\n",
      "\n",
      "\n",
      "class ApprovalState(TypedDict):\n",
      "    action_details: str\n",
      "    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\n",
      "\n",
      "\n",
      "def approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\n",
      "    # Expose details so the caller can render them in a UI\n",
      "    decision = interrupt({\n",
      "        \"question\": \"Approve this action?\",\n",
      "        \"details\": state[\"action_details\"],\n",
      "    })\n",
      "\n",
      "    # Route to the appropriate node after resume\n",
      "    return Command(goto=\"proceed\" if decision else \"cancel\")\n",
      "\n",
      "\n",
      "def proceed_node(state: ApprovalState):\n",
      "    return {\"status\": \"approved\"}\n",
      "\n",
      "\n",
      "def cancel_node(state: ApprovalState):\n",
      "    return {\"status\": \"rejected\"}\n",
      "\n",
      "\n",
      "builder = StateGraph(ApprovalState)\n",
      "builder.add_node(\"approval\", approval_node)\n",
      "builder.add_node(\"proceed\", proceed_node)\n",
      "builder.add_node(\"cancel\", cancel_node)\n",
      "builder.add_edge(START, \"approval\")\n",
      "builder.add_edge(\"approval\", \"proceed\")\n",
      "builder.add_edge(\"approval\", \"cancel\")\n",
      "builder.add_edge(\"proceed\", END)\n",
      "builder.add_edge(\"cancel\", END)\n",
      "\n",
      "# Use a more durable checkpointer in production\n",
      "checkpointer = MemorySaver()\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "config = {\"configurable\": {\"thread_id\": \"approval-123\"}}\n",
      "initial = graph.invoke(\n",
      "    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\n",
      "    config=config,\n",
      ")\n",
      "print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'question': ..., 'details': ...})]\n",
      "\n",
      "# Resume with the decision; True routes to proceed, False to cancel\n",
      "resumed = graph.invoke(Command(resume=True), config=config)\n",
      "print(resumed[\"status\"])  # -> \"approved\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> [('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
      "[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Memory storage\n",
      "content===> from langgraph.store.memory import InMemoryStore\n",
      "\n",
      "\n",
      "def embed(texts: list[str]) -> list[list[float]]:\n",
      "    # Replace with an actual embedding function or LangChain embeddings object\n",
      "    return [[1.0, 2.0] * len(texts)]\n",
      "\n",
      "\n",
      "# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\n",
      "store = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\n",
      "user_id = \"my-user\"\n",
      "application_context = \"chitchat\"\n",
      "namespace = (user_id, application_context)\n",
      "store.put(\n",
      "    namespace,\n",
      "    \"a-memory\",\n",
      "    {\n",
      "        \"rules\": [\n",
      "            \"User likes short, direct language\",\n",
      "            \"User only speaks English & python\",\n",
      "        ],\n",
      "        \"my-key\": \"my-value\",\n",
      "    },\n",
      ")\n",
      "# get the \"memory\" by ID\n",
      "item = store.get(namespace, \"a-memory\")\n",
      "# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\n",
      "items = store.search(\n",
      "    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-astradb\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Athena\n",
      "content===> from langchain_community.document_loaders.athena import AthenaLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> import pytest\n",
      "\n",
      "@pytest.fixture(scope=\"session\")\n",
      "def vcr_config():\n",
      "    return {\n",
      "        \"filter_headers\": [\n",
      "            (\"authorization\", \"XXXX\"),\n",
      "            (\"x-api-key\", \"XXXX\"),\n",
      "            # ... other headers you want to mask\n",
      "        ],\n",
      "        \"filter_query_parameters\": [\n",
      "            (\"api_key\", \"XXXX\"),\n",
      "            (\"key\", \"XXXX\"),\n",
      "        ],\n",
      "    }\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Lens\n",
      "content===> pip install google-search-results langchain-community # Requires langchain-community\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Context still getting bloated\n",
      "content===> system_prompt=\"\"\"...\n",
      "\n",
      "IMPORTANT: Return only the essential summary.\n",
      "Do NOT include raw data, intermediate search results, or detailed tool outputs.\n",
      "Your response should be under 500 words.\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Error handling strategies\n",
      "content===> ToolStrategy(\n",
      "    schema=ProductRating,\n",
      "    handle_errors=\"Please provide a valid rating between 1-5 and include a comment.\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text structure-based\n",
      "content===> from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "\n",
      "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
      "texts = text_splitter.split_text(document)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Drive\n",
      "content===> pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Specify a backend\n",
      "content===> BackendFactory = Callable[[ToolRuntime], BackendProtocol]\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PowerBI individual tools\n",
      "content===> from langchain_community.tools.powerbi.tool import InfoPowerBITool\n",
      "from langchain_community.tools.powerbi.tool import ListPowerBITool\n",
      "from langchain_community.tools.powerbi.tool import QueryPowerBITool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PostgresStore (production)\n",
      "content===> from langgraph.store.postgres import PostgresStore\n",
      "import os\n",
      "\n",
      "store = PostgresStore(connection_string=os.environ[\"DATABASE_URL\"])\n",
      "agent = create_deep_agent(store=store, use_longterm_memory=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Edit tool arguments\n",
      "content===> if result.get(\"__interrupt__\"):\n",
      "    interrupts = result[\"__interrupt__\"][0].value\n",
      "    action_request = interrupts[\"action_requests\"][0]\n",
      "\n",
      "    # Original args from the agent\n",
      "    print(action_request[\"args\"])  # {\"to\": \"everyone@company.com\", ...}\n",
      "\n",
      "    # User decides to edit the recipient\n",
      "    decisions = [{\n",
      "        \"type\": \"edit\",\n",
      "        \"edited_action\": {\n",
      "            \"name\": action_request[\"name\"],  # Must include the tool name\n",
      "            \"args\": {\"to\": \"team@company.com\", \"subject\": \"...\", \"body\": \"...\"}\n",
      "        }\n",
      "    }]\n",
      "\n",
      "    result = agent.invoke(\n",
      "        Command(resume={\"decisions\": decisions}),\n",
      "        config=config\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content blocks\n",
      "content===> from langchain_anthropic import ChatAnthropic\n",
      "\n",
      "model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n",
      "response = model.invoke(\"What's the capital of France?\")\n",
      "\n",
      "# Unified access to content blocks\n",
      "for block in response.content_blocks:\n",
      "    if block[\"type\"] == \"reasoning\":\n",
      "        print(f\"Model reasoning: {block['reasoning']}\")\n",
      "    elif block[\"type\"] == \"text\":\n",
      "        print(f\"Response: {block['text']}\")\n",
      "    elif block[\"type\"] == \"tool_call\":\n",
      "        print(f\"Tool call: {block['name']}({block['args']})\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SearchApi\n",
      "content===> from langchain_community.utilities import SearchApiAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Enforce\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[...],\n",
      "    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> How it works\n",
      "content===> async for event in model.astream_events(\"Hello\"):\n",
      "\n",
      "    if event[\"event\"] == \"on_chat_model_start\":\n",
      "        print(f\"Input: {event['data']['input']}\")\n",
      "\n",
      "    elif event[\"event\"] == \"on_chat_model_stream\":\n",
      "        print(f\"Token: {event['data']['chunk'].text}\")\n",
      "\n",
      "    elif event[\"event\"] == \"on_chat_model_end\":\n",
      "        print(f\"Full message: {event['data']['output'].text}\")\n",
      "\n",
      "    else:\n",
      "        pass\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> # Note: The example code shows VertexAIMultiTurnSearchRetriever, confirm if VertexAISearchRetriever is separate or related.\n",
      "# Assuming it might be related or a typo in the original doc:\n",
      "from langchain_google_community import VertexAISearchRetriever # Verify class name if needed\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Serper.dev\n",
      "content===> from langchain_community.utilities import GoogleSerperAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream Writer\n",
      "content===> runtime.stream_writer\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bedrock Converse\n",
      "content===> from langchain_aws import ChatBedrockConverse\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Presidio\n",
      "content===> from langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Using with subgraphs called as functions\n",
      "content===> def node_in_parent_graph(state: State):\n",
      "    some_code()  # <-- This will re-execute when resumed\n",
      "    # Invoke a subgraph as a function.\n",
      "    # The subgraph contains an `interrupt` call.\n",
      "    subgraph_result = subgraph.invoke(some_input)\n",
      "\n",
      "async function node_in_subgraph(state: State) {\n",
      "    someOtherCode(); # <-- This will also re-execute when resumed\n",
      "    result = interrupt(\"What's your name?\")\n",
      "    ...\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Setup\n",
      "content===> import uuid\n",
      "\n",
      "from typing_extensions import TypedDict, NotRequired\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "\n",
      "\n",
      "class State(TypedDict):\n",
      "    topic: NotRequired[str]\n",
      "    joke: NotRequired[str]\n",
      "\n",
      "\n",
      "model = init_chat_model(\n",
      "    \"claude-sonnet-4-5-20250929\",\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "\n",
      "def generate_topic(state: State):\n",
      "    \"\"\"LLM call to generate a topic for the joke\"\"\"\n",
      "    msg = model.invoke(\"Give me a funny topic for a joke\")\n",
      "    return {\"topic\": msg.content}\n",
      "\n",
      "\n",
      "def write_joke(state: State):\n",
      "    \"\"\"LLM call to write a joke based on the topic\"\"\"\n",
      "    msg = model.invoke(f\"Write a short joke about {state['topic']}\")\n",
      "    return {\"joke\": msg.content}\n",
      "\n",
      "\n",
      "# Build workflow\n",
      "workflow = StateGraph(State)\n",
      "\n",
      "# Add nodes\n",
      "workflow.add_node(\"generate_topic\", generate_topic)\n",
      "workflow.add_node(\"write_joke\", write_joke)\n",
      "\n",
      "# Add edges to connect nodes\n",
      "workflow.add_edge(START, \"generate_topic\")\n",
      "workflow.add_edge(\"generate_topic\", \"write_joke\")\n",
      "workflow.add_edge(\"write_joke\", END)\n",
      "\n",
      "# Compile\n",
      "checkpointer = InMemorySaver()\n",
      "graph = workflow.compile(checkpointer=checkpointer)\n",
      "graph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Comprehend Moderation Chain\n",
      "content===> from langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLMs and augmentations\n",
      "content===> # Schema for structured output\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class SearchQuery(BaseModel):\n",
      "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
      "    justification: str = Field(\n",
      "        None, description=\"Why this query is relevant to the user's request.\"\n",
      "    )\n",
      "\n",
      "\n",
      "# Augment the LLM with schema for structured output\n",
      "structured_llm = llm.with_structured_output(SearchQuery)\n",
      "\n",
      "# Invoke the augmented LLM\n",
      "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
      "\n",
      "# Define a tool\n",
      "def multiply(a: int, b: int) -> int:\n",
      "    return a * b\n",
      "\n",
      "# Augment the LLM with tools\n",
      "llm_with_tools = llm.bind_tools([multiply])\n",
      "\n",
      "# Invoke the LLM with input that triggers the tool call\n",
      "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
      "\n",
      "# Get the tool call\n",
      "msg.tool_calls\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import faiss\n",
      "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
      "from langchain_community.vectorstores import FAISS\n",
      "\n",
      "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
      "index = faiss.IndexFlatL2(embedding_dim)\n",
      "\n",
      "vector_store = FAISS(\n",
      "    embedding_function=embeddings,\n",
      "    index=index,\n",
      "    docstore=InMemoryDocstore(),\n",
      "    index_to_docstore_id={},\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> SerpApi\n",
      "content===> from langchain_community.utilities import SerpAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Reads\n",
      "content===> from langchain.tools import tool, ToolRuntime\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "@tool\n",
      "def check_authentication(\n",
      "    runtime: ToolRuntime\n",
      ") -> str:\n",
      "    \"\"\"Check if user is authenticated.\"\"\"\n",
      "    # Read from State: check current auth status\n",
      "    current_state = runtime.state\n",
      "    is_authenticated = current_state.get(\"authenticated\", False)\n",
      "\n",
      "    if is_authenticated:\n",
      "        return \"User is authenticated\"\n",
      "    else:\n",
      "        return \"User is not authenticated\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[check_authentication]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Before agent guardrails\n",
      "content===> from typing import Any\n",
      "\n",
      "from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n",
      "from langgraph.runtime import Runtime\n",
      "\n",
      "class ContentFilterMiddleware(AgentMiddleware):\n",
      "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
      "\n",
      "    def __init__(self, banned_keywords: list[str]):\n",
      "        super().__init__()\n",
      "        self.banned_keywords = [kw.lower() for kw in banned_keywords]\n",
      "\n",
      "    @hook_config(can_jump_to=[\"end\"])\n",
      "    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
      "        # Get the first user message\n",
      "        if not state[\"messages\"]:\n",
      "            return None\n",
      "\n",
      "        first_message = state[\"messages\"][0]\n",
      "        if first_message.type != \"human\":\n",
      "            return None\n",
      "\n",
      "        content = first_message.content.lower()\n",
      "\n",
      "        # Check for banned keywords\n",
      "        for keyword in self.banned_keywords:\n",
      "            if keyword in content:\n",
      "                # Block execution before any processing\n",
      "                return {\n",
      "                    \"messages\": [{\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n",
      "                    }],\n",
      "                    \"jump_to\": \"end\"\n",
      "                }\n",
      "\n",
      "        return None\n",
      "\n",
      "# Use the custom guardrail\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[search_tool, calculator_tool],\n",
      "    middleware=[\n",
      "        ContentFilterMiddleware(\n",
      "            banned_keywords=[\"hack\", \"exploit\", \"malware\"]\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "# This request will be blocked before any processing\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n",
      "})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> HuggingFaceEmbeddings\n",
      "content===> HuggingFaceEmbeddings\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Get state\n",
      "content===> StateSnapshot(\n",
      "    values={'foo': 'b', 'bar': ['a', 'b']},\n",
      "    next=(),\n",
      "    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n",
      "    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n",
      "    created_at='2024-08-29T19:19:38.821749+00:00',\n",
      "    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Repository structure\n",
      "content===> langchain-google-genai\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> YouTube Transcripts Loader\n",
      "content===> youtube-transcript-api\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> VertexPairWiseStringEvaluator\n",
      "content===> from langchain_google_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Repository structure\n",
      "content===> langchain-standard-tests\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Static model\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(\n",
      "    model=\"gpt-5\",\n",
      "    temperature=0.1,\n",
      "    max_tokens=1000,\n",
      "    timeout=30\n",
      "    # ... (other params)\n",
      ")\n",
      "agent = create_agent(model, tools=tools)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool error handling\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import wrap_tool_call\n",
      "from langchain_core.messages import ToolMessage\n",
      "\n",
      "\n",
      "@wrap_tool_call\n",
      "def handle_tool_errors(request, handler):\n",
      "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
      "    try:\n",
      "        return handler(request)\n",
      "    except Exception as e:\n",
      "        # Return a custom error message to the model\n",
      "        return ToolMessage(\n",
      "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
      "            tool_call_id=request.tool_call[\"id\"]\n",
      "        )\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[search, get_weather],\n",
      "    middleware=[handle_tool_errors]\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> test_agent_trajectory.yaml\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Standard content blocks\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling strategy\n",
      "content===> class ToolStrategy(Generic[SchemaT]):\n",
      "    schema: type[SchemaT]\n",
      "    tool_message_content: str | None\n",
      "    handle_errors: Union[\n",
      "        bool,\n",
      "        str,\n",
      "        type[Exception],\n",
      "        tuple[type[Exception], ...],\n",
      "        Callable[[Exception], str],\n",
      "    ]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Log probabilities\n",
      "content===> model = init_chat_model(\n",
      "    model=\"gpt-4o\",\n",
      "    model_provider=\"openai\"\n",
      ").bind(logprobs=True)\n",
      "\n",
      "response = model.invoke(\"Why do parrots talk?\")\n",
      "print(response.response_metadata[\"logprobs\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI Search\n",
      "content===> google-cloud-discoveryengine\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install\n",
      "content===> from langgraph.graph import StateGraph, MessagesState, START, END\n",
      "\n",
      "def mock_llm(state: MessagesState):\n",
      "    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
      "\n",
      "graph = StateGraph(MessagesState)\n",
      "graph.add_node(mock_llm)\n",
      "graph.add_edge(START, \"mock_llm\")\n",
      "graph.add_edge(\"mock_llm\", END)\n",
      "graph = graph.compile()\n",
      "\n",
      "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/overview\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Specify a backend\n",
      "content===> create_deep_agent(backend=...)\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face dataset\n",
      "content===> pip install datasets\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "from langgraph.checkpoint.postgres import PostgresSaver  \n",
      "\n",
      "model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
      "\n",
      "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
      "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n",
      "    # checkpointer.setup()\n",
      "\n",
      "    def call_model(state: MessagesState):\n",
      "        response = model.invoke(state[\"messages\"])\n",
      "        return {\"messages\": response}\n",
      "\n",
      "    builder = StateGraph(MessagesState)\n",
      "    builder.add_node(call_model)\n",
      "    builder.add_edge(START, \"call_model\")\n",
      "\n",
      "    graph = builder.compile(checkpointer=checkpointer)  \n",
      "\n",
      "    config = {\n",
      "        \"configurable\": {\n",
      "            \"thread_id\": \"1\"\n",
      "        }\n",
      "    }\n",
      "\n",
      "    for chunk in graph.stream(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
      "        config,  \n",
      "        stream_mode=\"values\"\n",
      "    ):\n",
      "        chunk[\"messages\"][-1].pretty_print()\n",
      "\n",
      "    for chunk in graph.stream(\n",
      "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
      "        config,  \n",
      "        stream_mode=\"values\"\n",
      "    ):\n",
      "        chunk[\"messages\"][-1].pretty_print()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Azure PowerBI\n",
      "content===> pip install azure-identity\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Log to a project\n",
      "content===> export LANGSMITH_PROJECT=my-agent-project\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream custom data\n",
      "content===> from typing import TypedDict\n",
      "from langgraph.config import get_stream_writer\n",
      "from langgraph.graph import StateGraph, START\n",
      "\n",
      "class State(TypedDict):\n",
      "    query: str\n",
      "    answer: str\n",
      "\n",
      "def node(state: State):\n",
      "    # Get the stream writer to send custom data\n",
      "    writer = get_stream_writer()\n",
      "    # Emit a custom key-value pair (e.g., progress update)\n",
      "    writer({\"custom_key\": \"Generating custom data inside node\"})\n",
      "    return {\"answer\": \"some data\"}\n",
      "\n",
      "graph = (\n",
      "    StateGraph(State)\n",
      "    .add_node(node)\n",
      "    .add_edge(START, \"node\")\n",
      "    .compile()\n",
      ")\n",
      "\n",
      "inputs = {\"query\": \"example\"}\n",
      "\n",
      "# Set stream_mode=\"custom\" to receive the custom data in the stream\n",
      "for chunk in graph.stream(inputs, stream_mode=\"custom\"):\n",
      "    print(chunk)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> System prompt\n",
      "content===> from deepagents import create_deep_agent\n",
      "\n",
      "research_instructions = \"\"\"\\\n",
      "You are an expert researcher. Your job is to conduct \\\n",
      "thorough research, and then write a polished report. \\\n",
      "\"\"\"\n",
      "\n",
      "agent = create_deep_agent(\n",
      "    system_prompt=research_instructions,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Amazon Bedrock (Knowledge Bases)\n",
      "content===> pip install langchain-aws\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 3. Update the state\n",
      "content===> {'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Namespace\n",
      "content===> langchain.chat_models\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure SQL Database\n",
      "content===> !pip install langchain-sqlserver==0.1.1\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use a virtual filesystem\n",
      "content===> WHERE path LIKE $1 || '%'\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic Usage\n",
      "content===> user_id = \"1\"\n",
      "namespace_for_memory = (user_id, \"memories\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Basic Usage\n",
      "content===> memory_id = str(uuid.uuid4())\n",
      "memory = {\"food_preference\" : \"I like pizza\"}\n",
      "in_memory_store.put(namespace_for_memory, memory_id, memory)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. Test your application in Studio\n",
      "content===> langgraph dev --tunnel\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use MCP tools\n",
      "content===> langchain-mcp-adapters\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> File structure\n",
      "content===> my-app/\n",
      "├── my_agent # all project code lies within here\n",
      "│   ├── utils # utilities for your graph\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── tools.py # tools for your graph\n",
      "│   │   ├── nodes.py # node functions for your graph\n",
      "│   │   └── state.py # state definition of your graph\n",
      "│   ├── __init__.py\n",
      "│   └── agent.py # code for constructing your graph\n",
      "├── .env # environment variables\n",
      "├── requirements.txt # package dependencies\n",
      "└── langgraph.json # configuration file for LangGraph\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> CompositeBackend (router)\n",
      "content===> from deepagents import create_deep_agent\n",
      "from deepagents.backends import FilesystemBackend\n",
      "from deepagents.backends.composite import build_composite_state_backend\n",
      "\n",
      "composite_backend = lambda rt: CompositeBackend(\n",
      "    default=StateBackend(rt)\n",
      "    routes={\n",
      "        \"/memories/\": StoreBackend(rt),\n",
      "        \"/docs/\": CustomBackend()\n",
      "    }\n",
      ")\n",
      "\n",
      "agent = create_deep_agent(backend=composite_backend)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Dynamically selecting tools\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import AgentMiddleware, ModelRequest\n",
      "from typing import Callable\n",
      "\n",
      "\n",
      "class ToolSelectorMiddleware(AgentMiddleware):\n",
      "    def wrap_model_call(\n",
      "        self,\n",
      "        request: ModelRequest,\n",
      "        handler: Callable[[ModelRequest], ModelResponse],\n",
      "    ) -> ModelResponse:\n",
      "        \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n",
      "        # Select a small, relevant subset of tools based on state/context\n",
      "        relevant_tools = select_relevant_tools(request.state, request.runtime)\n",
      "        request.tools = relevant_tools\n",
      "        return handler(request)\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=all_tools,  # All available tools need to be registered upfront\n",
      "    # Middleware can be used to select a smaller subset that's relevant for the given run.\n",
      "    middleware=[ToolSelectorMiddleware()],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Invocation\n",
      "content===> result = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agent jumps\n",
      "content===> from langchain.agents.middleware import AgentMiddleware, hook_config\n",
      "from typing import Any\n",
      "\n",
      "class ConditionalMiddleware(AgentMiddleware):\n",
      "    @hook_config(can_jump_to=[\"end\", \"tools\"])\n",
      "    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
      "        if some_condition(state):\n",
      "            return {\"jump_to\": \"end\"}\n",
      "        return None\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> evaluator = create_trajectory_llm_as_judge(\n",
      "    model=\"openai:o3-mini\",\n",
      "    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n",
      ")\n",
      "evaluation = judge_with_reference(\n",
      "    outputs=result[\"messages\"],\n",
      "    reference_outputs=reference_trajectory,\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Popular providers\n",
      "content===> langchain-nvidia-ai-endpoints\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Handle interrupts\n",
      "content===> import uuid\n",
      "from langgraph.types import Command\n",
      "\n",
      "# Create config with thread_id for state persistence\n",
      "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
      "\n",
      "# Invoke the agent\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Delete the file temp.txt\"}]\n",
      "}, config=config)\n",
      "\n",
      "# Check if execution was interrupted\n",
      "if result.get(\"__interrupt__\"):\n",
      "    # Extract interrupt information\n",
      "    interrupts = result[\"__interrupt__\"][0].value\n",
      "    action_requests = interrupts[\"action_requests\"]\n",
      "    review_configs = interrupts[\"review_configs\"]\n",
      "\n",
      "    # Create a lookup map from tool name to review config\n",
      "    config_map = {cfg[\"action_name\"]: cfg for cfg in review_configs}\n",
      "\n",
      "    # Display the pending actions to the user\n",
      "    for action in action_requests:\n",
      "        review_config = config_map[action[\"name\"]]\n",
      "        print(f\"Tool: {action['name']}\")\n",
      "        print(f\"Arguments: {action['args']}\")\n",
      "        print(f\"Allowed decisions: {review_config['allowed_decisions']}\")\n",
      "\n",
      "    # Get user decisions (one per action_request, in order)\n",
      "    decisions = [\n",
      "        {\"type\": \"approve\"}  # User approved the deletion\n",
      "    ]\n",
      "\n",
      "    # Resume execution with decisions\n",
      "    result = agent.invoke(\n",
      "        Command(resume={\"decisions\": decisions}),\n",
      "        config=config  # Must use the same config!\n",
      "    )\n",
      "\n",
      "# Process final result\n",
      "print(result[\"messages\"][-1][\"content\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Checkpointer libraries\n",
      "content===> langgraph-checkpoint-sqlite\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 3. Environment variables\n",
      "content===> LANGSMITH_API_KEY=lsv2...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI visual QnA\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Handle errors appropriately\n",
      "content===> from langgraph.types import RetryPolicy\n",
      "\n",
      "workflow.add_node(\n",
      "    \"search_documentation\",\n",
      "    search_documentation,\n",
      "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Trends\n",
      "content===> google-search-results\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Delete messages\n",
      "content===> from langchain.messages import RemoveMessage  \n",
      "\n",
      "def delete_messages(state):\n",
      "    messages = state[\"messages\"]\n",
      "    if len(messages) > 2:\n",
      "        # remove the earliest two messages\n",
      "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Cloud Providers\n",
      "content===> AzureBlobStorageLoader\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Simplified namespace\n",
      "content===> pip install -U langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calls\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "\n",
      "def get_weather(location: str) -> str:\n",
      "    \"\"\"Get the weather at a location.\"\"\"\n",
      "    ...\n",
      "\n",
      "model_with_tools = model.bind_tools([get_weather])\n",
      "response = model_with_tools.invoke(\"What's the weather in Paris?\")\n",
      "\n",
      "for tool_call in response.tool_calls:\n",
      "    print(f\"Tool: {tool_call['name']}\")\n",
      "    print(f\"Args: {tool_call['args']}\")\n",
      "    print(f\"ID: {tool_call['id']}\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Filter by node\n",
      "content===> # The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n",
      "# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n",
      "# with information about the graph node where the LLM was called and other information\n",
      "for msg, metadata in graph.stream(\n",
      "    inputs,\n",
      "    stream_mode=\"messages\",  \n",
      "):\n",
      "    # Filter the streamed tokens by the langgraph_node field in the metadata\n",
      "    # to only include the tokens from the specified node\n",
      "    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\n",
      "        ...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI Warehouse\n",
      "content===> GoogleDocumentAIWarehouseRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Content block reference\n",
      "content===> {\n",
      "    \"type\": \"reasoning\",\n",
      "    \"reasoning\": \"The user is asking about...\",\n",
      "    \"extras\": {\"signature\": \"abc123\"},\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Approve or reject\n",
      "content===> # To approve\n",
      "graph.invoke(Command(resume=True), config=config)\n",
      "\n",
      "# To reject\n",
      "graph.invoke(Command(resume=False), config=config)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Serialization withpickle\n",
      "content===> from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n",
      "\n",
      "# ... Define the graph ...\n",
      "graph.compile(\n",
      "    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Llama on Vertex AI Model Garden\n",
      "content===> from langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> ProviderStrategy\n",
      "content===> response_format=ContactInfo\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma local from Hugging Face\n",
      "content===> langchain-google-vertexai\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Create a LangGraph app 🌱\n",
      "content===> langgraph new path/to/your/app --template new-langgraph-project-python\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Gemma local from Hugging Face\n",
      "content===> from langchain_google_vertexai.gemma import GemmaLocalHF\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import SummarizationMiddleware\n",
      "from langgraph.checkpoint.memory import InMemorySaver\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "\n",
      "\n",
      "checkpointer = InMemorySaver()\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",\n",
      "    tools=[],\n",
      "    middleware=[\n",
      "        SummarizationMiddleware(\n",
      "            model=\"gpt-4o-mini\",\n",
      "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
      "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
      "        )\n",
      "    ],\n",
      "    checkpointer=checkpointer,\n",
      ")\n",
      "\n",
      "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
      "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
      "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
      "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
      "\n",
      "final_response[\"messages\"][-1].pretty_print()\n",
      "\"\"\"\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "Your name is Bob!\n",
      "\"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Do not reorderinterruptcalls within a node\n",
      "content===> def node_a(state: State):\n",
      "    # ❌ Bad: conditionally skipping interrupts changes the order\n",
      "    name = interrupt(\"What's your name?\")\n",
      "\n",
      "    # On first run, this might skip the interrupt\n",
      "    # On resume, it might not skip it - causing index mismatch\n",
      "    if state.get(\"needs_age\"):\n",
      "        age = interrupt(\"What's your age?\")\n",
      "\n",
      "    city = interrupt(\"What's your city?\")\n",
      "\n",
      "    return {\"name\": name, \"city\": city}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> AzureCosmosDBNoSqlVectorStore\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> embed_query(text: str) → List[float]\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Response Format\n",
      "content===> ToolStrategy[StructuredResponseT]\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Identify a checkpoint\n",
      "content===> # The states are returned in reverse chronological order.\n",
      "states = list(graph.get_state_history(config))\n",
      "\n",
      "for state in states:\n",
      "    print(state.next)\n",
      "    print(state.config[\"configurable\"][\"checkpoint_id\"])\n",
      "    print()\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 6. View your agent in Studio\n",
      "content===> https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stateful tool usage\n",
      "content===> from langchain_mcp_adapters.tools import load_mcp_tools\n",
      "\n",
      "client = MultiServerMCPClient({...})\n",
      "async with client.session(\"math\") as session:\n",
      "    tools = await load_mcp_tools(session)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Image captions\n",
      "content===> pip install transformers pillow\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 1. Install the LangGraph CLI\n",
      "content===> # Python >= 3.11 is required.\n",
      "pip install -U \"langgraph-cli[inmem]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Do not reorderinterruptcalls within a node\n",
      "content===> def node_a(state: State):\n",
      "    # ✅ Good: interrupt calls happen in the same order every time\n",
      "    name = interrupt(\"What's your name?\")\n",
      "    age = interrupt(\"What's your age?\")\n",
      "    city = interrupt(\"What's your city?\")\n",
      "\n",
      "    return {\n",
      "        \"name\": name,\n",
      "        \"age\": age,\n",
      "        \"city\": city\n",
      "    }\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-pinecone\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft PowerPoint\n",
      "content===> from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Bigtable\n",
      "content===> pip install langchain-google-bigtable\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Minor changes\n",
      "content===> LanguageModelOutputVar\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Drive\n",
      "content===> pip install langchain-google-community[drive]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Keep system prompts detailed\n",
      "content===> research_subagent = {\n",
      "    \"name\": \"research-agent\",\n",
      "    \"description\": \"Conducts in-depth research using web search and synthesizes findings\",\n",
      "    \"system_prompt\": \"\"\"You are a thorough researcher. Your job is to:\n",
      "\n",
      "    1. Break down the research question into searchable queries\n",
      "    2. Use internet_search to find relevant information\n",
      "    3. Synthesize findings into a comprehensive but concise summary\n",
      "    4. Cite sources when making claims\n",
      "\n",
      "    Output format:\n",
      "    - Summary (2-3 paragraphs)\n",
      "    - Key findings (bullet points)\n",
      "    - Sources (with URLs)\n",
      "\n",
      "    Keep your response under 500 words to maintain clean context.\"\"\",\n",
      "    \"tools\": [internet_search],\n",
      "}\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Container Apps dynamic sessions\n",
      "content===> POOL_MANAGEMENT_ENDPOINT\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Handling tool errors\n",
      "content===> # Example coming soon\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft Azure PowerBI\n",
      "content===> from langchain_community.agent_toolkits import PowerBIToolkit\n",
      "from langchain_community.utilities.powerbi import PowerBIDataset\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Text-to-Speech\n",
      "content===> from langchain_google_community import TextToSpeechTool\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU langchain-postgres\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> langchain-classic\n",
      "content===> pip install langchain-classic\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Namespace\n",
      "content===> # Agent building\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "# Messages and content\n",
      "from langchain.messages import AIMessage, HumanMessage\n",
      "\n",
      "# Tools\n",
      "from langchain.tools import tool\n",
      "\n",
      "# Model initialization\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langchain.embeddings import init_embeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Streaming\n",
      "content===> for chunk in agent.stream({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n",
      "}, stream_mode=\"values\"):\n",
      "    # Each chunk contains the full state at that point\n",
      "    latest_message = chunk[\"messages\"][-1]\n",
      "    if latest_message.content:\n",
      "        print(f\"Agent: {latest_message.content}\")\n",
      "    elif latest_message.tool_calls:\n",
      "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use in production\n",
      "content===> pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Places\n",
      "content===> pip install googlemaps langchain # Requires base langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Summarize messages\n",
      "content===> from langgraph.graph import MessagesState\n",
      "class State(MessagesState):\n",
      "    summary: str\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> System Message\n",
      "content===> from langchain.messages import SystemMessage, HumanMessage\n",
      "\n",
      "system_msg = SystemMessage(\"\"\"\n",
      "You are a senior Python developer with expertise in web frameworks.\n",
      "Always provide code examples and explain your reasoning.\n",
      "Be concise but thorough in your explanations.\n",
      "\"\"\")\n",
      "\n",
      "messages = [\n",
      "    system_msg,\n",
      "    HumanMessage(\"How do I create a REST API?\")\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> BigQuery\n",
      "content===> from langchain_google_community import BigQueryLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Multiple tool calls\n",
      "content===> config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [{\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Delete temp.txt and send an email to admin@example.com\"\n",
      "    }]\n",
      "}, config=config)\n",
      "\n",
      "if result.get(\"__interrupt__\"):\n",
      "    interrupts = result[\"__interrupt__\"][0].value\n",
      "    action_requests = interrupts[\"action_requests\"]\n",
      "\n",
      "    # Two tools need approval\n",
      "    assert len(action_requests) == 2\n",
      "\n",
      "    # Provide decisions in the same order as action_requests\n",
      "    decisions = [\n",
      "        {\"type\": \"approve\"},  # First tool: delete_file\n",
      "        {\"type\": \"reject\"}    # Second tool: send_email\n",
      "    ]\n",
      "\n",
      "    result = agent.invoke(\n",
      "        Command(resume={\"decisions\": decisions}),\n",
      "        config=config\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure Cognitive Services\n",
      "content===> pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Connect to your agent\n",
      "content===> http://localhost:2024\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Caching\n",
      "content===> document_embedding_cache\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deprecations\n",
      "content===> langchain.agents.middleware.human_in_the_loop.HITLRequest\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Running tests locally\n",
      "content===> make integration_tests\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Interface\n",
      "content===> mdelete(key: Sequence[str]) -> None\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> In production\n",
      "content===> pip install langgraph-checkpoint-postgres\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Hugging Face model loader\n",
      "content===> Hugging Face Models API\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 5. Test the API\n",
      "content===> from langgraph_sdk import get_sync_client # or get_client for async\n",
      "\n",
      "client = get_sync_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")\n",
      "\n",
      "for chunk in client.runs.stream(\n",
      "    None,    # Threadless run\n",
      "    \"agent\", # Name of agent. Defined in langgraph.json.\n",
      "    input={\n",
      "        \"messages\": [{\n",
      "            \"role\": \"human\",\n",
      "            \"content\": \"What is LangGraph?\",\n",
      "        }],\n",
      "    },\n",
      "    stream_mode=\"updates\",\n",
      "):\n",
      "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
      "    print(chunk.data)\n",
      "    print(\"\\n\\n\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/deploy\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Scholar\n",
      "content===> from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
      "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> from langchain_aws import BedrockEmbeddings\n",
      "\n",
      "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add policy hooks\n",
      "content===> from deepagents.backends.filesystem import FilesystemBackend\n",
      "from deepagents.backends.protocol import WriteResult, EditResult\n",
      "\n",
      "class GuardedBackend(FilesystemBackend):\n",
      "    def __init__(self, *, deny_prefixes: list[str], **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.deny_prefixes = [p if p.endswith(\"/\") else p + \"/\" for p in deny_prefixes]\n",
      "\n",
      "    def write(self, file_path: str, content: str) -> WriteResult:\n",
      "        if any(file_path.startswith(p) for p in self.deny_prefixes):\n",
      "            return WriteResult(error=f\"Writes are not allowed under {file_path}\")\n",
      "        return super().write(file_path, content)\n",
      "\n",
      "    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:\n",
      "        if any(file_path.startswith(p) for p in self.deny_prefixes):\n",
      "            return EditResult(error=f\"Edits are not allowed under {file_path}\")\n",
      "        return super().edit(file_path, old_string, new_string, replace_all)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use a virtual filesystem\n",
      "content===> from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult\n",
      "from deepagents.backends.utils import FileInfo, GrepMatch\n",
      "\n",
      "class S3Backend(BackendProtocol):\n",
      "    def __init__(self, bucket: str, prefix: str = \"\"):\n",
      "        self.bucket = bucket\n",
      "        self.prefix = prefix.rstrip(\"/\")\n",
      "\n",
      "    def _key(self, path: str) -> str:\n",
      "        return f\"{self.prefix}{path}\"\n",
      "\n",
      "    def ls_info(self, path: str) -> list[FileInfo]:\n",
      "        # List objects under _key(path); build FileInfo entries (path, size, modified_at)\n",
      "        ...\n",
      "\n",
      "    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:\n",
      "        # Fetch object; return numbered content or an error string\n",
      "        ...\n",
      "\n",
      "    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:\n",
      "        # Optionally filter server‑side; else list and scan content\n",
      "        ...\n",
      "\n",
      "    def glob_info(self, pattern: str, path: str = \"/\") -> list[FileInfo]:\n",
      "        # Apply glob relative to path across keys\n",
      "        ...\n",
      "\n",
      "    def write(self, file_path: str, content: str) -> WriteResult:\n",
      "        # Enforce create‑only semantics; return WriteResult(path=file_path, files_update=None)\n",
      "        ...\n",
      "\n",
      "    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:\n",
      "        # Read → replace (respect uniqueness vs replace_all) → write → return occurrences\n",
      "        ...\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure OpenAI\n",
      "content===> from langchain_openai import AzureOpenAIEmbeddings\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> LangGraph v1 migration guide\n",
      "content===> pip install -U langgraph langchain-core\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Token usage\n",
      "content===> from langchain.chat_models import init_chat_model\n",
      "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
      "\n",
      "model_1 = init_chat_model(model=\"gpt-4o-mini\")\n",
      "model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
      "\n",
      "callback = UsageMetadataCallbackHandler()\n",
      "result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
      "result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
      "callback.usage_metadata\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Subagent middleware\n",
      "content===> from langchain_core.tools import tool\n",
      "from langchain.agents import create_agent\n",
      "from deepagents.middleware.subagents import SubAgentMiddleware\n",
      "\n",
      "\n",
      "@tool\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get the weather in a city.\"\"\"\n",
      "    return f\"The weather in {city} is sunny.\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    middleware=[\n",
      "        SubAgentMiddleware(\n",
      "            default_model=\"claude-sonnet-4-5-20250929\",\n",
      "            default_tools=[],\n",
      "            subagents=[\n",
      "                {\n",
      "                    \"name\": \"weather\",\n",
      "                    \"description\": \"This subagent can get weather in cities.\",\n",
      "                    \"system_prompt\": \"Use the get_weather tool to get the weather in a city.\",\n",
      "                    \"tools\": [get_weather],\n",
      "                    \"model\": \"gpt-4.1\",\n",
      "                    \"middleware\": [],\n",
      "                }\n",
      "            ],\n",
      "        )\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> with_structured_output\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Execution order\n",
      "content===> middleware1.wrap_model_call()\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> PlayWright Browser Toolkit\n",
      "content===> pip install playwright lxml\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"WATSONX_APIKEY\"):\n",
      "  os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n",
      "\n",
      "from langchain_ibm import WatsonxEmbeddings\n",
      "\n",
      "embeddings = WatsonxEmbeddings(\n",
      "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
      "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
      "    project_id=\"<WATSONX PROJECT_ID>\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add metadata to traces\n",
      "content===> response = agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]},\n",
      "    config={\n",
      "        \"tags\": [\"production\", \"email-assistant\", \"v1.0\"],\n",
      "        \"metadata\": {\n",
      "            \"user_id\": \"user_123\",\n",
      "            \"session_id\": \"session_456\",\n",
      "            \"environment\": \"production\"\n",
      "        }\n",
      "    }\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agent jumps\n",
      "content===> @hook_config(can_jump_to=[...])\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Configurable models\n",
      "content===> from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class GetWeather(BaseModel):\n",
      "    \"\"\"Get the current weather in a given location\"\"\"\n",
      "\n",
      "        location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      "\n",
      "\n",
      "class GetPopulation(BaseModel):\n",
      "    \"\"\"Get the current population in a given location\"\"\"\n",
      "\n",
      "        location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      "\n",
      "\n",
      "model = init_chat_model(temperature=0)\n",
      "model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n",
      "\n",
      "model_with_tools.invoke(\n",
      "    \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\n",
      ").tool_calls\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Add persistence\n",
      "content===> subgraph_builder = StateGraph(...)\n",
      "subgraph = subgraph_builder.compile(checkpointer=True)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Static runtime context\n",
      "content===> @dataclass\n",
      "class ContextSchema:\n",
      "    user_name: str\n",
      "\n",
      "graph.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]},\n",
      "    context={\"user_name\": \"John Smith\"}  \n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> View thread state\n",
      "content===> config = {\n",
      "    \"configurable\": {\n",
      "        \"thread_id\": \"1\",  \n",
      "        # optionally provide an ID for a specific checkpoint,\n",
      "        # otherwise the latest checkpoint is shown\n",
      "        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #\n",
      "\n",
      "    }\n",
      "}\n",
      "graph.get_state(config)  \n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Step 5: Run the agent\n",
      "content===> result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
      "\n",
      "# Print the agent's response\n",
      "print(result[\"messages\"][-1].content)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Example\n",
      "content===> task(name=\"general-purpose\", task=\"Research quantum computing trends\")\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Azure AI Data\n",
      "content===> from langchain.document_loaders import AzureAIDataLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Microsoft OneDrive File\n",
      "content===> from langchain_community.document_loaders import OneDriveFileLoader\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Deploy DocumentDB on AWS\n",
      "content===> from langchain_community.vectorstores import DocumentDBVectorSearch\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> 2. Prepare your agent\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "def send_email(to: str, subject: str, body: str):\n",
      "    \"\"\"Send an email\"\"\"\n",
      "    email = {\n",
      "        \"to\": to,\n",
      "        \"subject\": subject,\n",
      "        \"body\": body\n",
      "    }\n",
      "    # ... email sending logic\n",
      "\n",
      "    return f\"Email sent to {to}\"\n",
      "\n",
      "agent = create_agent(\n",
      "    \"gpt-4o\",\n",
      "    tools=[send_email],\n",
      "    system_prompt=\"You are an email assistant. Always use the send_email tool.\",\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Agent progress\n",
      "content===> from langchain.agents import create_agent\n",
      "\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-5-nano\",\n",
      "    tools=[get_weather],\n",
      ")\n",
      "for chunk in agent.stream(  \n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
      "    stream_mode=\"updates\",\n",
      "):\n",
      "    for step, data in chunk.items():\n",
      "        print(f\"step: {step}\")\n",
      "        print(f\"content: {data['messages'][-1].content_blocks}\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Message content\n",
      "content===> from langchain.messages import HumanMessage\n",
      "\n",
      "# String content\n",
      "human_message = HumanMessage(\"Hello, how are you?\")\n",
      "\n",
      "# Provider-native format (e.g., OpenAI)\n",
      "human_message = HumanMessage(content=[\n",
      "    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
      "    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n",
      "])\n",
      "\n",
      "# List of standard content blocks\n",
      "human_message = HumanMessage(content_blocks=[\n",
      "    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
      "    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n",
      "])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Do not wrapinterruptcalls in try/except\n",
      "content===> def node_a(state: State):\n",
      "    # ❌ Bad: wrapping interrupt in bare try/except\n",
      "    # will catch the interrupt exception\n",
      "    try:\n",
      "        interrupt(\"What's your name?\")\n",
      "    except Exception as e:\n",
      "        print(e)\n",
      "    return state\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> High-level API\n",
      "content===> print(graph.channels)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Featured providers\n",
      "content===> langchain-nvidia-ai-endpoints\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Use semantic search\n",
      "content===> \n",
      "from langchain.embeddings import init_embeddings\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langgraph.store.base import BaseStore\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "from langgraph.graph import START, MessagesState, StateGraph\n",
      "\n",
      "model = init_chat_model(\"gpt-4o-mini\")\n",
      "\n",
      "# Create store with semantic search enabled\n",
      "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
      "store = InMemoryStore(\n",
      "    index={\n",
      "        \"embed\": embeddings,\n",
      "        \"dims\": 1536,\n",
      "    }\n",
      ")\n",
      "\n",
      "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
      "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n",
      "\n",
      "def chat(state, *, store: BaseStore):\n",
      "    # Search based on user's last message\n",
      "    items = store.search(\n",
      "        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n",
      "    )\n",
      "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
      "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
      "    response = model.invoke(\n",
      "        [\n",
      "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n",
      "            *state[\"messages\"],\n",
      "        ]\n",
      "    )\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "\n",
      "builder = StateGraph(MessagesState)\n",
      "builder.add_node(chat)\n",
      "builder.add_edge(START, \"chat\")\n",
      "graph = builder.compile(store=store)\n",
      "\n",
      "for message, metadata in graph.stream(\n",
      "    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n",
      "    stream_mode=\"messages\",\n",
      "):\n",
      "    print(message.content, end=\"\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Create an agent\n",
      "content===> # pip install -qU \"langchain[anthropic]\" to call the model\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_weather],\n",
      "    system_prompt=\"You are a helpful assistant\",\n",
      ")\n",
      "\n",
      "# Run the agent\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Structured output\n",
      "content===> def create_agent(\n",
      "    ...\n",
      "    response_format: Union[\n",
      "        ToolStrategy[StructuredResponseT],\n",
      "        ProviderStrategy[StructuredResponseT],\n",
      "        type[StructuredResponseT],\n",
      "    ]\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Code quality standards\n",
      "content===> def process_documents(\n",
      "    docs: list[Document],\n",
      "    processor: DocumentProcessor,\n",
      "    *,\n",
      "    batch_size: int = 100\n",
      ") -> ProcessingResult:\n",
      "    \"\"\"Process documents in batches.\n",
      "\n",
      "    Args:\n",
      "        docs: List of documents to process.\n",
      "        processor: Document processing instance.\n",
      "        batch_size: Number of documents per batch.\n",
      "\n",
      "    Returns:\n",
      "        Processing results with success/failure counts.\n",
      "    \"\"\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Install LangGraph\n",
      "content===> pip install -U langchain\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/install\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Document AI\n",
      "content===> from langchain_core.document_loaders.blob_loaders import Blob\n",
      "from langchain_google_community import DocAIParser\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Model fallback\n",
      "content===> from langchain.agents import create_agent\n",
      "from langchain.agents.middleware import ModelFallbackMiddleware\n",
      "\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"gpt-4o\",  # Primary model\n",
      "    tools=[...],\n",
      "    middleware=[\n",
      "        ModelFallbackMiddleware(\n",
      "            \"gpt-4o-mini\",  # Try first on error\n",
      "            \"claude-3-5-sonnet-20241022\",  # Then this\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Parallelization\n",
      "content===> # Graph state\n",
      "class State(TypedDict):\n",
      "    topic: str\n",
      "    joke: str\n",
      "    story: str\n",
      "    poem: str\n",
      "    combined_output: str\n",
      "\n",
      "\n",
      "# Nodes\n",
      "def call_llm_1(state: State):\n",
      "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
      "\n",
      "    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
      "    return {\"joke\": msg.content}\n",
      "\n",
      "\n",
      "def call_llm_2(state: State):\n",
      "    \"\"\"Second LLM call to generate story\"\"\"\n",
      "\n",
      "    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n",
      "    return {\"story\": msg.content}\n",
      "\n",
      "\n",
      "def call_llm_3(state: State):\n",
      "    \"\"\"Third LLM call to generate poem\"\"\"\n",
      "\n",
      "    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n",
      "    return {\"poem\": msg.content}\n",
      "\n",
      "\n",
      "def aggregator(state: State):\n",
      "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
      "\n",
      "    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n",
      "    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n",
      "    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n",
      "    combined += f\"POEM:\\n{state['poem']}\"\n",
      "    return {\"combined_output\": combined}\n",
      "\n",
      "\n",
      "# Build workflow\n",
      "parallel_builder = StateGraph(State)\n",
      "\n",
      "# Add nodes\n",
      "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
      "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
      "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
      "parallel_builder.add_node(\"aggregator\", aggregator)\n",
      "\n",
      "# Add edges to connect nodes\n",
      "parallel_builder.add_edge(START, \"call_llm_1\")\n",
      "parallel_builder.add_edge(START, \"call_llm_2\")\n",
      "parallel_builder.add_edge(START, \"call_llm_3\")\n",
      "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
      "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
      "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
      "parallel_builder.add_edge(\"aggregator\", END)\n",
      "parallel_workflow = parallel_builder.compile()\n",
      "\n",
      "# Show workflow\n",
      "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n",
      "\n",
      "# Invoke\n",
      "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
      "print(state[\"combined_output\"])\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Memory (Store)\n",
      "content===> from typing import Any\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "from langchain.agents import create_agent\n",
      "from langchain.tools import tool, ToolRuntime\n",
      "\n",
      "\n",
      "# Access memory\n",
      "@tool\n",
      "def get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n",
      "    \"\"\"Look up user info.\"\"\"\n",
      "    store = runtime.store\n",
      "    user_info = store.get((\"users\",), user_id)\n",
      "    return str(user_info.value) if user_info else \"Unknown user\"\n",
      "\n",
      "# Update memory\n",
      "@tool\n",
      "def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n",
      "    \"\"\"Save user info.\"\"\"\n",
      "    store = runtime.store\n",
      "    store.put((\"users\",), user_id, user_info)\n",
      "    return \"Successfully saved user info.\"\n",
      "\n",
      "store = InMemoryStore()\n",
      "agent = create_agent(\n",
      "    model,\n",
      "    tools=[get_user_info, save_user_info],\n",
      "    store=store\n",
      ")\n",
      "\n",
      "# First session: save user info\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n",
      "})\n",
      "\n",
      "# Second session: get user info\n",
      "agent.invoke({\n",
      "    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n",
      "})\n",
      "# Here is the user info for user with ID \"abc123\":\n",
      "# - Name: Foo\n",
      "# - Age: 25\n",
      "# - Email: foo@langchain.dev\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"MISTRALAI_API_KEY\"):\n",
      "  os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n",
      "\n",
      "from langchain_mistralai import MistralAIEmbeddings\n",
      "\n",
      "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Google Cloud\n",
      "content===> gcloud auth application-default login\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Stream custom data\n",
      "content===> [\"updates\", \"custom\"]\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Build a real-world agent\n",
      "content===> from dataclasses import dataclass\n",
      "\n",
      "# We use a dataclass here, but Pydantic models are also supported.\n",
      "@dataclass\n",
      "class ResponseFormat:\n",
      "    \"\"\"Response schema for the agent.\"\"\"\n",
      "    # A punny response (always required)\n",
      "    punny_response: str\n",
      "    # Any interesting information about the weather if available\n",
      "    weather_conditions: str | None = None\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Base URL or proxy\n",
      "content===> from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(\n",
      "    model=\"gpt-4o\",\n",
      "    openai_proxy=\"http://proxy.example.com:8080\"\n",
      ")\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> External index\n",
      "content===> TavilySearchAPIRetriever\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Running tests locally\n",
      "content===> make format\n",
      "make lint\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Protocol reference\n",
      "content===> write(file_path: str, content: str) -> WriteResult\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Defining formats\n",
      "content===> from pydantic import BaseModel, Field\n",
      "\n",
      "class CustomerSupportTicket(BaseModel):\n",
      "    \"\"\"Structured ticket information extracted from customer message.\"\"\"\n",
      "\n",
      "    category: str = Field(\n",
      "        description=\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n",
      "    )\n",
      "    priority: str = Field(\n",
      "        description=\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n",
      "    )\n",
      "    summary: str = Field(\n",
      "        description=\"One-sentence summary of the customer's issue\"\n",
      "    )\n",
      "    customer_sentiment: str = Field(\n",
      "        description=\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n",
      "    )\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Vertex AI image captioning\n",
      "content===> from langchain_google_vertexai.vision_models import VertexAIImageCaptioningChat\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Tool calling\n",
      "content===> model.bind_tools([get_weather], parallel_tool_calls=False)\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> code\n",
      "title===> Top integrations\n",
      "content===> pip install -qU \"langchain[langchain-xai]\"\n",
      "\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      " code 의 갯수: 1072\n"
     ]
    }
   ],
   "source": [
    "# 코드 블록만 출력\n",
    "count = 0\n",
    "for i in unique_data:\n",
    "    if i['type'] == 'code':\n",
    "        print(f\"type===> {i['type']}\")\n",
    "        print(f\"title===> {i['title']}\")\n",
    "        print(f\"content===> {i['content']}\")\n",
    "        print(f\"side_link===> {i['side_link']}\")\n",
    "        print(f\"head_menu_name===> {i['head_menu_name']}\")\n",
    "        print(f\"side_menu_name===> {i['side_menu_name']}\")\n",
    "        count += 1\n",
    "        print(\"----------------------------------\")\n",
    "#print(f\"총 코드 블록 개수: {count}\")\n",
    "print(f\" code 의 갯수: {count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04decdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type===> text\n",
      "title===> File structure\n",
      "content===> Below are examples of directory structures for applications:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom stores\n",
      "content===> You can also implement your own custom store by extending the BaseStore class. See the store interface documentation for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory Store\n",
      "content===> A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\n",
      "\n",
      "But, what if we want to retain some information across threads ? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!\n",
      "\n",
      "With checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic runtime context\n",
      "content===> Dynamic runtime context represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.\n",
      "\n",
      "Example shows how to incorporate state into an agent prompt .\n",
      "\n",
      "State can also be accessed by the agent’s tools , which can read or update the state as needed. See tool calling guide for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> System prompt\n",
      "content===> Deep agents come with a built-in system prompt inspired by Claude Code’s system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.\n",
      "\n",
      "Each deep agent tailored to a use case should include a custom system prompt specific to that use case.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining formats\n",
      "content===> Schema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Choosing a pattern\n",
      "content===> You can mix both patterns — use handoffs for agent switching, and have each agent call subagents as tools for specialized tasks.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard content\n",
      "content===> In v1, messages gain provider-agnostic standard content blocks. Access them via @[ message.content_blocks ][content_blocks] for a consistent, typed view across providers. The existing message.content field remains unchanged for strings or provider-native structures.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. View your agent in Studio\n",
      "content===> Studio makes each step of your agent easily observable. Replay any input and inspect the exact prompt, tool arguments, return values, and token/latency metrics. If a tool throws an exception, Studio records it with surrounding state so you can spend less time debugging.\n",
      "\n",
      "Keep your dev server running, edit prompts or tool signatures, and watch Studio hot-reload. Re-run the conversation thread from any step to verify behavior changes. See Manage threads for more details.\n",
      "\n",
      "As your agent grows, the same view scales from a single-tool demo to multi-node graphs, keeping decisions legible and reproducible.\n",
      "\n",
      "For an in-depth look at Studio, check out the overview page .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Persistence\n",
      "content===> LangGraph API handles checkpointing automatically When using the LangGraph API, you don’t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Store is required\n",
      "content===> You must provide a Store when enabling long-term memory:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Minimize tool sets\n",
      "content===> Only give subagents the tools they need. This improves focus and security:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> View subgraph state\n",
      "content===> When you enable persistence , you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.\n",
      "\n",
      "You can inspect the graph state via graph.get_state(config) . To view the subgraph state, you can use graph.get_state(config, subgraphs=True) .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PowerBI individual tools\n",
      "content===> You can use individual tools from the Azure PowerBI Toolkit:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Large tool result eviction\n",
      "content===> The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.\n",
      "\n",
      "How it works:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ProviderStrategy\n",
      "content===> To learn about structured output, see Structured output .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context editing\n",
      "content===> Manage conversation context by trimming, summarizing, or clearing tool uses.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Spanner\n",
      "content===> Vector store using Cloud Spanner .\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Trends\n",
      "content===> Query Google Trends data. Requires google-search-results package and SerpApi key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Backends\n",
      "content===> This page explains how to choose a backend , route different paths to different backends , implement your own virtual filesystem (e.g., S3 or Postgres), add policy hooks , and comply with the backend protocol .\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 5. Install dependencies\n",
      "content===> In the root of your new LangGraph app, install the dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memorystore for Redis\n",
      "content===> Vector store using Memorystore for Redis .\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Generative AI (Gemini API & AI Studio)\n",
      "content===> Start for free and get your API key from Google AI Studio .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context editing\n",
      "content===> Token counting method. Options: \"approximate\" or \"model\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Handle errors appropriately\n",
      "content===> Add a retry policy to automatically retry network issues and rate limits:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agents\n",
      "content===> An LLM Agent runs tools in a loop to achieve a goal .\n",
      "An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n",
      "\n",
      "create_agent builds a graph -based agent runtime using LangGraph . A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Message\n",
      "content===> See the RAG tutorial for an end-to-end example of building retrieval agents with LangChain.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> You will need to install uv if you haven’t previously.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> High-level API\n",
      "content===> The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure OpenAI\n",
      "content===> Microsoft Azure , often referred to as Azure is a cloud computing platform run by Microsoft , which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI\n",
      "content===> Access chat models like Gemini via the Vertex AI platform.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prompt caching\n",
      "content===> Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ToolRuntime\n",
      "content===> Use ToolRuntime to access all runtime information in a single parameter. Simply add runtime: ToolRuntime to your tool signature, and it will be automatically injected without being exposed to the LLM.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ScaNN (Local Index)\n",
      "content===> Google ScaNN (Scalable Nearest Neighbors) is a python package.\n",
      "\n",
      "ScaNN is a method for efficient vector similarity search at scale.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangSmith Integration\n",
      "content===> Results will be automatically logged to LangSmith.\n",
      "\n",
      "Alternatively, you can create a dataset in LangSmith and use the evaluate function:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messages\n",
      "content===> Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n",
      "\n",
      "Messages are objects that contain:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Environment variables\n",
      "content===> For a production deployment, you will typically want to configure the environment variables in the deployment environment.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hugging Face model loader\n",
      "content===> Load model information from Hugging Face Hub , including README content.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static model\n",
      "content===> For more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI . See Chat models for other available chat model classes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Integration Platforms\n",
      "content===> The following platforms provide access to multiple tools and services through a unified interface:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SearchApi\n",
      "content===> See usage examples and authorization instructions .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Additional resources\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context engineering\n",
      "content===> Learn methods for providing AI applications the right information and tools to accomplish a task.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Co-locate Python and JavaScript/TypeScript content\n",
      "content===> We don’t want a lack of parity to block contributions. If a feature is only available in one language, it’s okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.\n",
      "\n",
      "If you need help translating content between Python and JavaScript/TypeScript, please ask in the community slack or tag a maintainer in your PR.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> LangChain’s streaming system lets you surface live feedback from agent runs to your application.\n",
      "\n",
      "What’s possible with LangChain streaming:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Setup\n",
      "content===> Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> When long-term memory is enabled, deep agents maintain two separate filesystems :\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom tool message content\n",
      "content===> Without tool_message_content , our final ToolMessage would be:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI image captioning\n",
      "content===> Implementation of the Image Captioning model as an LLM. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Customizing agent context\n",
      "content===> At the heart of multi-agent design is context engineering - deciding what information each agent sees. LangChain gives you fine-grained control over:\n",
      "\n",
      "The quality of your system heavily depends on context engineering. The goal is to ensure that each agent has access to the correct data it needs to perform its task, whether it’s acting as a tool or as an active agent.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Building a knowledge base\n",
      "content===> A knowledge base is a repository of documents or structured data used during retrieval.\n",
      "\n",
      "If you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.\n",
      "\n",
      "If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\n",
      "\n",
      "See the following tutorial to build a searchable knowledge base and minimal RAG workflow:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Example: Summarization\n",
      "content===> One of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context , summarization persistently updates state - permanently replacing old messages with a summary that’s saved for all future turns.\n",
      "\n",
      "LangChain offers built-in middleware for this:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trim messages\n",
      "content===> Most LLMs have a maximum supported context window (denominated in tokens).\n",
      "\n",
      "One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens ) to use for handling the boundary.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft PowerPoint\n",
      "content===> Microsoft PowerPoint is a presentation program by Microsoft.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Planning\n",
      "content===> Add todo list management capabilities for complex multi-step tasks.\n",
      "\n",
      "This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> An identifier associated with the tool call.\n",
      "\n",
      "The name of the tool to be called.\n",
      "\n",
      "Partial tool arguments (may be incomplete JSON)\n",
      "\n",
      "Purpose: Streaming server-side tool call fragments\n",
      "\n",
      "Always \"server_tool_call_chunk\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invocation\n",
      "content===> For streaming steps and / or tokens from the agent, refer to the streaming guide.\n",
      "\n",
      "Otherwise, the agent follows the LangGraph Graph API and supports all associated methods.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interface\n",
      "content===> The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupts in tools\n",
      "content===> This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Step 2: Identify what each step needs to do\n",
      "content===> For each node in your graph, determine what type of operation it represents and what context it needs to work properly.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Channels\n",
      "content===> Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SearchApi\n",
      "content===> SearchApi provides API access to Google search, YouTube, etc. Requires langchain-community .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ToolStrategy\n",
      "content===> ToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trim messages\n",
      "content===> Remove first or last N messages (before calling LLM)\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory (Store)\n",
      "content===> Access persistent data across conversations using the store. The store is accessed via runtime.store and allows you to save and retrieve user-specific or application-specific data.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Ollama\n",
      "content===> For a complete list of supported models and variants, see the Ollama model library .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/ollama\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Ollama\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Anthropic\n",
      "content===> Claude models for advanced reasoning and conversation.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> The general-purpose subagent\n",
      "content===> In addition to any user-defined subagents, deep agents have access to a general-purpose subagent at all times. This subagent:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent middleware\n",
      "content===> A subagent is defined with a name , description , system prompt , and tools . You can also provide a subagent with a custom model , or with additional middleware . This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.\n",
      "\n",
      "For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hugging Face dataset\n",
      "content===> Hugging Face Hub is home to over 75,000 datasets in more than 100 languages\n",
      "that can be used for a broad range of tasks across NLP, Computer Vision, and Audio.\n",
      "They used for a diverse range of tasks such as translation, automatic speech\n",
      "recognition, and image classification.\n",
      "\n",
      "We need to install datasets python package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Modify\n",
      "content===> Transform prompts, tool selection, and output formatting\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Read short-term memory in a tool\n",
      "content===> The tool_runtime parameter is hidden from the tool signature (so the model doesn’t see it), but the tool can access the state through it.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> langchain-classic\n",
      "content===> Legacy functionality has moved to langchain-classic to keep the core packages lean and focused.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Page structure\n",
      "content===> Every documentation page must begin with YAML frontmatter:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangChain Academy\n",
      "content===> Courses and exercises to level up your LangChain skills.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document AI Warehouse\n",
      "content===> Requires installation of relevant Document AI packages (check specific docs).\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Connect to your agent\n",
      "content===> Agent Chat UI can connect to both local and deployed agents .\n",
      "\n",
      "After starting Agent Chat UI, you’ll need to configure it to connect to your agent:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> The reasoning content\n",
      "\n",
      "Additional provider-specific data\n",
      "\n",
      "Example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Retrievers\n",
      "content===> A retriever is an interface that returns documents given an unstructured query.\n",
      "It is more general than a vector store.\n",
      "A retriever does not need to be able to store documents, only to return (or retrieve) them.\n",
      "Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra .\n",
      "\n",
      "Retrievers accept a string query as input and return a list of Documents as output.\n",
      "\n",
      "Note that all vector stores can be cast to retrievers. Refer to the vector store integration docs for available vector stores.\n",
      "This page lists custom retrievers, implemented via subclassing BaseRetriever.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> The text content\n",
      "\n",
      "List of annotations for the text\n",
      "\n",
      "Additional provider-specific data\n",
      "\n",
      "Example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Rate limiting\n",
      "content===> The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Database\n",
      "content===> The following table shows tools that can be used to automate tasks in databases:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How to make your first Pull Request\n",
      "content===> If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.\n",
      "\n",
      "If you are looking for something to work on, check out the issues labeled “good first issue” or “help wanted” in our repos:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Why do agents fail?\n",
      "content===> When agents fail, it’s usually because the LLM call inside the agent took the wrong action / didn’t do what we expected. LLMs fail for one of two reasons:\n",
      "\n",
      "More often than not - it’s actually the second reason that causes agents to not be reliable.\n",
      "\n",
      "Context engineering is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of “right” context is the number one blocker for more reliable agents, and LangChain’s agent abstractions are uniquely designed to facilitate context engineering.\n",
      "\n",
      "New to context engineering? Start with the conceptual overview to understand the different types of context and when to use them.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> See the astream_events() reference for event types and other details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Local development\n",
      "content===> For customization or local development, you can run Agent Chat UI locally:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> Static string or callable function for custom description\n",
      "\n",
      "Important: Human-in-the-loop middleware requires a checkpointer to maintain state across interruptions.\n",
      "\n",
      "See the human-in-the-loop documentation for complete examples and integration patterns.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Monitor\n",
      "content===> Track agent behavior with logging, analytics, and debugging\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Case Studies\n",
      "content===> See how teams are using LangChain and LangGraph in production.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document AI\n",
      "content===> Google Cloud Document AI is a Google Cloud\n",
      "service that transforms unstructured data from documents into structured data, making it easier\n",
      "to understand, analyze, and consume.\n",
      "\n",
      "We need to set up a GCS bucket and create your own OCR processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs:// )\n",
      "and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID .\n",
      "We can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangSmith Integration\n",
      "content===> Results will be automatically logged to LangSmith.\n",
      "\n",
      "To learn more about evaluating your agent, see the LangSmith docs .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft OneDrive\n",
      "content===> Microsoft OneDrive (formerly SkyDrive ) is a file-hosting service operated by Microsoft.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deep Agents overview\n",
      "content===> deepagents is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Log to a project\n",
      "content===> You can set a custom project name for your entire application by setting the LANGSMITH_PROJECT environment variable:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configurable models\n",
      "content===> We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Short-term vs. long-term filesystem\n",
      "content===> By default, these tools write to a local “filesystem” in your graph state. If you provide a Store object to your agent runtime, you can also enable saving to long-term memory, which persists across different threads of your agent.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Evaluator-optimizer\n",
      "content===> In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\n",
      "\n",
      "Evaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Anthropic prompt caching\n",
      "content===> Time to live for cached content. Valid values: \"5m\" or \"1h\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invoke\n",
      "content===> A list of messages can be provided to a model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation. See the messages guide for more detail on roles, types, and content.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Writes\n",
      "content===> See Tools for comprehensive examples of accessing state, store, and runtime context in tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> The tools argument to create_agent accepts a list of:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Determinism and Consistent Replay\n",
      "content===> When you resume a workflow run, the code does NOT resume from the same line of code where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped.\n",
      "\n",
      "As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes .\n",
      "\n",
      "To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:\n",
      "\n",
      "For some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows\n",
      "how to structure your code using tasks to avoid these issues. The same principles apply to the StateGraph (Graph API) .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> create_react_agent→create_agent\n",
      "content===> LangGraph v1 deprecates the create_react_agent prebuilt. Use LangChain’s create_agent , which runs on LangGraph and adds a flexible middleware system.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Persistent context\n",
      "content===> What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. Build and compile the agent\n",
      "content===> To learn how to trace your agent with LangSmith, see the LangSmith documentation .\n",
      "\n",
      "Congratulations! You’ve built your first agent using the LangGraph Graph API.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 5. Test the API\n",
      "content===> LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the Platform setup overview .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/deploy\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Running tests locally\n",
      "content===> All type hints must be valid\n",
      "\n",
      "Push your branch and open a pull request. Follow the provided form template. Note related issues using a closing keyword . After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat models\n",
      "content===> The image_url can be a public URL, a GCS URI ( gs://... ), a local file path, a base64 encoded image string ( data:image/png;base64,... ), or a PIL Image object.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invocation config\n",
      "content===> These configuration values are particularly useful when:\n",
      "\n",
      "Identifies this specific invocation in logs and traces. Not inherited by sub-calls.\n",
      "\n",
      "Labels inherited by all sub-calls for filtering and organization in debugging tools.\n",
      "\n",
      "Custom key-value pairs for tracking additional context, inherited by all sub-calls.\n",
      "\n",
      "Controls the maximum number of parallel calls when using batch() or batch_as_completed() .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Textract\n",
      "content===> Amazon Textract is a machine\n",
      "learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Implementation (Coming soon)\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> No automatic cleanup\n",
      "content===> Long-term files persist indefinitely. There’s no built-in TTL or automatic cleanup. You’ll need to implement cleanup strategies if needed.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> See the human-in-the-loop documentation for complete details on implementing approval workflows.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tutorials (Learn)\n",
      "content===> Lessons that guide users through practical activities to build understanding\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory\n",
      "content===> Second, checkpointers allow for “memory” between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Integration packages\n",
      "content===> LangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deploy\n",
      "content===> LangSmith is the fastest way to turn agents into production systems. Traditional hosting platforms are built for stateless, short-lived web apps, while LangGraph is purpose-built for stateful, long-running agents , so you can go from repo to reliable cloud deployment in minutes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/deploy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Token usage\n",
      "content===> Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.\n",
      "\n",
      "You can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Keep state raw, format prompts on-demand\n",
      "content===> A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.\n",
      "\n",
      "This separation means:\n",
      "\n",
      "Let’s define our state:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reference\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Setup\n",
      "content===> Next, we need to set API keys for Anthropic (the LLM we will use)\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Threads\n",
      "content===> A thread’s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Co-marketing\n",
      "content===> With over 60 million monthly downloads, LangChain has a large audience of developers building LLM applications. Beyond just listing integrations, we aim to highlight high-quality, educational examples that inspire developers and advance the ecosystem.\n",
      "\n",
      "While we occasionally share integrations, we prioritize content that provides\n",
      "meaningful insights and best practices. Our main social channels are Twitter and LinkedIn , where we highlight the best examples.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/comarketing\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Co-marketing\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Manage short-term memory\n",
      "content===> Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today’s LLMs. A full history may not fit inside an LLM’s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.\n",
      "\n",
      "Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.\n",
      "\n",
      "For more information on common techniques for managing messages, see the Add and manage memory guide.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Delete messages\n",
      "content===> When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Planning\n",
      "content===> Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n",
      "\n",
      "Custom description for the write_todos tool. Uses built-in description if not specified.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Built on LangGraph\n",
      "content===> Because create_agent is built on LangGraph , you automatically get built in support for long running and reliable agents via:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Observability\n",
      "content===> Add observability with LangSmith for debugging and monitoring\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom state\n",
      "content===> Defining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Streaming and chunks\n",
      "content===> During streaming, you’ll receive AIMessageChunk objects that can be combined into a full message object:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Try out your agent\n",
      "content===> Let’s run our agent with an urgent billing issue that needs human review:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calls\n",
      "content===> Other structured data, such as reasoning or citations, can also appear in message content .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream\n",
      "content===> The resulting message can be treated the same as a message that was generated with invoke() - for example, it can be aggregated into a message history and passed back to the model as conversational context.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ChatGPTPluginRetriever\n",
      "content===> Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> OpenAI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud SQL for MySQL\n",
      "content===> Google Cloud SQL for MySQL is a fully-managed MySQL database service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Handle interrupts\n",
      "content===> When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> Integration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls.\n",
      "\n",
      "You can use vcrpy to achieve this. If you’re using pytest , the pytest-recording plugin provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI Search\n",
      "content===> Install the google-cloud-discoveryengine package for underlying access.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Zotero\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory storage\n",
      "content===> LangGraph stores long-term memories as JSON documents in a store .\n",
      "\n",
      "Each memory is organized under a custom namespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Start with the process you want to automate\n",
      "content===> Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:\n",
      "\n",
      "The agent should:\n",
      "\n",
      "Example scenarios to handle:\n",
      "\n",
      "To implement an agent in LangGraph, you will usually follow the same five steps.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> VertexStringEvaluator\n",
      "content===> Evaluate a single prediction string using Vertex AI models.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state viastate_schema\n",
      "content===> Use the state_schema parameter when your custom state needs to be accessed by tools:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangChain overview\n",
      "content===> LangChain v1.0 is now available!\n",
      "\n",
      "For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\n",
      "\n",
      "If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\n",
      "\n",
      "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
      "\n",
      "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Mocking Chat Model\n",
      "content===> If we invoke the model again, it will return the next item in the iterator:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> After agent guardrails\n",
      "content===> Use “after agent” hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuring interrupts\n",
      "content===> To use HITL, add the middleware to the agent’s middleware list when creating the agent.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream graph state\n",
      "content===> Use the stream modes updates and values to stream the state of the graph as it executes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Middleware\n",
      "content===> Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n",
      "\n",
      "Middleware integrates seamlessly into the agent’s execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n",
      "\n",
      "For comprehensive middleware documentation including decorators like @before_model , @after_model , and @wrap_tool_call , see Middleware .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> YouTube Audio Loader\n",
      "content===> Download audio from YouTube videos. Requires yt_dlp , pydub , librosa .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context overview\n",
      "content===> Context engineering is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n",
      "\n",
      "Runtime context refers to local context: data and dependencies your code needs to run. It does not refer to:\n",
      "\n",
      "Runtime context can be used to optimize the LLM context. For example, you can use user metadata\n",
      "in the runtime context to fetch user preferences and feed them into the context window.\n",
      "\n",
      "LangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ProviderStrategy\n",
      "content===> As of langchain 1.0 , simply passing a schema (e.g., response_format=ContactInfo ) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> MCP Toolbox\n",
      "content===> MCP Toolbox provides a simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB . With MCP Toolbox, you can seamlessly integrate your database with LangChain to build powerful, data-driven applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Short-term vs. long-term filesystem\n",
      "content===> If you enable use_longterm_memory=True and provide a Store in your agent runtime, then any files prefixed with /memories/ are saved to the long-term memory store. Note that any agents deployed on LangGraph Platform are automatically provided with a long-term memory store.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> create_agent\n",
      "content===> The new standard for building agents in LangChain, replacing langgraph.prebuilt.create_react_agent .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Learn\n",
      "content===> In the Learn section of the documentation, you’ll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hybrid RAG\n",
      "content===> Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n",
      "\n",
      "Typical components include:\n",
      "\n",
      "The architecture often supports multiple iterations between these steps:\n",
      "\n",
      "This architecture is suitable for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Document Intelligence\n",
      "content===> Azure AI Document Intelligence (formerly known\n",
      "as Azure Form Recognizer ) is machine-learning\n",
      "based service that extracts texts (including handwriting), tables, document structures,\n",
      "and key-value-pairs\n",
      "from digital or scanned PDFs, images, Office and HTML files.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tailor configurations by risk\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multimodal\n",
      "content===> See the integrations page for details on specific providers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Install LangGraph\n",
      "content===> To work with specific LLM provider packages, you will need install them separately.\n",
      "\n",
      "Refer to the integrations page for provider-specific installation instructions.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/install\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> If we create a new thread, we can still access the same memories so long as the user_id is the same.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud SQL for PostgreSQL\n",
      "content===> Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tool emulator\n",
      "content===> Model to use for generating emulated tool responses. Can be a model identifier string or BaseChatModel instance.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Edit tool arguments\n",
      "content===> When \"edit\" is in the allowed decisions, you can modify the tool arguments before execution:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use a virtual filesystem\n",
      "content===> Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.\n",
      "\n",
      "Design guidelines:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Firestore (Datastore Mode)\n",
      "content===> Google Cloud Firestore in Datastore mode .\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Install\n",
      "content===> Install the langchain-mcp-adapters library to use MCP tools in LangGraph:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2. Prepare your agent\n",
      "content===> We’ll use the following simple agent as an example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpointer libraries\n",
      "content===> Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory\n",
      "content===> AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dependencies\n",
      "content===> A LangGraph application may depend on other Python packages.\n",
      "\n",
      "You will generally need to specify the following information for dependencies to be set up correctly:\n",
      "\n",
      "A file in the directory that specifies the dependencies (e.g. requirements.txt , pyproject.toml , or package.json ).\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Time Travel\n",
      "content===> Third, checkpointers allow for “time travel” , allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Firestore (Native Mode)\n",
      "content===> Google Cloud Firestore is a NoSQL document database.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> This overview covers text-based embedding models . LangChain does not currently support multimodal embeddings.\n",
      "\n",
      "Embedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its semantic meaning . These vectors allow machines to compare and search text based on meaning rather than exact words.\n",
      "\n",
      "In practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase “machine learning” , embeddings can surface documents that discuss related concepts even when different wording is used.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model Context\n",
      "content===> Control what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Short-term memory\n",
      "content===> Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n",
      "\n",
      "LangGraph manages short-term memory as part of the agent’s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph’s state, the bot can access the full context for a given conversation while maintaining separation between different threads.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Middleware\n",
      "content===> Middleware provides a way to more tightly control what happens inside the agent.\n",
      "\n",
      "The core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n",
      "\n",
      "Middleware exposes hooks before and after each of those steps:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream\n",
      "content===> Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn’t streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n",
      "\n",
      "LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\n",
      "\n",
      "In LangGraph agents , for example, you can call model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configurable models\n",
      "content===> We can call declarative operations like bind_tools , with_structured_output , with_configurable , etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> List of allowed decisions: \"approve\" , \"edit\" , or \"reject\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SageMaker Tracking\n",
      "content===> Amazon SageMaker is a fully managed service that is used to quickly\n",
      "and easily build, train and deploy machine learning (ML) models.\n",
      "\n",
      "Amazon SageMaker Experiments is a capability\n",
      "of Amazon SageMaker that lets you organize, track,\n",
      "compare and evaluate ML experiments and model versions.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Additional resources\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> The strict mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Run a local server\n",
      "content===> This guide shows you how to run a LangGraph application locally.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic model\n",
      "content===> For model configuration details, see Models . For dynamic model selection patterns, see Dynamic model in middleware .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFaceInstructEmbeddings\n",
      "content===> We can use the HuggingFaceInstructEmbeddings class to run open source embedding models locally.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configurable models\n",
      "content===> You can also create a runtime-configurable model by specifying configurable_fields . If you don’t specify a model value, then 'model' and 'model_provider' will be configurable by default.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Blob Storage\n",
      "content===> See usage examples for the Azure Blob Storage Loader .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invoke a graph from a node\n",
      "content===> This is an example with two levels of subgraphs: parent -> child -> grandchild.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Data Steps\n",
      "content===> Use when you need to retrieve information from external sources\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multimodal\n",
      "content===> OpenAI and AWS Bedrock Converse ,\n",
      "for example, require a filename for PDFs. See the provider page for your chosen model for specifics.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Get state history\n",
      "content===> You can get the full history of the graph execution for a given thread by calling graph.get_state_history(config) . This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic tool definition\n",
      "content===> The simplest way to create a tool is with the @tool decorator. By default, the function’s docstring becomes the tool’s description that helps the model understand when to use it:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> as_node\n",
      "content===> The final thing you can optionally specify when calling update_state is as_node . If you provided it, the update will be applied as if it came from node as_node . If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> An identifier associated with the tool call.\n",
      "\n",
      "Name of the tool being called\n",
      "\n",
      "Partial tool arguments (may be incomplete JSON)\n",
      "\n",
      "Position of this chunk in the stream\n",
      "\n",
      "Purpose: Search results\n",
      "\n",
      "Always \"server_tool_result\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Update the state\n",
      "content===> update_state will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SQL Agent\n",
      "content===> Build a SQL agent to interact with databases with human-in-the-loop review.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Next steps\n",
      "content===> Now that you’ve built your first deep agent:\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> Next, build a practical weather forecasting agent that demonstrates key production concepts:\n",
      "\n",
      "Let’s walk through each step:\n",
      "\n",
      "The system prompt defines your agent’s role and behavior. Keep it specific and actionable:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Example: Summarization\n",
      "content===> When the conversation exceeds the token limit, SummarizationMiddleware automatically:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> It is distinct from batch APIs supported by inference providers, such as OpenAI or Anthropic .\n",
      "\n",
      "By default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed() :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft Excel\n",
      "content===> Microsoft Excel is a spreadsheet editor developed by\n",
      "Microsoft for Windows, macOS, Android, iOS and iPadOS.\n",
      "It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming\n",
      "language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.\n",
      "\n",
      "The UnstructuredExcelLoader is used to load Microsoft Excel files. The loader works with both .xlsx and .xls files.\n",
      "The page content will be the raw text of the Excel file. If you use the loader in \"elements\" mode, an HTML\n",
      "representation of the Excel file will be available in the document metadata under the text_as_html key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deterministic guardrails\n",
      "content===> Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invocation config\n",
      "content===> Handlers for monitoring and responding to events during execution.\n",
      "\n",
      "Maximum recursion depth for chains to prevent infinite loops in complex pipelines.\n",
      "\n",
      "See full RunnableConfig reference for all supported attributes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multi-agent\n",
      "content===> Multi-agent systems break a complex application into multiple specialized agents that work together to solve problems.\n",
      "Instead of relying on a single agent to handle every step, multi-agent architectures allow you to compose smaller, focused agents into a coordinated workflow.\n",
      "\n",
      "Multi-agent systems are useful when:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tokens\n",
      "content===> To stream tokens as they are produced by the LLM, use stream_mode=\"messages\" . Below you can see the output of the agent streaming tool calls and the final response.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> As we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> History\n",
      "content===> A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/philosophy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Philosophy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Places\n",
      "content===> Search for places information. Requires googlemaps package and a Google Maps API key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Episodic memory\n",
      "content===> Episodic memory , in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.\n",
      "\n",
      "In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it’s easier to “show” than “tell” and LLMs learn well from examples. Few-shot learning lets you “program” your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.\n",
      "\n",
      "Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity ( using a BM25-like algorithm for keyword based similarity).\n",
      "\n",
      "See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Enforce\n",
      "content===> Apply rate limits, guardrails, and PII detection\n",
      "\n",
      "Add middleware by passing it to create_agent :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reporting issues\n",
      "content===> Please report any issues discovered with 1.0 on GitHub using the 'v1' label .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Datadog\n",
      "content===> Monitoring and analytics platform for applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat models\n",
      "content===> Microsoft offers three main options for accessing chat models through Azure:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Development environment\n",
      "content===> Our Python projects use uv for dependency management. Make sure you have the latest version installed.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud Vision loader\n",
      "content===> Load data using Google Cloud Vision API.\n",
      "\n",
      "Install with Vision dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trim messages\n",
      "content===> To trim message history in an agent, use the @before_model middleware decorator:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining tools\n",
      "content===> If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AWS Glue\n",
      "content===> The AWS Glue Data Catalog is a centralized metadata\n",
      "repository that allows you to manage, access, and share metadata about\n",
      "your data stored in AWS. It acts as a metadata store for your data assets,\n",
      "enabling various AWS services and your applications to query and connect\n",
      "to the data they need efficiently.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use with any LLM\n",
      "content===> You can use stream_mode=\"custom\" to stream data from any LLM API — even if that API does not implement the LangChain chat model interface.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serialize standard content\n",
      "content===> Learn more: Messages , Standard content blocks , and Multimodal .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Updated return type for chat models\n",
      "content===> The return type signature for chat model invocation has been fixed from BaseMessage to AIMessage . Custom chat models implementing bind_tools should update their return signature:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic usage example\n",
      "content===> LangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Repository structure\n",
      "content===> LangChain is organized as a monorepo with multiple packages:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> Follow the PR template provided. If applicable, reference the issue you’re fixing using a closing keyword (e.g. Fixes #123 ).\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick edit: fix a typo\n",
      "content===> For simple changes like fixing typos, you can edit directly on GitHub without setting up a local development environment:\n",
      "\n",
      "Prerequisites:\n",
      "\n",
      "Navigate to any documentation page, scroll to the bottom of the page, and click the link “Edit the source of this page on GitHub”\n",
      "\n",
      "GitHub will prompt you to fork the repository to your account. Make sure to fork into your personal account\n",
      "\n",
      "Correct the typo directly in GitHub’s web editor\n",
      "\n",
      "Click Commit changes... and give your commit a descriptive title like fix(docs): summary of change . If applicable, add an extended description\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Async with Python < 3.11\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> After model\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Profile\n",
      "content===> Memories can be a single, continuously updated “profile” of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you’ve selected to represent your domain.\n",
      "\n",
      "When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Acreom\n",
      "content===> Knowledge management platform with AI-powered organization.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tutorial: Agentic RAG with Self-Correction\n",
      "content===> An example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom SQL Agent\n",
      "content===> Implement a SQL agent directly in LangGraph for maximum flexibility.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon DocumentDB Vector Search\n",
      "content===> Amazon DocumentDB (with MongoDB Compatibility) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\n",
      "With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.\n",
      "Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2. Deploy to LangSmith\n",
      "content===> Log in to LangSmith . In the left sidebar, select Deployments .\n",
      "\n",
      "Click the + New Deployment button. A pane will open where you can fill in the required fields.\n",
      "\n",
      "If you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n",
      "\n",
      "Select your application’s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/deploy\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Philosophy\n",
      "content===> Aim to follow these core principles for all code contributions:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Structured output\n",
      "content===> create_agent has improved structured output generation:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tailor configurations by risk\n",
      "content===> Configure different tools based on their risk level:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> A LangGraph application consists of one or more graphs, a configuration file ( langgraph.json ), a file that specifies dependencies, and an optional .env file that specifies environment variables.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multiple structured outputs error\n",
      "content===> When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a ToolMessage and prompts the model to retry:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Control\n",
      "content===> Add retries, fallbacks, and early termination logic\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pre-model hook\n",
      "content===> Pre-model hooks are now implemented as middleware with the before_model method.\n",
      "This new pattern is more extensible—you can define multiple middlewares to run before the model is called,\n",
      "reusing common patterns across different agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Response Format\n",
      "content===> Schema specification for the model’s final response.\n",
      "\n",
      "All of these types of model context can draw from state (short-term memory), store (long-term memory), or runtime context (static configuration).\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Install LangChain\n",
      "content===> LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/install\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Wrap-style hooks\n",
      "content===> Intercept execution with full control over handler calls. Use for retries, caching, and transformation.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bing Search\n",
      "content===> Follow the documentation here to get a detail explanations and instructions of this tool.\n",
      "\n",
      "The environment variable BING_SUBSCRIPTION_KEY and BING_SEARCH_URL are required from Bing Search resource.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Delete messages\n",
      "content===> When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream subgraph outputs\n",
      "content===> The outputs will be streamed as tuples (namespace, data) , where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\") .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Backwards compatibility\n",
      "content===> Breaking changes to public APIs are not allowed except for critical security fixes.\n",
      "\n",
      "See our versioning policy for details on major version releases.\n",
      "\n",
      "Maintain compatibility:\n",
      "\n",
      "Always preserve :\n",
      "\n",
      "Acceptable modifications :\n",
      "\n",
      "Adding new optional parameters\n",
      "\n",
      "Adding new methods to classes\n",
      "\n",
      "Improving performance without changing behavior\n",
      "\n",
      "Adding new modules or functions\n",
      "\n",
      "Would this break existing user code?\n",
      "\n",
      "Check if your target is public\n",
      "\n",
      "If needed, is it exported in __init__.py ?\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static prompt rename\n",
      "content===> The prompt parameter has been renamed to system_prompt :\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model\n",
      "content===> This functionality has been ported to the middleware interface in v1.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Similarity metrics & indexing\n",
      "content===> Embedding similarity may be computed using:\n",
      "\n",
      "Efficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pre-bound models\n",
      "content===> To better support structured output, create_agent no longer accepts pre-bound models with tools or configuration:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Message\n",
      "content===> The name of the tool that was called.\n",
      "\n",
      "Additional data not sent to the model but can be accessed programmatically.\n",
      "\n",
      "The artifact field stores supplementary data that won’t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model’s context.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Metadata filtering\n",
      "content===> Filtering by metadata (e.g., source, date) can refine search results:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Key Insights\n",
      "content===> Building this email agent has shown us the LangGraph way of thinking:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deprecations\n",
      "content===> The following table lists all items deprecated in LangGraph v1:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> Inside your app’s directory, create a configuration file langgraph.json :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AlloyDB for PostgreSQL\n",
      "content===> Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prompt\n",
      "content===> Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft SharePoint\n",
      "content===> Microsoft SharePoint is a website-based collaboration system\n",
      "that uses workflow applications, “list” databases, and other web parts and security features to\n",
      "empower business teams to work together developed by Microsoft.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Error handling strategies\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Ways to Contribute\n",
      "content===> Found a bug? Please help us fix it by following these steps:\n",
      "\n",
      "Check if the issue already exists in our GitHub Issues for the respective repo:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent middleware\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Collection\n",
      "content===> Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you’re less likely to lose information over time. It’s easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream .\n",
      "\n",
      "However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith ) to help you tune the behavior.\n",
      "\n",
      "Working with document collections also shifts complexity to memory search over the list. The Store currently supports both semantic search and filtering by content .\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Aleph Alpha\n",
      "content===> European AI company’s multilingual language models.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Error handling\n",
      "content===> Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Structured outputs\n",
      "content===> It can be useful to return the raw AIMessage object alongside the parsed representation to access response metadata such as token counts . To do this, set include_raw=True when calling with_structured_output :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Speech-to-Text\n",
      "content===> Google Cloud Speech-to-Text transcribes audio files.\n",
      "\n",
      "Install with Speech-to-Text dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Long-term memory\n",
      "content===> Extend agents with persistent memory across threads using LangGraph’s Store. Agents can save and retrieve information from previous conversations.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Creating workers in LangGraph\n",
      "content===> Orchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Log to a project\n",
      "content===> You can set a custom project name for your entire application by setting the LANGSMITH_PROJECT environment variable:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2. Create a LangGraph app 🌱\n",
      "content===> Additional templates If you use langgraph new without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic prompts\n",
      "content===> Dynamic prompts are a core context engineering pattern— they adapt what you tell the model based on the current conversation state. To do this, use the @dynamic_prompt decorator:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Step 3: Design your state\n",
      "content===> State is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Test writing guidelines\n",
      "content===> In order to write effective tests, there’s a few good practices to follow:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory overview\n",
      "content===> Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n",
      "\n",
      "This conceptual guide covers two types of memory, based on their recall scope:\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool retry\n",
      "content===> Maximum number of retry attempts after the initial call (3 total attempts with default)\n",
      "\n",
      "Optional list of tools or tool names to apply retry logic to. If None , applies to all tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Development environment\n",
      "content===> For changes to community integrations (located in a separate repo ):\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using tasks in nodes\n",
      "content===> If a node contains multiple operations, you may find it easier to convert each operation into a task rather than refactor the operations into individual nodes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. Test your application in Studio\n",
      "content===> Studio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the langgraph dev command:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Kendra\n",
      "content===> With Kendra , we can search across a wide range of content types, including documents, FAQs, knowledge bases,\n",
      "manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and\n",
      "contextual meanings to provide highly relevant search results.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Lens\n",
      "content===> Perform visual searches. Requires google-search-results package and SerpApi key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Match decision order to actions\n",
      "content===> The decisions list must match the order of action_requests :\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> What's new in v1\n",
      "content===> LangChain v1 is a focused, production-ready foundation for building agents. We’ve streamlined the framework around three core improvements:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> When you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests will fail . You should delete the corresponding cassette files and rerun the tests to record fresh interactions.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.\n",
      "\n",
      "Most models supporting tool calling enable parallel tool calls by default. Some (including OpenAI and Anthropic ) allow you to disable this feature. To do this, set parallel_tool_calls=False :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state viastate_schema\n",
      "content===> To learn more about memory, see Memory . For information on implementing long-term memory that persists across sessions, see Long-term memory .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> What belongs in state?\n",
      "content===> Ask yourself these questions about each piece of data:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> langchain-classic\n",
      "content===> If you use any of this functionality, install langchain-classic :\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use time-travel\n",
      "content===> For a conceptual overview of time-travel, see Time travel .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Decision types\n",
      "content===> You can customize which decisions are available for each tool:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Decorator-based middleware\n",
      "content===> For simple middleware that only needs a single hook, decorators provide the quickest way to add functionality:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Integration Testing\n",
      "content===> Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain’s agentevals package provides evaluators specifically designed for testing agent trajectories with live models.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream\n",
      "content===> Invoke the model, but stream the output as it is generated in real-time.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Error handling strategies\n",
      "content===> If handle_errors is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> In a workflow\n",
      "content===> This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Airbyte\n",
      "content===> Data integration platform for ETL and ELT pipelines.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool retry\n",
      "content===> Multiplier for exponential backoff. Each retry waits initial_delay * (backoff_factor ** retry_number) seconds. Set to 0.0 for constant delay.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Getting Started\n",
      "content===> Here is a quick example of how to use MCP Toolbox to connect to your database:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Debugging\n",
      "content===> Use the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static model\n",
      "content===> Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n",
      "\n",
      "To initialize a static model from a model identifier string :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Philosophy\n",
      "content===> LangChain is driven by a few core beliefs:\n",
      "\n",
      "With LangChain, we have two core focuses:\n",
      "\n",
      "Different providers expose different APIs, with different model parameters and different message formats.\n",
      "Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.\n",
      "\n",
      "Models should be used for more than just text generation - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define tools that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/philosophy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Philosophy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory\n",
      "content===> state_schema is still supported for backwards compatibility on create_agent .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Graphs\n",
      "content===> Use the graphs key in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> \"async\"\n",
      "content===> Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there’s a small risk that checkpoints might not be written if the process crashes during execution.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom middleware\n",
      "content===> You can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent’s execution:\n",
      "\n",
      "Build custom middleware by implementing any of these hooks on a subclass of the AgentMiddleware class:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> URL pointing to the file location.\n",
      "\n",
      "Base64-encoded file data.\n",
      "\n",
      "Reference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).\n",
      "\n",
      "File MIME type (e.g., application/pdf )\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph runtime\n",
      "content===> Compiling a StateGraph or creating an @entrypoint produces a Pregel instance that can be invoked with input.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Gemma local from Kaggle\n",
      "content===> Local Gemma model loaded from Kaggle. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AI Message\n",
      "content===> Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new AIMessage object and insert it into the message history as if it came from the model.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Message\n",
      "content===> For example, a retrieval tool could retrieve a passage from a document for reference by a model. Where message content contains text that the model will reference, an artifact can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangSmith Integration\n",
      "content===> For tracking experiments over time, you can log evaluator results to LangSmith , a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.\n",
      "\n",
      "First, set up LangSmith by setting the required environment variables:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Resuming Workflows\n",
      "content===> Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Testing and validation\n",
      "content===> Verify the build completes without errors\n",
      "\n",
      "Check that all internal links work correctly\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpointer interface\n",
      "content===> If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke , .astream , .abatch ), asynchronous versions of the above methods will be used ( .aput , .aput_writes , .aget_tuple , .alist ).\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Debugging with interrupts\n",
      "content===> To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Image captions\n",
      "content===> It uses the Hugging Face models to generate image captions.\n",
      "\n",
      "We need to install several python packages.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Enable tracing\n",
      "content===> All LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> values\n",
      "content===> Let’s assume you have defined the state of your graph with the following schema (see full example above):\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> File structure\n",
      "content===> The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here’s how we might use semantic search in a node to find relevant memories:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model fallback\n",
      "content===> Additional fallback models to try in order if previous models fail\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deep Agents Middleware\n",
      "content===> Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently.\n",
      "\n",
      "The following sections explain what each middleware provides.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> InMemoryStore (development)\n",
      "content===> Good for testing and development, but data is lost on restart:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prerequisites\n",
      "content===> Before you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> The text content\n",
      "\n",
      "MIME type of the text (e.g., text/plain , text/markdown )\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Example\n",
      "content===> Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: task(name=\"general-purpose\", task=\"Research quantum computing trends\") . The subagent performs all the searches internally and returns only a summary.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Retrieval Pipeline\n",
      "content===> A typical retrieval workflow looks like this:\n",
      "\n",
      "Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Purpose: Streaming tool call fragments\n",
      "\n",
      "Always \"tool_call_chunk\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Guardrails\n",
      "content===> Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent’s execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\n",
      "\n",
      "Common use cases include:\n",
      "\n",
      "You can implement guardrails using middleware to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\n",
      "\n",
      "Guardrails can be implemented using two complementary approaches:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarize messages\n",
      "content===> Summarize earlier messages in the history and replace them with a summary\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Message metadata\n",
      "content===> The name field behavior varies by provider - some use it for user identification, others ignore it. To check, refer to the model provider’s reference .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Testing requirements\n",
      "content===> Directories are relative to the package you’re working in.\n",
      "\n",
      "Every code change must include comprehensive tests.\n",
      "\n",
      "Location : tests/unit_tests/\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool use in the ReAct loop\n",
      "content===> Agents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n",
      "\n",
      "Prompt: Identify the current most popular wireless headphones and verify availability.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Keep state raw, format prompts on-demand\n",
      "content===> Notice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Backends\n",
      "content===> Deep agents expose a filesystem surface to the agent via tools like ls , read_file , write_file , edit_file , glob , and grep . These tools operate through a pluggable backend.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Step 1: Map out your workflow as discrete steps\n",
      "content===> Start by identifying the distinct steps in your process. Each step will become a node (a function that does one specific thing). Then sketch how these steps connect to each other.\n",
      "\n",
      "The arrows show possible paths, but the actual decision of which path to take happens inside each node.\n",
      "\n",
      "Now that you’ve identified the components in your workflow, let’s understand what each node needs to do:\n",
      "\n",
      "Notice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply).\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document structure-based\n",
      "content===> Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it’s beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:\n",
      "\n",
      "Examples of structure-based splitting:\n",
      "\n",
      "Available text splitters :\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add metadata to traces\n",
      "content===> tracing_context also accepts tags and metadata for fine-grained control:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pre-bound models\n",
      "content===> Dynamic model functions can return pre-bound models if structured output is not used.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Semantic Search\n",
      "content===> You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use MCP tools\n",
      "content===> langchain-mcp-adapters enables agents to use tools defined across one or more MCP server.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> create_agent\n",
      "content===> Under the hood, create_agent is built on the basic agent loop — calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Partial execution\n",
      "content===> Here’s an example that executes only the second and third nodes in a linear graph:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use with chat models\n",
      "content===> Chat models accept a sequence of message objects as input and return an AIMessage as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Read short-term memory in a tool\n",
      "content===> Access short term memory (state) in a tool using the ToolRuntime parameter.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dictionary format\n",
      "content===> You can also specify messages directly in OpenAI chat completions format.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add a graph as a node\n",
      "content===> When the parent graph and subgraph can communicate over a shared state key (channel) in the schema , you can add a graph as a node in another graph. For example, in multi-agent systems, the agents often communicate over a shared messages key.\n",
      "\n",
      "If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool retry\n",
      "content===> Either a tuple of exception types to retry on, or a callable that takes an exception and returns True if it should be retried.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems—such as APIs, databases, or file systems—using structured input.\n",
      "\n",
      "Tools are components that agents call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs. Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models , allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.\n",
      "\n",
      "Server-side tool use\n",
      "\n",
      "Some chat models (e.g., OpenAI , Anthropic , and Gemini ) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messages\n",
      "content===> Messages make up the prompt that is sent to the LLM.\n",
      "It’s critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n",
      "\n",
      "Inject uploaded file context from State when relevant to current query:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Repository structure\n",
      "content===> Located in libs/partners/ , these are independently versioned packages for specific integrations. For example:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ToolRuntime\n",
      "content===> Updating state:\n",
      "\n",
      "Use Command to update the agent’s state or control the graph’s execution flow:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Customizing agent memory\n",
      "content===> You can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph overview\n",
      "content===> LangGraph v1.0 is now available!\n",
      "\n",
      "For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\n",
      "\n",
      "If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\n",
      "\n",
      "Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n",
      "\n",
      "LangGraph is very low-level, and focused entirely on agent orchestration . Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools .\n",
      "\n",
      "We will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\n",
      "\n",
      "LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/overview\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SubAgent (Dictionary-based)\n",
      "content===> For most use cases, define subagents as dictionaries:\n",
      "\n",
      "Required fields:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Getting help\n",
      "content===> Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post .\n",
      "\n",
      "You now have everything you need to contribute high-quality documentation to LangChain! 🎤🦜\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in subgraphs\n",
      "content===> If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent Chat UI\n",
      "content===> Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking.\n",
      "\n",
      "Agent Chat UI is open source and can be adapted to your application needs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Self-improving instructions\n",
      "content===> An agent can update its own instructions based on feedback:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> State type restrictions\n",
      "content===> create_agent only supports TypedDict for state schemas. Pydantic models and dataclasses are no longer supported.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. View your agent in Studio\n",
      "content===> Safari blocks localhost connections to Studio. To work around this, run the above command with --tunnel to access Studio via a secure tunnel.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard content blocks\n",
      "content===> Content block support is currently only available for the following integrations:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Abso\n",
      "content===> Custom AI integration platform for enterprise workflows.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Embedding Models\n",
      "content===> Microsoft offers two main options for accessing embedding models through Azure:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> Tools should be well-documented: their name, description, and argument names become part of the model’s prompt.\n",
      "LangChain’s @tool decorator adds metadata and enables runtime injection via the ToolRuntime parameter.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. Test your application in Studio\n",
      "content===> For a LangGraph Server running on a custom host/port, update the baseURL parameter.\n",
      "\n",
      "Use the --tunnel flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> When streaming responses, tool calls are progressively built through ToolCallChunk . This allows you to see tool calls as they’re being generated rather than waiting for the complete response.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard content blocks\n",
      "content===> Broader support for content blocks will be rolled out gradually across more providers.\n",
      "\n",
      "The new content_blocks property introduces a standard representation for message content that works across providers:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> Prefix for action request descriptions\n",
      "\n",
      "InterruptOnConfig options:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Cloud\n",
      "content===> Access Gemini models, Vertex AI Model Garden and other Google Cloud services via Vertex AI and specific cloud integrations.\n",
      "\n",
      "Vertex AI models require the langchain-google-vertexai package. Other services might require additional packages like langchain-google-community , langchain-google-cloud-sql-pg , etc.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serialize standard content\n",
      "content===> Standard content blocks are not serialized into the content attribute by default. If you need to access standard content blocks in the content attribute (e.g., when sending messages to a client), you can opt-in to serializing them into content .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add persistence\n",
      "content===> You only need to provide the checkpointer when compiling the parent graph . LangGraph will automatically propagate the checkpointer to the child subgraphs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> YouTube Search Tool\n",
      "content===> Search YouTube videos without the official API. Requires youtube_search package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> RAG Agent\n",
      "content===> Create a Retrieval Augmented Generation (RAG) agent.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> General guidelines\n",
      "content===> Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.\n",
      "\n",
      "Documentation sections don’t exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.\n",
      "\n",
      "Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Transient context\n",
      "content===> What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what’s saved in state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud SQL for PostgreSQL\n",
      "content===> Vector store using Cloud SQL for PostgreSQL .\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> References\n",
      "content===> Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarization\n",
      "content===> Automatically summarize conversation history when approaching token limits.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft\n",
      "content===> All LangChain integrations with Microsoft Azure and other Microsoft products.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Disable streaming for specific chat models\n",
      "content===> If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\n",
      "models that do not support it.\n",
      "\n",
      "Set disable_streaming=True when initializing the model.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. View your agent in Studio\n",
      "content===> Studio makes each step of your agent easily observable. Replay any input and inspect the exact prompt, tool arguments, return values, and token/latency metrics. If a tool throws an exception, Studio records it with surrounding state so you can spend less time debugging.\n",
      "\n",
      "Keep your dev server running, edit prompts or tool signatures, and watch Studio hot-reload. Re-run the conversation thread from any step to verify behavior changes. See Manage threads for more details.\n",
      "\n",
      "As your agent grows, the same view scales from a single-tool demo to multi-node graphs, keeping decisions legible and reproducible.\n",
      "\n",
      "For an in-depth look at Studio, check out the overview page .\n",
      "\n",
      "For more information about local and deployed agents, see Set up local LangGraph Server and Deploy .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ProviderStrategy\n",
      "content===> ProviderStrategy uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure OpenAI\n",
      "content===> Azure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3 , Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using CompiledSubAgent\n",
      "content===> For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Zotero\n",
      "content===> Reference management and research tool.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Observability\n",
      "content===> Traces capture every step your agent takes, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This enables you to debug your agents, evaluate performance, and monitor usage.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Resuming interrupts\n",
      "content===> After an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> When to use it\n",
      "content===> The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Message content\n",
      "content===> Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.\n",
      "\n",
      "LangChain chat models accept message content in the content attribute, and can contain:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Environment variables\n",
      "content===> Be sure not to commit your .env to version control systems such as Git!\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Jobs\n",
      "content===> Query job listings. Requires google-search-results package and SerpApi key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Manual formatting and linting\n",
      "content===> Code formatting and linting are enforced via CI/CD. Run these commands before committing to ensure your code passes checks.\n",
      "\n",
      "Run formatting and linting:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. View your agent in Studio\n",
      "content===> Your agent will be accessible via API ( http://127.0.0.1:2024 ) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Provider strategy\n",
      "content===> LangChain automatically uses ProviderStrategy when you pass a schema type directly to create_agent.response_format and the model supports native structured output:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFacePipeline\n",
      "content===> We can use the HuggingFacePipeline class to run open source models locally.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent harness capabilities\n",
      "content===> This page lists out the components that make up the agent harness.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Customizing agent memory\n",
      "content===> By default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model Context Protocol (MCP)\n",
      "content===> Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the langchain-mcp-adapters library.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Search\n",
      "content===> Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you’ll work with the following capabilities:\n",
      "\n",
      "See set up instructions .\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFaceBgeEmbeddings\n",
      "content===> BGE models on the HuggingFace are one of the best open-source embedding models .\n",
      "BGE model is created by the Beijing Academy of Artificial Intelligence (BAAI) . BAAI is a private non-profit organization engaged in AI research and development.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ZenGuard AI\n",
      "content===> If you’d like to contribute an integration, see Contributing integrations .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat models\n",
      "content===> Chat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to traditional, plaintext LLMs) .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Enable tracing\n",
      "content===> To enable tracing for your application, set the following environment variables:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Structured output\n",
      "content===> Error handling : Control error handling via the handle_errors parameter to ToolStrategy :\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Gemma local from Hugging Face\n",
      "content===> Local Gemma model loaded from HuggingFace. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent progress\n",
      "content===> For example, if you have an agent that calls a tool once, you should see the following updates:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Connect to your agent\n",
      "content===> Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n",
      "\n",
      "Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarize messages\n",
      "content===> The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\n",
      "Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n",
      "\n",
      "To summarize message history in an agent, use the built-in SummarizationMiddleware :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interface\n",
      "content===> Each document loader may define its own parameters, but they share a common API:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> File system access\n",
      "content===> The harness provides six tools for file system operations, making files first-class citizens in the agent’s environment:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Token usage\n",
      "content===> An AIMessage can hold token counts and other usage metadata in its usage_metadata field:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Define model node\n",
      "content===> The model node is used to call the LLM and decide whether to call a tool or not.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> By default, the model has the freedom to choose which bound tool to use based on the user’s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or any tool from a given list:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Enable tracing\n",
      "content===> You can get your API key from your LangSmith settings .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI image editor\n",
      "content===> Given an image and a prompt, edit the image. Currently only supports mask-free editing. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> For simple bugfixes, you can get started immediately:\n",
      "\n",
      "Fork the LangChain or LangGraph repo to your personal GitHub account\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Neptune\n",
      "content===> Amazon Neptune is a high-performance graph analytics and serverless database for superior scalability and availability.\n",
      "\n",
      "For the Cypher and SPARQL integrations below, we need to install the langchain-aws library.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Where to customize\n",
      "content===> There are several points where you can control how context is passed between the main agent and its subagents:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Why use subagents?\n",
      "content===> Subagents solve the context bloat problem . When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work—the main agent receives only the final result, not the dozens of tool calls that produced it.\n",
      "\n",
      "When to use subagents:\n",
      "\n",
      "When NOT to use subagents:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google\n",
      "content===> See Google’s guide on migrating from the Gemini API to Vertex AI for more details on the differences.\n",
      "\n",
      "Integration packages for Gemini models and the Vertex AI platform are maintained in the langchain-google repository. You can find a host of LangChain integrations with other Google APIs and services in the googleapis Github organization and the langchain-google-community package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Isolate storage by assistant ID\n",
      "content===> Each assistant gets its own namespace in the Store, preventing cross-contamination.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tool selector\n",
      "content===> Instructions for the selection model. Uses built-in prompt if not specified.\n",
      "\n",
      "Maximum number of tools to select. Defaults to no limit.\n",
      "\n",
      "List of tool names to always include in the selection\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI Model Garden\n",
      "content===> Access Gemini, and hundreds of OSS models via Vertex AI Model Garden service. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ToolRuntime\n",
      "content===> Accessing state:\n",
      "\n",
      "Tools can access the current graph state using ToolRuntime :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph v1 migration guide\n",
      "content===> This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the what’s new page.\n",
      "\n",
      "To upgrade:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Drive\n",
      "content===> Google Drive file storage. Currently supports Google Docs.\n",
      "\n",
      "Install with Drive dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Parameters\n",
      "content===> A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n",
      "\n",
      "The name or identifier of the specific model you want to use with a provider.\n",
      "\n",
      "The key required for authenticating with the model’s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable .\n",
      "\n",
      "Controls the randomness of the model’s output. A higher number makes responses more creative; lower ones make them more deterministic.\n",
      "\n",
      "The maximum time (in seconds) to wait for a response from the model before canceling the request.\n",
      "\n",
      "Limits the total number of tokens in the response, effectively controlling how long the output can be.\n",
      "\n",
      "The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n",
      "\n",
      "Using init_chat_model , pass these parameters as inline **kwargs :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Social Platforms\n",
      "content===> The below document loaders allow you to load documents from different social media platforms.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multimodal\n",
      "content===> Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks .\n",
      "\n",
      "All LangChain chat models with underlying multimodal capabilities support:\n",
      "\n",
      "See the multimodal section of the messages guide for details.\n",
      "\n",
      "Some models can return multimodal data as part of their response. If invoked to do so, the resulting AIMessage will have content blocks with multimodal types.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Advanced considerations\n",
      "content===> Or why separate Doc Search from Draft Reply?\n",
      "\n",
      "The answer involves trade-offs between resilience and observability.\n",
      "\n",
      "The resilience consideration: LangGraph’s durable execution creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.\n",
      "\n",
      "Why we chose this breakdown for the email agent:\n",
      "\n",
      "Isolation of external services: Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.\n",
      "\n",
      "Intermediate visibility: Having Classify Intent as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom tool message content\n",
      "content===> The tool_message_content parameter allows you to customize the message that appears in the conversation history when structured output is generated:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Message content\n",
      "content===> You can think of a message’s content as the payload of data that gets sent to the model. Messages have a content attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> When binding user-defined tools, the model’s response includes a request to execute a tool. When using a model separately from an agent , it is up to you to perform the requested action and return the result back to the model for use in subsequent reasoning. Note that when using an agent , the agent loop will handle the tool execution loop for you.\n",
      "\n",
      "Below, we show some common ways you can use tool calling.\n",
      "\n",
      "When a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes agent abstractions that handle this orchestration for you.\n",
      "\n",
      "Here’s a simple example of how to do this:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 1. Create a repository on GitHub\n",
      "content===> Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide . Then, push your code to the repository.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/deploy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static model\n",
      "content===> Model identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\" ). Refer to the reference to see a full list of model identifier string mappings.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Testing first\n",
      "content===> Every change must include comprehensive tests to verify correctness and prevent regressions\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> Callback events allow LangGraph stream() and astream_events() to surface the chat model’s output in real-time.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use with any LLM\n",
      "content===> Let’s invoke the graph with an AIMessage that includes a tool call:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Advanced considerations\n",
      "content===> Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn’t prescribe this.\n",
      "\n",
      "Performance considerations: More nodes doesn’t mean slower execution. LangGraph writes checkpoints in the background by default ( async durability mode ), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use \"exit\" mode to checkpoint only at completion, or \"sync\" mode to block execution until each checkpoint is written.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> After model\n",
      "content===> Access short term memory (state) in @after_model middleware to process messages after model calls.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> URL pointing to the image location.\n",
      "\n",
      "Base64-encoded image data.\n",
      "\n",
      "Reference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).\n",
      "\n",
      "Image MIME type (e.g., image/jpeg , image/png )\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Before model\n",
      "content===> Access short term memory (state) in @before_model middleware to process messages before model calls.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state viastate_schema\n",
      "content===> Use the state_schema parameter as a shortcut to define custom state that is only used in tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reporting issues\n",
      "content===> Please report any issues discovered with 1.0 on GitHub using the 'v1' label .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuring interrupts\n",
      "content===> You must configure a checkpointer to persist the graph state across interrupts.\n",
      "In production, use a persistent checkpointer like AsyncPostgresSaver . For testing or prototyping, use InMemorySaver .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AWS Lambda\n",
      "content===> Amazon AWS Lambda is a serverless computing service provided by Amazon Web Services ( AWS ). It helps developers to build and run applications and services without\n",
      "provisioning or managing servers. This serverless architecture enables you to focus on writing and\n",
      "deploying code, while AWS automatically takes care of scaling, patching, and managing the\n",
      "infrastructure required to run your applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> To-do list tracking\n",
      "content===> The harness provides a write_todos tool that agents can use to maintain a structured task list.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling strategy\n",
      "content===> Custom content for the tool message returned when structured output is generated.\n",
      "If not provided, defaults to a message showing the structured response data.\n",
      "\n",
      "Error handling strategy for structured output validation failures. Defaults to True .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> create_agent automatically returns a compiled LangGraph graph that we can pass to the graphs key in our configuration file.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Initialize a model\n",
      "content===> The easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SerpApi\n",
      "content===> SerpApi provides API access to Google search results. Requires langchain-community .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text splitters\n",
      "content===> Break large docs into smaller chunks that will be retrievable individually and fit within a model’s context window.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> LangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching embeddings .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bedrock Chat\n",
      "content===> Amazon Bedrock is a fully managed service that offers a choice of\n",
      "high-performing foundation models (FMs) from leading AI companies like AI21 Labs , Anthropic , Cohere , Meta , Stability AI , and Amazon via a single API, along with a broad set of capabilities you need to\n",
      "build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock ,\n",
      "you can easily experiment with and evaluate top FMs for your use case, privately customize them with\n",
      "your data using techniques such as fine-tuning and Retrieval Augmented Generation ( RAG ), and build\n",
      "agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is\n",
      "serverless, you don’t have to manage any infrastructure, and you can securely integrate and deploy\n",
      "generative AI capabilities into your applications using the AWS services you are already familiar with.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupt decision types\n",
      "content===> The middleware defines three built-in ways a human can respond to an interrupt:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Async with Python < 3.11\n",
      "content===> In Python versions < 3.11, asyncio tasks do not support the context parameter.\n",
      "This limits LangGraph ability to automatically propagate context, and affects LangGraph’s streaming mechanisms in two key ways:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Structured output\n",
      "content===> In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Node changes\n",
      "content===> Structured output used to be generated in a separate node from the main agent. This is no longer the case.\n",
      "We generate structured output in the main loop, reducing cost and latency.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multimodal\n",
      "content===> Multimodality refers to the ability to work with data that comes in different\n",
      "forms, such as text, audio, images, and video. LangChain includes standard types\n",
      "for these data that can be used across providers.\n",
      "\n",
      "Chat models can accept multimodal data as input and generate\n",
      "it as output. Below we show short examples of input messages featuring multimodal data.\n",
      "\n",
      "Extra keys can be included top-level in the content block or nested in \"extras\": {\"key\": value} .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Control the input to the subagent\n",
      "content===> There are two main levers to control the input that the main agent passes to a subagent:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Validating human input\n",
      "content===> Sometimes you need to validate input from humans and ask again if it’s invalid. You can do this using multiple interrupt calls in a loop.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory storage\n",
      "content===> For more information about the memory store, see the Persistence guide.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Translate\n",
      "content===> Google Translate is a multilingual neural machine\n",
      "translation service developed by Google to translate text, documents and websites\n",
      "from one language into another.\n",
      "\n",
      "The GoogleTranslateTransformer allows you to translate text and HTML with the Google Cloud Translation API .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat Completions API\n",
      "content===> Refer to the OpenRouter documentation for more details.\n",
      "\n",
      "To capture reasoning tokens ,\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vector stores\n",
      "content===> Specialized databases for storing and searching embeddings.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Get state history\n",
      "content===> In our example, the output of get_state_history will look like this:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Initialization\n",
      "content===> To initialize a vector store, provide it with an embedding model:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Gmail\n",
      "content===> Load chat history from Gmail threads.\n",
      "\n",
      "Install with Gmail dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph\n",
      "content===> Have an idea for a new feature or enhancement?\n",
      "\n",
      "Search the issues for the respective repository for existing feature requests:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> The Human-in-the-Loop (HITL) middleware lets you add human oversight to agent tool calls.\n",
      "When a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.\n",
      "\n",
      "It does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an interrupt that halts execution. The graph state is saved using LangGraph’s persistence layer , so execution can pause safely and resume later.\n",
      "\n",
      "A human decision then determines what happens next: the action can be approved as-is ( approve ), modified before running ( edit ), or rejected with feedback ( reject ).\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dangling tool call repair\n",
      "content===> The harness fixes message history when tool calls are interrupted or cancelled before receiving results.\n",
      "\n",
      "The problem:\n",
      "\n",
      "The solution:\n",
      "\n",
      "Why it’s useful:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> You can accumulate chunks to build complete tool calls:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent Chat UI\n",
      "content===> LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using create_agent . This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith ).\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bring-your-own documents\n",
      "content===> The below retrievers allow you to index and search a custom corpus of documents.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Partial execution\n",
      "content===> For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to restructure these sections as subgraphs , which you can invoke in isolation as normal.\n",
      "\n",
      "However, if you do not wish to make changes to your agent graph’s overall structure, you can use LangGraph’s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Threads\n",
      "content===> A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs . When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\n",
      "\n",
      "When invoking a graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trace selectively\n",
      "content===> You may opt to trace specific invocations or parts of your application using LangSmith’s tracing_context context manager:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invoke\n",
      "content===> The model takes messages as input and outputs messages after generating a complete response.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> Now, simply decorate your tests with the vcr marker:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Finance\n",
      "content===> The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Name of the tool to call\n",
      "\n",
      "Arguments to pass to the tool\n",
      "\n",
      "Unique identifier for this tool call\n",
      "\n",
      "Example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Features\n",
      "content===> Studio automatically renders tool calls and results in an intuitive interface.\n",
      "\n",
      "Navigate through conversation history and fork from any point\n",
      "\n",
      "View and modify agent state at any point during execution\n",
      "\n",
      "Built-in support for reviewing and responding to agent requests\n",
      "\n",
      "You can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Node-style hooks\n",
      "content===> Run sequentially at specific execution points. Use for logging, validation, and state updates.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text property\n",
      "content===> Use of the .text() method on message objects should drop the parentheses, as it is now a property:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Log to a project\n",
      "content===> You can set the project name programmatically for specific operations:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a.envfile\n",
      "content===> You will find a .env.example in the root of your new LangGraph app. Create a .env file in the root of your new LangGraph app and copy the contents of the .env.example file into it, filling in the necessary API keys:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messages\n",
      "content===> LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2. Define state\n",
      "content===> The graph’s state is used to store the messages and the number of LLM calls.\n",
      "\n",
      "State in LangGraph persists throughout the agent’s execution.\n",
      "\n",
      "The Annotated type with operator.add ensures that new messages are appended to the existing list rather than replacing it.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Side effects called beforeinterruptmust be idempotent\n",
      "content===> Because interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Transport types\n",
      "content===> MCP supports different transport mechanisms for client-server communication:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent harness capabilities\n",
      "content===> We think of deepagents as an “agent harness”. It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use Cases\n",
      "content===> Below are tutorials for common use cases, organized by framework.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Rate limiting\n",
      "content===> LangChain in comes with (an optional) built-in InMemoryRateLimiter . This limiter is thread safe and can be shared by multiple threads in the same process.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Database for PostgreSQL\n",
      "content===> Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It’s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n",
      "\n",
      "See set up instructions for Azure Database for PostgreSQL.\n",
      "\n",
      "You need to enable pgvector extension in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the PGVector in LangChain to connect to Azure Database for PostgreSQL.\n",
      "\n",
      "See a usage example . Simply use the connection string from your Azure Portal.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2-step RAG\n",
      "content===> In 2-Step RAG , the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add metadata to traces\n",
      "content===> tracing_context also accepts tags and metadata for fine-grained control:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AI Message\n",
      "content===> An AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Structured outputs\n",
      "content===> Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured outputs.\n",
      "\n",
      "Pydantic models provide the richest feature set with field validation, descriptions, and nested structures.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document loaders\n",
      "content===> Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph\n",
      "content===> If no requests exist, start a new discussion under the relevant category so that project maintainers and the community can provide feedback.\n",
      "\n",
      "Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.\n",
      "\n",
      "Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pre-model hook\n",
      "content===> Common use cases include:\n",
      "\n",
      "v1 now has summarization middleware as a built in option:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom state\n",
      "content===> state_schema is still supported for backwards compatibility on create_agent .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Case studies\n",
      "content===> This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we’d love for you to share your story and add it to the list. You’re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/case-studies\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Case studies\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Drive\n",
      "content===> Retrieve documents from Google Drive.\n",
      "\n",
      "Install required packages:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model\n",
      "content===> By default, deepagents uses \"claude-sonnet-4-5-20250929\" . You can customize this by passing any LangChain model object .\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM Steps\n",
      "content===> When a step needs to understand, analyze, generate text, or make reasoning decisions:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Webpages\n",
      "content===> The below document loaders allow you to load webpages.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Handoffs\n",
      "content===> In handoffs , agents can directly pass control to each other. The “active” agent changes, and the user interacts with whichever agent currently has control.\n",
      "\n",
      "Flow:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Usage\n",
      "content===> LangChain’s agent manages short-term memory as a part of your agent’s state.\n",
      "\n",
      "By storing these in the graph’s state, the agent can access the full context for a given conversation while maintaining separation between different threads.\n",
      "\n",
      "State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n",
      "\n",
      "Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft Presidio\n",
      "content===> Presidio (Origin from Latin praesidium ‘protection, garrison’)\n",
      "helps to ensure sensitive data is properly managed and governed. It provides fast identification and\n",
      "anonymization modules for private entities in text and images such as credit card numbers, names,\n",
      "locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\n",
      "\n",
      "First, you need to install several python packages and download a SpaCy model.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent jumps\n",
      "content===> To exit early from middleware, return a dictionary with jump_to :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in production\n",
      "content===> You need to call store.setup() the first time you’re using Postgres store\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> In addition to any tools that you provide, deep agents also get access to a number of default tools:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model call limit\n",
      "content===> Limit the number of model calls to prevent infinite loops or excessive costs.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tokens\n",
      "content===> If your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Durability modes\n",
      "content===> for persistence policy management, with the following mapping:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Productivity tools\n",
      "content===> The below document loaders allow you to load data from commonly used productivity tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom state\n",
      "content===> Custom state extends the default agent state with additional fields. You can define custom state in two ways:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model\n",
      "content===> The model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2. Prepare your agent\n",
      "content===> We’ll use the following simple agent as an example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quickstart\n",
      "content===> This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Encryption\n",
      "content===> Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes , which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument):\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invoke a graph from a node\n",
      "content===> A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have completely different schemas from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system.\n",
      "\n",
      "If that’s the case for your application, you need to define a node function that invokes the subgraph . This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text splitters\n",
      "content===> Text splitters break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.\n",
      "\n",
      "There are several strategies for splitting documents, each with its own advantages.\n",
      "\n",
      "For most use cases, start with the RecursiveCharacterTextSplitter . It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Similarity search\n",
      "content===> Issue a semantic query using similarity_search , which returns the closest embedded documents:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom middleware\n",
      "content===> Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\n",
      "\n",
      "You can create middleware in two ways:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trajectory match\n",
      "content===> Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.\n",
      "\n",
      "Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn’t require additional LLM calls.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Always use a checkpointer\n",
      "content===> Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpointer interface\n",
      "content===> Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat Completions API\n",
      "content===> To use OpenRouter, you will need to sign up for an account and obtain an API key .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add long-term memory\n",
      "content===> Use long-term memory to store user-specific or application-specific data across conversations.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Disable streaming\n",
      "content===> In some applications you might need to disable streaming of individual tokens for a given model.\n",
      "\n",
      "This is useful in multi-agent systems to control which agents stream their output.\n",
      "\n",
      "See the Models guide to learn how to disable streaming.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Namespace\n",
      "content===> Most of these are re-exported from langchain-core for convenience, which gives you a focused API surface for building agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Related resources\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tokens\n",
      "content===> Use the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Generative AI (Gemini API & AI Studio)\n",
      "content===> Access Google Gemini models directly using the Gemini API, best suited for rapid development and experimentation. Gemini models are available in Google AI Studio .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Similarity metrics\n",
      "content===> Several metrics are commonly used to compare embeddings:\n",
      "\n",
      "Here’s an example of computing cosine similarity between two vectors:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Wrong subagent being selected\n",
      "content===> Problem : Main agent calls inappropriate subagent for the task.\n",
      "\n",
      "Solution : Differentiate subagents clearly in descriptions:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ActiveLoop DeepLake\n",
      "content===> Vector database for AI applications with deep learning focus.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagents\n",
      "content===> Deep agents can create subagents to delegate work. You can specify custom subagents in the subagents parameter. Subagents are useful for context quarantine (keeping the main agent’s context clean) and for providing specialized instructions.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> System Prompt\n",
      "content===> The system prompt sets the LLM’s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n",
      "\n",
      "Access message count or conversation context from state:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use with any LLM\n",
      "content===> This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use time-travel\n",
      "content===> When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:\n",
      "\n",
      "LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.\n",
      "\n",
      "To use time-travel in LangGraph:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool error handling\n",
      "content===> To customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Runtime context\n",
      "content===> When you invoke an agent, it’s often the case that you want to pass two types of data:\n",
      "\n",
      "In v1, static context is supported by setting the context parameter to invoke and stream .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Contributing to documentation\n",
      "content===> Accessible documentation is a vital part of LangChain. We welcome both documentation for new features/ integrations , as well as community improvements to existing docs.\n",
      "\n",
      "We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please open a new issue .\n",
      "\n",
      "All documentation falls under one of four categories:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Studio\n",
      "content===> This guide will walk you through how to use Studio to visualize, interact, and debug your agent locally.\n",
      "\n",
      "Studio is our free-to-use, powerful agent IDE that integrates with LangSmith to enable tracing, evaluation, and prompt engineering. See exactly how your agent thinks, trace every decision, and ship smarter, more reliable agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Next steps\n",
      "content===> Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:\n",
      "\n",
      "Deployment quickstart : Deploy your LangGraph app using LangSmith.\n",
      "\n",
      "LangSmith : Learn about foundational LangSmith concepts.\n",
      "\n",
      "Python SDK Reference : Explore the Python SDK API Reference.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Mintlify components\n",
      "content===> Use appropriate Mintlify components to enhance readability:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Migrate tocreate_agent\n",
      "content===> Prior to v1.0, we recommended using langgraph.prebuilt.create_react_agent to build agents. Now, we recommend you use langchain.agents.create_agent to build agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Conversation history summarization\n",
      "content===> The harness automatically compresses old conversation history when token usage becomes excessive.\n",
      "\n",
      "Configuration:\n",
      "\n",
      "Why it’s useful:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic tool definition\n",
      "content===> Type hints are required as they define the tool’s input schema. The docstring should be informative and concise to help the model understand the tool’s purpose.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Local models\n",
      "content===> LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\n",
      "\n",
      "Ollama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop patterns\n",
      "content===> Learn how to add tool approval before execution, batch approval, and other patterns\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream Writer\n",
      "content===> Stream custom updates from tools as they execute using runtime.stream_writer . This is useful for providing real-time feedback to users about what a tool is doing.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invocation config\n",
      "content===> When invoking a model, you can pass additional configuration through the config parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human Message\n",
      "content===> A HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add metadata to traces\n",
      "content===> This custom metadata and tags will be attached to the trace in LangSmith.\n",
      "\n",
      "To learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Initialize a model\n",
      "content===> See init_chat_model for more detail, including information on how to pass model parameters .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Code quality standards\n",
      "content===> Quality requirements:\n",
      "\n",
      "Required : Complete type annotations for all functions\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Environment variables\n",
      "content===> If you’re working with a deployed LangGraph application locally, you can configure environment variables in the env key of the LangGraph configuration file .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick start\n",
      "content===> By default, the trace will be logged to the project with the name default . To configure a custom project name, see Log to a project .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Ollama\n",
      "content===> All LangChain integrations with Ollama .\n",
      "\n",
      "Ollama allows you to run open-source models (like gpt-oss ) locally.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/ollama\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Ollama\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Middleware\n",
      "content===> Middleware is the defining feature of create_agent . It offers a highly customizable entry-point, raising the ceiling for what you can build.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent middleware\n",
      "content===> Handing off tasks to subagents isolates context, keeping the main (supervisor) agent’s context window clean while still going deep on a task.\n",
      "\n",
      "The subagents middleware allows you to supply subagents through a task tool.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AzureOpenAI\n",
      "content===> Wrapper for (legacy) OpenAI text completion models hosted on Azure.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> OpenAI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Advanced considerations\n",
      "content===> Different failure modes: LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.\n",
      "\n",
      "Reusability and testing: Smaller nodes are easier to test in isolation and reuse in other workflows.\n",
      "\n",
      "A different valid approach: You could combine Read Email and Classify Intent into a single node. You’d lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Contributing\n",
      "content===> Welcome! Thank you for your interest in contributing.\n",
      "\n",
      "LangChain has helped form the largest developer community in generative AI, and we’re always open to new contributors. Whether you’re fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent interrupts\n",
      "content===> Each subagent can have its own interrupt_on configuration that overrides the main agent’s settings:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> create_agent\n",
      "content===> create_agent is the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent while offering greater customization potential by using middleware .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Studio\n",
      "content===> This guide will walk you through how to use Studio to visualize, interact, and debug your agent locally.\n",
      "\n",
      "Studio is our free-to-use, powerful agent IDE that integrates with LangSmith to enable tracing, evaluation, and prompt engineering. See exactly how your agent thinks, trace every decision, and ship smarter, more reliable agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> In the hot path\n",
      "content===> Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\n",
      "\n",
      "However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\n",
      "\n",
      "As an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 5. Install dependencies\n",
      "content===> In the root of your new LangGraph app, install the dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> When we use the LangSmith, either locally (e.g., in Studio ) or hosted with LangSmith , the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Semantic Search\n",
      "content===> Now when searching, you can use natural language queries to find relevant memories:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PII detection\n",
      "content===> Type of PII to detect. Can be a built-in type ( email , credit_card , ip , mac_address , url ) or a custom type name.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model\n",
      "content===> The actual model (including configuration) to be called.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AWS (Amazon)\n",
      "content===> All LangChain integrations with the Amazon AWS platform.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Wrap-style hooks\n",
      "content===> You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n",
      "\n",
      "Example: Model retry middleware\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent not being called\n",
      "content===> Problem : Main agent tries to do work itself instead of delegating.\n",
      "\n",
      "Solutions :\n",
      "\n",
      "Make descriptions more specific:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> The argument will no longer accept ToolNode instances.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Try out your agent\n",
      "content===> The graph pauses when it hits interrupt() , saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The thread_id ensures all state for this conversation is preserved together.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> exampleparameter removed fromAIMessage\n",
      "content===> The example parameter has been removed from AIMessage objects. We recommend migrating to use additional_kwargs for passing extra metadata as needed.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic system prompt\n",
      "content===> For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware .\n",
      "\n",
      "The @dynamic_prompt decorator creates middleware that generates system prompts dynamically based on the model request:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hugging Face Hub Tools\n",
      "content===> Hugging Face Tools support text I/O and are loaded using the load_huggingface_tool function.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pause usinginterrupt\n",
      "content===> The interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick start\n",
      "content===> The fastest way to get started is using the hosted version:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Rules of interrupts\n",
      "content===> When you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> OpenAIModerationChain\n",
      "content===> Detect text that could be hateful, violent, etc.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> OpenAI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> New features\n",
      "content===> We aim to keep the bar high for new features. We generally don’t accept new core abstractions, changes to infra, changes to dependencies, or new agents/chains from outside contributors without an existing issue that demonstrates an acute need for them.\n",
      "\n",
      "In general, feature contribution requirements include:\n",
      "\n",
      "Open an issue describing:\n",
      "\n",
      "We will reject features that are likely to lead to security vulnerabilities or reports.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agents must use correct paths\n",
      "content===> The agent must learn to use the /memories/ prefix for persistence. The system prompt teaches this, but the agent must follow the instructions.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream multiple modes\n",
      "content===> The streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE prompt and configure the reference_outputs variable:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agents\n",
      "content===> Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n",
      "\n",
      "create_agent provides a production-ready agent implementation.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> All retrievers\n",
      "content===> Note: The descriptions in the table below are truncated for readability.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Base URL or proxy\n",
      "content===> When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective reference for details.\n",
      "\n",
      "For deployments requiring HTTP proxies, some model integrations support proxy configuration:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configurable models\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tool selector\n",
      "content===> Model for tool selection. Can be a model string or BaseChatModel instance. Defaults to the agent’s main model.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> System prompt\n",
      "content===> When no system_prompt is provided, the agent will infer its task from the messages directly.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon API Gateway\n",
      "content===> API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of\n",
      "concurrent API calls, including traffic management, CORS support, authorization and access control,\n",
      "throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs.\n",
      "You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Services individual tools\n",
      "content===> The azure_ai_services toolkit includes the following tools:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Caching\n",
      "content===> Embeddings can be stored or temporarily cached to avoid needing to recompute them.\n",
      "\n",
      "Caching embeddings can be done using a CacheBackedEmbeddings . This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Break into discrete steps\n",
      "content===> Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Validating human input\n",
      "content===> Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Decision types\n",
      "content===> The allowed_decisions list controls what actions a human can take when reviewing a tool call:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Conceptual guides\n",
      "content===> Conceptual guide cover core concepts abstractly, providing deep understanding.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Install LangChain\n",
      "content===> See the Integrations tab for a full list of available integrations.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/install\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\n",
      "\n",
      "Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangChain\n",
      "content===> LangChain agent implementations make it easy to get started for most use cases.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Planning and task decomposition\n",
      "content===> Deep agents include a built-in write_todos tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuration file\n",
      "content===> See the LangGraph configuration file reference for details on all supported keys in the JSON file.\n",
      "\n",
      "The LangGraph CLI defaults to using the configuration file langgraph.json in the current directory.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Usage\n",
      "content===> To add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Graph API\n",
      "content===> Explore LangGraph’s declarative graph-building API.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model call limit\n",
      "content===> Maximum model calls across all runs in a thread. Defaults to no limit.\n",
      "\n",
      "Maximum model calls per single invocation. Defaults to no limit.\n",
      "\n",
      "Behavior when limit is reached. Options: \"end\" (graceful termination) or \"error\" (raise exception)\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Durable execution\n",
      "content===> Durable execution is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require human-in-the-loop , where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps — even after a significant delay (e.g., a week later).\n",
      "\n",
      "LangGraph’s built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted — whether by a system failure or for human-in-the-loop interactions — it can be resumed from its last recorded state.\n",
      "\n",
      "If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\n",
      "To make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks . You can use tasks from both the StateGraph (Graph API) and the Functional API .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Isolate storage by assistant ID\n",
      "content===> For multi-tenant applications, provide an assistant_id to isolate storage:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Enable tracing\n",
      "content===> By default, the trace will be logged to the project with the name default . To configure a custom project name, see Log to a project .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> What you can control\n",
      "content===> To build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Semantic Search\n",
      "content===> Build a semantic search engine over a PDF with LangChain components.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Athena\n",
      "content===> Amazon Athena is a serverless, interactive analytics service built\n",
      "on open-source frameworks, supporting open-table and file formats.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How-to guides\n",
      "content===> Task-oriented instructions for users who know what they want to accomplish\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state via middleware\n",
      "content===> See the middleware documentation for more details on defining custom state via middleware.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serialization withpickle\n",
      "content===> The default serializer, JsonPlusSerializer , uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Don't Store\n",
      "content===> Can you derive it from other data? If yes, compute it when needed instead of storing it in state.\n",
      "\n",
      "For our email agent, we need to track:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Semantic Search\n",
      "content===> Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Replay\n",
      "content===> You must pass these when invoking the graph as part of the configurable portion of the config:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add persistence\n",
      "content===> If you want the subgraph to have its own memory , you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon OpenSearch Service\n",
      "content===> Amazon OpenSearch Service performs\n",
      "interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is\n",
      "an open source,\n",
      "distributed search and analytics suite derived from Elasticsearch . Amazon OpenSearch Service offers the\n",
      "latest versions of OpenSearch , support for many versions of Elasticsearch , as well as\n",
      "visualization capabilities powered by OpenSearch Dashboards and Kibana .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summary of changes\n",
      "content===> LangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of create_react_agent in favor of LangChain’s new create_agent function.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bigtable\n",
      "content===> Google Cloud Bigtable is a fully managed NoSQL Big Data database service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> You can also set the tool_args_match_mode property and/or tool_args_match_overrides to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serper.dev\n",
      "content===> Google Serper provides API access to Google search results. Requires langchain-community .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Selecting formats\n",
      "content===> Dynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.\n",
      "\n",
      "Configure structured output based on conversation state:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> In LangGraph, Pregel combines actors and channels into a single application. Actors read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the Pregel Algorithm / Bulk Synchronous Parallel model.\n",
      "\n",
      "Each step consists of three phases:\n",
      "\n",
      "Repeat until no actors are selected for execution, or a maximum number of steps is reached.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic model\n",
      "content===> Pre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Write long-term memory from tools\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Other Google Products\n",
      "content===> Integrations with various Google services beyond the core Cloud Platform.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Integration Testing\n",
      "content===> AgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a trajectory match or by using an LLM judge :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tool selector\n",
      "content===> Use an LLM to intelligently select relevant tools before calling the main model.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI Vector Search\n",
      "content===> Google Cloud Vertex AI Vector Search from Google Cloud,\n",
      "formerly known as Vertex AI Matching Engine , provides the industry’s leading high-scale\n",
      "low latency vector database. These vector databases are commonly\n",
      "referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Setup\n",
      "content===> Set up LangSmith for LangGraph development Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AWS S3 Directory and File\n",
      "content===> Amazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory AWS S3 Buckets\n",
      "\n",
      "See a usage example for S3DirectoryLoader .\n",
      "\n",
      "See a usage example for S3FileLoader .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft Presidio\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Install LangGraph\n",
      "content===> To use LangGraph you will usually want to access LLMs and define tools.\n",
      "You can do this however you see fit.\n",
      "\n",
      "One way to do this (which we will use in the docs) is to use LangChain .\n",
      "\n",
      "Install LangChain with:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/install\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Install\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add metadata to traces\n",
      "content===> You can annotate your traces with custom metadata and tags:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Core benefits\n",
      "content===> LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/overview\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Anthropic prompt caching\n",
      "content===> Reduce costs by caching repetitive prompt prefixes with Anthropic models.\n",
      "\n",
      "Perfect for:\n",
      "\n",
      "Learn more about Anthropic Prompt Caching strategies and limitations.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> Tools let a model interact with external systems by calling functions you define.\n",
      "Tools can depend on runtime context and also interact with agent memory .\n",
      "\n",
      "Notice below how the get_user_location tool uses runtime context:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Minor changes\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Setup\n",
      "content===> To build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deprecation ofcreate_react_agent\n",
      "content===> The LangGraph create_react_agent prebuilt has been deprecated in favor of LangChain’s create_agent . It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/customization\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Customization\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Decision types\n",
      "content===> Use approve to approve the tool call as-is and execute it without changes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream subgraph outputs\n",
      "content===> To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent spawning\n",
      "content===> A built-in task tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent’s context clean while still going deep on specific subtasks.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud SQL for MySQL\n",
      "content===> Vector store using Cloud SQL for MySQL .\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom middleware\n",
      "content===> For more information, see the complete middleware guide .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> See the LangGraph configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\n",
      "\n",
      "So far, our project structure looks like this:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Durability modes\n",
      "content===> LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application’s requirements. The durability modes, from least to most durable, are as follows:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> This guide shows a typical structure of an application and shows how the required information to deploy an application using the LangSmith is specified.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream multiple modes\n",
      "content===> You can pass a list as the stream_mode parameter to stream multiple modes at once.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using with subgraphs called as functions\n",
      "content===> When invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and the interrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AI Message\n",
      "content===> AIMessage objects are returned by the model when calling it, which contains all of the associated metadata in the response.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Firestore (Native Mode)\n",
      "content===> Vector store using Firestore .\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tool emulator\n",
      "content===> Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Cosmos DB NoSQL\n",
      "content===> Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available\n",
      "in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.\n",
      "\n",
      "Sign Up for free to get started today.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Graph structure emerges naturally\n",
      "content===> You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Partial execution\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Observability\n",
      "content===> Observability is crucial for understanding how your agents behave in production. With LangChain’s create_agent , you get built-in observability through LangSmith - a powerful platform for tracing, debugging, evaluating, and monitoring your LLM applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Log probabilities\n",
      "content===> Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the logprobs parameter when initializing the model:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n",
      "\n",
      "You may hear the term “function calling”. We use this interchangeably with “tool calling”.\n",
      "\n",
      "To make tools that you have defined available for use by a model, you must bind them using bind_tools() . In subsequent invocations, the model can choose to call any of the bound tools as needed.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trim messages\n",
      "content===> To trim message history, use the trim_messages function:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tool emulator\n",
      "content===> List of tool names (str) or BaseTool instances to emulate. If None (default), ALL tools will be emulated. If empty list, no tools will be emulated.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prompt caching\n",
      "content===> Prompt caching is often only engaged above a minimum input token threshold. See provider pages for details.\n",
      "\n",
      "Cache usage will be reflected in the usage metadata of the model response.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> values\n",
      "content===> These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let’s walk through an example.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic Usage\n",
      "content===> Each memory type is a Python class ( Item ) with certain attributes. We can access it as a dictionary by converting via .dict as above.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Embedding models\n",
      "content===> An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> State is shared memory\n",
      "content===> Store raw data, not formatted text. This lets different nodes use the same information in different ways.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invocation\n",
      "content===> You can invoke an agent by passing an update to its State . All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud Providers\n",
      "content===> The below document loaders allow you to load documents from your favorite cloud providers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multiple tool calls\n",
      "content===> When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> LangChain’s create_agent runs on LangGraph’s runtime under the hood.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> Each ToolMessage returned by the tool includes a tool_call_id that matches the original tool call, helping the model correlate results with requests.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic Usage\n",
      "content===> First, let’s showcase this in isolation without using LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agents\n",
      "content===> Agents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n",
      "\n",
      "To get started with agents, see the quickstart or read more about how they work in LangChain.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon MemoryDB\n",
      "content===> Amazon MemoryDB is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,\n",
      "enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.\n",
      "\n",
      "InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Playwright URL Loader\n",
      "content===> Playwright is an open-source automation tool\n",
      "developed by Microsoft that allows you to programmatically control and automate\n",
      "web browsers. It is designed for end-to-end testing, scraping, and automating\n",
      "tasks across various web browsers such as Chromium , Firefox , and WebKit .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AWS\n",
      "content===> Amazon Web Services cloud platform and AI services.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Provider-specific data structure\n",
      "\n",
      "Usage: For experimental or provider-unique features\n",
      "\n",
      "Additional provider-specific content types may be found within the reference documentation of each model provider.\n",
      "\n",
      "View the canonical type definitions in the API reference .\n",
      "\n",
      "Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the content property, but rather a new property that can be used to access the content of a message in a standardized format.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Message content\n",
      "content===> Specifying content_blocks when initializing a message will still populate message content , but provides a type-safe interface for doing so.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dall-E Image Generator\n",
      "content===> Text-to-image generation using OpenAI’s Dall-E models.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> OpenAI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Provider strategy\n",
      "content===> Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\n",
      "\n",
      "If the provider natively supports structured output for your model choice, it is functionally equivalent to write response_format=ProductReview instead of response_format=ToolStrategy(ProductReview) . In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic system prompt\n",
      "content===> For more details on message types and formatting, see Messages . For comprehensive middleware documentation, see Middleware .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic Usage\n",
      "content===> Memories are namespaced by a tuple , which in this specific example will be (<user_id>, \"memories\") . The namespace can be any length and represent anything, does not have to be user specific.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stable core APIs\n",
      "content===> Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Legacy code moved tolangchain-classic\n",
      "content===> Existing functionality outside the focus of standard interfaces and agents has been moved to the langchain-classic package. See the Simplified namespace section for details on what’s available in the core langchain package and what moved to langchain-classic .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google\n",
      "content===> All LangChain integrations with Google Cloud , Google Gemini and other Google products.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Scholar\n",
      "content===> Search academic papers. Requires google-search-results package and SerpApi key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prompt chaining\n",
      "content===> Prompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Middleware\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Include in State\n",
      "content===> Does it need to persist across steps? If yes, it goes in state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom state schema\n",
      "content===> Middleware can extend the agent’s state with custom properties. Define a custom state type and set it as the state_schema :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Cosmos DB for MongoDB (vCore)\n",
      "content===> Azure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support.\n",
      "You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account’s connection string.\n",
      "Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that’s stored in Azure Cosmos DB.\n",
      "\n",
      "See detailed configuration instructions .\n",
      "\n",
      "We need to install pymongo python package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multiple specialized subagents\n",
      "content===> Create specialized subagents for different domains:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a basic agent\n",
      "content===> To learn how to trace your agent with LangSmith, see the LangSmith documentation .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tutorial: Retrieval-Augmented Generation (RAG)\n",
      "content===> See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n",
      "This tutorial walks through two approaches:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> A vector store stores embedded data and performs similarity search.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph runtime\n",
      "content===> This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.\n",
      "\n",
      "Note: The Pregel runtime is named after Google’s Pregel algorithm , which describes an efficient method for large-scale parallel computation using graphs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool retry\n",
      "content===> Initial delay in seconds before first retry\n",
      "\n",
      "Maximum delay in seconds between retries (caps exponential backoff growth)\n",
      "\n",
      "Whether to add random jitter (±25%) to delay to avoid thundering herd\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deploy DocumentDB on AWS\n",
      "content===> Amazon DocumentDB (with MongoDB Compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\n",
      "\n",
      "AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see Cloud Computing with Amazon Web Services .\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-Loop\n",
      "content===> The harness pauses agent execution at specified tool calls to allow human approval/modification.\n",
      "\n",
      "Configuration:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream\n",
      "content===> Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\n",
      "\n",
      "Calling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> Mapping of tool names to approval configs. Values can be True (interrupt with default config), False (auto-approve), or an InterruptOnConfig object.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memorystore for Redis\n",
      "content===> Google Cloud Memorystore for Redis is a fully managed Redis service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic runtime context\n",
      "content===> Turning on memory Please see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agent’s state across multiple invocations. Otherwise, the state is scoped only to a single run.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pluggable storage backends\n",
      "content===> The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.\n",
      "\n",
      "Available backends:\n",
      "\n",
      "StateBackend - Ephemeral in-memory storage\n",
      "\n",
      "FilesystemBackend - Real filesystem access\n",
      "\n",
      "StoreBackend - Persistent cross-conversation storage\n",
      "\n",
      "CompositeBackend - Route different paths to different backends\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text structure-based\n",
      "content===> Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain’s RecursiveCharacterTextSplitter implements this concept:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream subgraph outputs\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interface\n",
      "content===> LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the Embeddings interface.\n",
      "\n",
      "Two main methods are available:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> History\n",
      "content===> LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.\n",
      "\n",
      "As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.\n",
      "\n",
      "Model APIs become more multimodal.\n",
      "\n",
      "Models started to accept files, images, videos, and more. We updated the langchain-core message format accordingly to allow developers to specify these multimodal inputs in a standard way.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/philosophy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Philosophy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory\n",
      "content===> Understand persistence of interactions within and across threads.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools and toolkits\n",
      "content===> To see a full list of integrations by component type, refer to the categories in the sidebar.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> From retrieval to RAG\n",
      "content===> Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they integrate retrieval with generation to produce grounded, context-aware answers.\n",
      "\n",
      "This is the core idea behind Retrieval-Augmented Generation (RAG) . The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quickstart\n",
      "content===> Here are a few pre-built filesystem backends that you can quickly use with your deep agent:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangChain v1 migration guide\n",
      "content===> This guide outlines the major changes between LangChain v1 and previous versions.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> El Carro for Oracle Workloads\n",
      "content===> Google El Carro Oracle Operator runs Oracle databases in Kubernetes.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuring interrupts\n",
      "content===> You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Cosmos DB NoSQL\n",
      "content===> Azure Cosmos DB for NoSQL now offers vector indexing and search in preview.\n",
      "This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors\n",
      "directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,\n",
      "but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,\n",
      "as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the\n",
      "efficiency of vector-based operations.\n",
      "\n",
      "See detail configuration instructions .\n",
      "\n",
      "We need to install azure-cosmos python package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream Writer\n",
      "content===> If you use runtime.stream_writer inside your tool, the tool must be invoked within a LangGraph execution context. See Streaming for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Protocol reference\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ChatHuggingFace\n",
      "content===> We can use the Hugging Face LLM classes or directly use the ChatHuggingFace class.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state via middleware\n",
      "content===> Middleware can also define custom state by setting the state_schema attribute.\n",
      "This helps to keep state extensions conceptually scoped to the relevant middleware and tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory storage\n",
      "content===> This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Step 5: Wire it together\n",
      "content===> The graph structure is minimal because routing happens inside nodes through Command objects. Each node declares where it can go using type hints like Command[Literal[\"node1\", \"node2\"]] , making the flow explicit and traceable.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Examples\n",
      "content===> Below are a few different examples to give you a sense of the Pregel API.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Embedding Models\n",
      "content===> Generate text embeddings using models like gemini-embedding-001 with the GoogleGenerativeAIEmbeddings class.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure OpenAI\n",
      "content===> Set the environment variables to get access to the Azure OpenAI service.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Code quality\n",
      "content===> Follow consistent style, documentation, and architecture patterns\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in subgraphs\n",
      "content===> If your graph contains subgraphs , you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Requirements\n",
      "content===> To leverage durable execution in LangGraph, you need to:\n",
      "\n",
      "Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.\n",
      "\n",
      "Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.\n",
      "\n",
      "Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside @[ task ] to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Anthropic prompt caching\n",
      "content===> Cache type. Only \"ephemeral\" is currently supported.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph ecosystem\n",
      "content===> While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/overview\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> RAG Architectures\n",
      "content===> RAG can be implemented in multiple ways, depending on your system’s needs. We outline each type in the sections below.\n",
      "\n",
      "Latency : Latency is generally more predictable in 2-Step RAG , as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent jumps\n",
      "content===> To enable jumping, decorate your hook with @hook_config(can_jump_to=[...]) :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trace selectively\n",
      "content===> You may opt to trace specific invocations or parts of your application using LangSmith’s tracing_context context manager:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> The unordered mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don’t care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn’t matter.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamically selecting tools\n",
      "content===> Select relevant tools at runtime to improve performance and accuracy.\n",
      "\n",
      "Benefits:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ChatGPTLoader\n",
      "content===> Load conversations.json from your ChatGPT data export folder.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> OpenAI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tutorial: Build a supervisor agent\n",
      "content===> Learn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents.\n",
      "This tutorial demonstrates:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Provider strategy\n",
      "content===> Some model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available.\n",
      "\n",
      "To use this strategy, configure a ProviderStrategy :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Error handling strategies\n",
      "content===> If handle_errors is a string, the agent will always prompt the model to re-try with a fixed tool message:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> The first time you run this test, your agent will make real network calls and pytest will generate a cassette file test_agent_trajectory.yaml in the tests/cassettes directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent’s requests don’t change from the previous run. If they do, the test will fail and you’ll need to delete the cassette and rerun the test to record fresh interactions.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFaceEmbeddings\n",
      "content===> We can use the HuggingFaceEmbeddings class to run open source embedding models locally.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Search\n",
      "content===> Perform web searches using Google Custom Search Engine (CSE). Requires GOOGLE_API_KEY and GOOGLE_CSE_ID .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure ML Chat Online Endpoint\n",
      "content===> See the documentation here for accessing chat\n",
      "models hosted with Azure Machine Learning .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ZeroxPDFLoader\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuring interrupts\n",
      "content===> When invoking the agent, pass a config that includes the thread ID to associate execution with a conversation thread.\n",
      "See the LangGraph interrupts documentation for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLMs and augmentations\n",
      "content===> Workflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling , structured outputs , and short term memory are a few options for tailoring LLMs to your needs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Spanner\n",
      "content===> Google Cloud Spanner is a fully managed, globally distributed relational database service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph runtime\n",
      "content===> Pregel implements LangGraph’s runtime, managing the execution of LangGraph applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SerpApi\n",
      "content===> See a usage example and authorization instructions .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic Usage\n",
      "content===> We can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Writes\n",
      "content===> Tool results can be used to help an agent complete a given task. Tools can both return results directly to the model\n",
      "and update the memory of the agent to make important context available to future steps.\n",
      "\n",
      "Write to State to track session-specific information using Command:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multiple specialized subagents\n",
      "content===> Workflow:\n",
      "\n",
      "Each subagent works with clean context focused only on its task.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFaceEndpoint\n",
      "content===> We can use the HuggingFaceEndpoint class to run open source models via serverless Inference Providers or via dedicated Inference Endpoints .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Filter by LLM invocation\n",
      "content===> You can associate tags with LLM invocations to filter the streamed tokens by LLM invocation.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure SQL Database\n",
      "content===> Sign Up for free to get started today.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLMs\n",
      "content===> Access the same Gemini models using the (legacy) LLM\n",
      "interface with the GoogleGenerativeAI class.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud Storage\n",
      "content===> Cloud Storage is a managed service for storing unstructured data.\n",
      "\n",
      "Install with GCS dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom updates\n",
      "content===> To stream updates from tools as they are executed, you can use get_stream_writer .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Anthropic prompt caching\n",
      "content===> Minimum number of messages before caching starts\n",
      "\n",
      "Behavior when using non-Anthropic models. Options: \"ignore\" , \"warn\" , or \"raise\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> External index\n",
      "content===> The below retrievers will search over an external index (e.g., constructed from Internet data or similar).\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/retrievers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Retrievers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hugging Face model loader\n",
      "content===> This loader interfaces with the Hugging Face Models API to fetch\n",
      "and load model metadata and README files.\n",
      "The API allows you to search and filter models based on\n",
      "specific criteria such as model tags, authors, and more.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> Pause agent execution for human approval before sensitive actions\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Installation\n",
      "content===> To get started, install the Toolbox server and client .\n",
      "\n",
      "Configure a tools.yaml to define your tools, and then execute toolbox to start the server:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Caching\n",
      "content===> The main supported way to initialize a CacheBackedEmbeddings is from_bytes_store . It takes the following parameters:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 1. Create a repository on GitHub\n",
      "content===> Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide . Then, push your code to the repository.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/deploy\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use descriptive paths\n",
      "content===> Organize long-term files with clear, hierarchical paths:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Gmail\n",
      "content===> Google Gmail is a free email service provided by Google.\n",
      "This toolkit works with emails through the Gmail API .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Security focused\n",
      "content===> Prioritize secure coding practices and vulnerability prevention\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard content blocks\n",
      "content===> A new content_blocks property that provides unified access to modern LLM features across providers.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Schema validation error\n",
      "content===> When structured output doesn’t match the expected schema, the agent provides specific error feedback:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reasoning\n",
      "content===> Newer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\n",
      "\n",
      "If supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarize messages\n",
      "content===> See SummarizationMiddleware for more configuration options.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PDFs\n",
      "content===> The below document loaders allow you to load PDF documents.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM-as-judge\n",
      "content===> Use a LLM to qualitatively validate your agent’s execution trajectory. The “judge” LLM reviews the agent’s decisions against a prompt rubric (which can include a reference trajectory).\n",
      "\n",
      "More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent’s trajectory without strict tool call or ordering requirements.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dependencies\n",
      "content===> A dependencies key in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Conceptual Overviews\n",
      "content===> These guides explain the core concepts and APIs underlying LangChain and LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> System Message\n",
      "content===> A SystemMessage represent an initial set of instructions that primes the model’s behavior. You can use a system message to set the tone, define the model’s role, and establish guidelines for responses.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Adding documents\n",
      "content===> Add Document objects (holding page_content and optional metadata) like so:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Rate limiting\n",
      "content===> Many chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n",
      "\n",
      "To help manage rate limits, chat model integrations accept a rate_limiter parameter that can be provided during initialization to control the rate at which requests are made.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Provider strategy\n",
      "content===> The schema defining the structured output format. Supports:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interface\n",
      "content===> Base stores are designed to work multiple key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Step 5: Wire it together\n",
      "content===> Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.\n",
      "\n",
      "To enable human-in-the-loop with interrupt() , we need to compile with a checkpointer to save state between runs:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Simplified package\n",
      "content===> The langchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Container Apps dynamic sessions\n",
      "content===> We need to get the POOL_MANAGEMENT_ENDPOINT environment variable from the Azure Container Apps service.\n",
      "See the instructions here .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in production\n",
      "content===> You need to call store.setup() the first time you’re using Redis store\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> In production, use a persistent checkpointer that saves to a database.\n",
      "See Add and manage memory for more details.\n",
      "\n",
      "Now assemble your agent with all the components and run it!\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard content blocks\n",
      "content===> LangChain provides a standard representation for message content that works across providers.\n",
      "\n",
      "Message objects implement a content_blocks property that will lazily parse the content attribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking or reasoning blocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock representation:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent jumps\n",
      "content===> Important: When jumping from before_model or after_model , jumping to \"model\" will cause all before_model middleware to run again.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in production\n",
      "content===> Setup To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don’t already have one.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Default message format for OpenAI Responses API\n",
      "content===> When interacting with the Responses API, langchain-openai now defaults to storing response items in message content . To restore previous behavior, set the LC_OUTPUT_VERSION environment variable to v0 , or specify output_version=\"v0\" when instantiating ChatOpenAI .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick start\n",
      "content===> No extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Data sources\n",
      "content===> Throughout this process, your agent accesses (reads / writes) different sources of data:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Graphs\n",
      "content===> You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context\n",
      "content===> Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through runtime.context .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupt decision types\n",
      "content===> The available decision types for each tool depend on the policy you configure in interrupt_on .\n",
      "When multiple tool calls are paused at the same time, each action requires a separate decision.\n",
      "Decisions must be provided in the same order as the actions appear in the interrupt request.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model fallback\n",
      "content===> Automatically fallback to alternative models when the primary model fails.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 5. Launch LangGraph server 🚀\n",
      "content===> The langgraph dev command starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see the Platform setup overview .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Translate\n",
      "content===> First, we need to install the langchain-google-community with translate dependencies.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Execution order\n",
      "content===> When using multiple middleware, understanding execution order is important:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent middleware\n",
      "content===> In addition to any user-defined subagents, the main agent has access to a general-purpose subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the general-purpose subagent is context isolation—the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Keep system prompts detailed\n",
      "content===> Include specific guidance on how to use tools and format outputs:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Local development\n",
      "content===> For customization or local development, you can run Agent Chat UI locally:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic configuration\n",
      "content===> The interrupt_on parameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Supported models\n",
      "content===> LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Evaluators\n",
      "content===> Evaluate model outputs using Vertex AI.\n",
      "\n",
      "Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. View your agent in Studio\n",
      "content===> Your agent will be accessible via API ( http://127.0.0.1:2024 ) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 :\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Testing individual nodes and edges\n",
      "content===> Compiled LangGraph agents expose references to each individual node as graph.nodes . You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Wrap-style hooks\n",
      "content===> Intercept execution and control when the handler is called:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> \"exit\"\n",
      "content===> Changes are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuration\n",
      "content===> subagents should be a list of dictionaries or CompiledSubAgent objects. There are two types:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Search\n",
      "content===> Azure AI Search is a cloud search service\n",
      "that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid\n",
      "queries at scale. See here for usage examples.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Contributing to code\n",
      "content===> Code contributions are always welcome! Whether you’re fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n",
      "\n",
      "Before submitting large new features or refactors , please first discuss your ideas in the forum . This ensures alignment with project goals and prevents duplicate work.\n",
      "\n",
      "This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. Be sure to link any relevant issues in your PR description. Use closing keywords to automatically close issues when the PR is merged.\n",
      "\n",
      "New integrations should follow the integration guidelines .\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context editing\n",
      "content===> Token count that triggers the edit\n",
      "\n",
      "Minimum tokens to reclaim\n",
      "\n",
      "Number of recent tool results to preserve\n",
      "\n",
      "Whether to clear tool call parameters\n",
      "\n",
      "List of tool names to exclude from clearing\n",
      "\n",
      "Placeholder text for cleared outputs\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PII detection\n",
      "content===> LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\n",
      "\n",
      "PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\n",
      "\n",
      "The PII middleware supports multiple strategies for handling detected PII:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph\n",
      "content===> Thank you for helping make LangChain better! 🦜❤️\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in production\n",
      "content===> You need to call checkpointer.setup() the first time you’re using Postgres checkpointer\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> All integration providers\n",
      "content===> Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hugging Face Text-to-Speech Model Inference.\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bugfixes\n",
      "content===> For bugfix contributions:\n",
      "\n",
      "Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n",
      "\n",
      "Add unit tests that would fail without your fix\n",
      "\n",
      "Make the minimal change necessary to resolve the issue\n",
      "\n",
      "Ensure that tests pass and no regressions are introduced\n",
      "\n",
      "Update docstrings if behavior changes, add comments for complex logic\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> See the RunnableConfig reference for a full list of supported attributes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> Tools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Common patterns\n",
      "content===> With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SystemMessageto string\n",
      "content===> If using SystemMessage objects in the system prompt, extract the string content:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Listing files\n",
      "content===> Files from the Store are prefixed with /memories/ in listings.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> We invoke the graph with a thread_id , as before, and also with a user_id , which we’ll use to namespace our memories to this particular user as we showed above.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> \"sync\"\n",
      "content===> Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\n",
      "\n",
      "You can specify the durability mode when calling any graph execution method:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> URL pointing to the video location.\n",
      "\n",
      "Base64-encoded video data.\n",
      "\n",
      "Reference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).\n",
      "\n",
      "Video MIME type (e.g., video/mp4 , video/webm )\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Installation and Setup\n",
      "content===> See detail configuration instructions .\n",
      "\n",
      "We need to install the pymongo python package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Long-term memory\n",
      "content===> Deep agents come with a local filesystem to offload memory. This filesystem is stored in state and is therefore transient to a single thread —files are lost when the conversation ends.\n",
      "\n",
      "You can extend deep agents with long-term memory by providing a LangGraph Store and setting use_longterm_memory=True . This enables persistent storage that survives across threads and conversations.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFaceInferenceAPIEmbeddings\n",
      "content===> We can use the HuggingFaceInferenceAPIEmbeddings class to run open source embedding models via Inference Providers .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure SQL Database\n",
      "content===> Azure SQL Database is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.\n",
      "\n",
      "By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.\n",
      "\n",
      "See detail configuration instructions .\n",
      "\n",
      "We need to install the langchain-sqlserver python package.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> System prompt\n",
      "content===> You can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Office 365 individual tools\n",
      "content===> You can use individual tools from the Office 365 Toolkit:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> User preferences\n",
      "content===> Store user preferences that persist across sessions:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> LangChain chat models can also stream semantic events using astream_events() .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Execution lifecycle\n",
      "content===> The middleware defines an after_model hook that runs after the model generates a response but before any tool calls are executed:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invoke\n",
      "content===> The most straightforward way to call a model is to use invoke() with a single message or a list of messages.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory (Store)\n",
      "content===> Tools can access and update the store through ToolRuntime :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cross-thread persistence\n",
      "content===> Files in /memories/ can be accessed from any thread:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Delete messages\n",
      "content===> You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\n",
      "\n",
      "To delete messages from the graph state, you can use the RemoveMessage . For RemoveMessage to work, you need to use a state key with add_messages reducer , like MessagesState .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> LangChain agents use LangGraph persistence to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/long-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> WRITER\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> values\n",
      "content===> The foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Filter by node\n",
      "content===> To stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> High-level API\n",
      "content===> LangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API .\n",
      "\n",
      "The StateGraph (Graph API) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory Store\n",
      "content===> LangGraph API handles stores automatically When using the LangGraph API, you don’t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI image captioning\n",
      "content===> Implementation of the Image Captioning model as a chat. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Hugging Face\n",
      "content===> All LangChain integrations with Hugging Face Hub and libraries like transformers , sentence transformers , and datasets .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ToolRuntime\n",
      "content===> ToolRuntime : A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate InjectedState , InjectedStore , get_runtime , and InjectedToolCallId annotations.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Inside middleware\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static model\n",
      "content===> Model instances give you complete control over configuration. Use them when you need to set specific parameters like temperature , max_tokens , timeouts , base_url , and other provider-specific settings. Refer to the reference to see available params and methods on your model.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Additional resources\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trim messages\n",
      "content===> Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens ) to use for handling the boundary.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom guardrails\n",
      "content===> For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Writing standard\n",
      "content===> Reference documentation has different standards - see the reference docs contributing guide for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI , ChatAnthropic ). Check the respective provider reference for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Update state\n",
      "content===> In addition to re-playing the graph from specific checkpoints , we can also edit the graph state. We do this using update_state . This method accepts three different arguments:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PlayWright Browser individual tools\n",
      "content===> You can use individual tools from the PlayWright Browser Toolkit.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Simplified namespace\n",
      "content===> The langchain namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat Completions API\n",
      "content===> This is a known limitation with ChatOpenAI and will be addressed in a future release.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> The --record-mode=once option records HTTP interactions on the first run and replays them on subsequent runs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Retrievers\n",
      "content===> A retriever is an interface that returns documents given an unstructured query.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context\n",
      "content===> Tools can access runtime context through ToolRuntime :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Python reference\n",
      "content===> A good reference should:\n",
      "\n",
      "See the contributing guide for Python reference docs .\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Side effects called beforeinterruptmust be idempotent\n",
      "content===> As an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use semantic search\n",
      "content===> Enable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Server-side tool use\n",
      "content===> Some providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\n",
      "\n",
      "If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Easy to use, highly flexible agent\n",
      "content===> LangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defaultmax_tokensinlangchain-anthropic\n",
      "content===> The max_tokens parameter in langchain-anthropic now defaults to higher values based on the model chosen, rather than the previous default of 1024 . If you relied on the old default, explicitly set max_tokens=1024 .\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard model interface\n",
      "content===> Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Name of the tool being called\n",
      "\n",
      "Partial tool arguments (may be incomplete JSON)\n",
      "\n",
      "Tool call identifier\n",
      "\n",
      "Position of this chunk in the stream\n",
      "\n",
      "Purpose: Malformed calls, intended to catch JSON parsing errors.\n",
      "\n",
      "Always \"invalid_tool_call\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> View subgraph state\n",
      "content===> Available only when interrupted Subgraph state can only be viewed when the subgraph is interrupted . Once you resume the graph, you won’t be able to access the subgraph state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Test\n",
      "content===> After you’ve prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.\n",
      "\n",
      "Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out this section that uses LangChain’s built-in create_agent instead.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic cross-conversation context\n",
      "content===> Dynamic cross-conversation context represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Writing memories\n",
      "content===> There are two primary methods for agents to write memories: “in the hot path” and “in the background” .\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Context\n",
      "content===> Tools are special in that they both read and write context.\n",
      "\n",
      "In the most basic case, when a tool executes, it receives the LLM’s request parameters and returns a tool message back. The tool does its work and produces a result.\n",
      "\n",
      "Tools can also fetch important information for the model that allows it to perform and complete tasks.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calls\n",
      "content===> When models make tool calls , they’re included in the AIMessage :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 2. Create a LangGraph app 🌱\n",
      "content===> Create a new app from the new-langgraph-project-python template . This template demonstrates a single-node application you can extend with your own logic.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream custom data\n",
      "content===> No get_stream_writer in async for Python < 3.11 In async code running on Python < 3.11, get_stream_writer will not work.\n",
      "Instead, add a writer parameter to your node or tool and pass it manually.\n",
      "See Async with Python < 3.11 for usage examples.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Semantic memory\n",
      "content===> Semantic memory , both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.\n",
      "\n",
      "Semantic memory is different from “semantic search,” which is a technique for finding similar content using “meaning” (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.\n",
      "\n",
      "Semantic memories can be managed in different ways:\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Migration guide\n",
      "content===> See our migration guide for help updating your code to LangChain v1.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Streaming node name rename\n",
      "content===> When streaming events from agents, the node name has changed from \"agent\" to \"model\" to better reflect the node’s purpose.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpoints\n",
      "content===> Note that we bar channel values contain outputs from both nodes as we have a reducer for bar channel.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI image generator\n",
      "content===> Generates an image from a prompt. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Planning middleware\n",
      "content===> Planning is integral to solving complex problems. If you’ve used Claude Code recently, you’ll notice how it writes out a to-do list before tackling complex, multi-part tasks. You’ll also notice how it can adapt and update this to-do list on the fly as more information comes in.\n",
      "\n",
      "TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it’s doing and what still needs to be done.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool call limit\n",
      "content===> Specific tool to limit. If not provided, limits apply to all tools.\n",
      "\n",
      "Maximum tool calls across all runs in a thread. Defaults to no limit.\n",
      "\n",
      "Maximum tool calls per single invocation. Defaults to no limit.\n",
      "\n",
      "Behavior when limit is reached. Options: \"end\" (graceful termination) or \"error\" (raise exception)\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> What's new in v1\n",
      "content===> LangGraph v1 is a stability-focused release for the agent runtime. It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n",
      "\n",
      "It’s designed to work hand-in-hand with LangChain v1 (whose create_agent is built on LangGraph) so you can start high-level and drop down to granular control when needed.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Errors are part of the flow\n",
      "content===> Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Testing requirements\n",
      "content===> Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n",
      "\n",
      "Not every code change will require an integration test, but keep in mind that we’ll require/ run integration tests separately as apart of our review process.\n",
      "\n",
      "Location : tests/integration_tests/\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Nodes are functions\n",
      "content===> They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Thinking in LangGraph\n",
      "content===> LangGraph can change how you think about the agents you build. When you build an agent with LangGraph, you will first break it apart into discrete steps called nodes . Then, you will describe the different decisions and transitions for each of your nodes. Finally, you will connect your nodes together through a shared state that each node can read from and write to. In this tutorial, we’ll guide you through the thought process of building a customer support email agent with LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quickstart\n",
      "content===> This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n",
      "\n",
      "For conceptual information, see Graph API overview and Functional API overview .\n",
      "\n",
      "For this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n",
      "\n",
      "Short term memory lets your application remember previous interactions within a single thread or conversation.\n",
      "\n",
      "A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n",
      "\n",
      "Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today’s LLMs; a full history may not fit inside an LLM’s context window, resulting in an context loss or errors.\n",
      "\n",
      "Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.\n",
      "\n",
      "Chat models accept context using messages , which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or “forget” stale information.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Examples\n",
      "content===> While most users will interact with Pregel through the StateGraph API or the @entrypoint decorator, it is possible to interact with Pregel directly.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> References\n",
      "content===> Technical descriptions of APIs and implementation details\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Streaming\n",
      "content===> LangChain implements a streaming system to surface real-time updates.\n",
      "\n",
      "Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupts\n",
      "content===> Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.\n",
      "\n",
      "Interrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you’re ready to continue, you resume execution by re-invoking the graph using Command , which then becomes the return value of the interrupt() call from inside the node.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Responding to interrupts\n",
      "content===> When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in interrupt_on . In that case, the invocation result will include an __interrupt__ field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> For more configurability over how the LLM evaluates the trajectory, visit the repository .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> Pause agent execution for human approval, editing, or rejection of tool calls before they execute.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Response Format\n",
      "content===> The structured response is returned in the structured_response key of the agent’s final state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory\n",
      "content===> Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n",
      "\n",
      "Information stored in the state can be thought of as the short-term memory of the agent:\n",
      "\n",
      "Custom state schemas must extend AgentState as a TypedDict .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools\n",
      "content===> Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n",
      "\n",
      "For more information, see Tools .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Supervisor Agent\n",
      "content===> Build a personal assistant that delegates to sub-agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Persistence\n",
      "content===> Conversations automatically persist across sessions with built-in checkpointing\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AzureOpenAIEmbeddings\n",
      "content===> Wrapper for OpenAI embedding models hosted on Azure.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> OpenAI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Replay\n",
      "content===> It’s also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id , then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id , and only execute the steps after the checkpoint.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interface\n",
      "content===> LangChain provides a unified interface for vector stores, allowing you to:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Benefits\n",
      "content===> For more information, see our guide on content blocks .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLMs\n",
      "content===> You can also use the (legacy) string-in, string-out LLM\n",
      "interface.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Control the output from the subagent\n",
      "content===> Two common strategies for shaping what the main agent receives back from a subagent:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messages\n",
      "content===> The full list of messages (conversation history) sent to the LLM.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messages\n",
      "content===> Transient vs Persistent Message Updates:\n",
      "\n",
      "The examples above use wrap_model_call to make transient updates - modifying what messages are sent to the model for a single call without changing what’s saved in state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Approve or reject\n",
      "content===> When you resume the graph, pass true to approve or false to reject:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document what gets persisted\n",
      "content===> In system prompts, clarify when to use long-term vs short-term storage:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Persistence\n",
      "content===> LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread , which can be accessed after graph execution. Because threads allow access to graph’s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we’ll discuss each of these concepts in more detail.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarize messages\n",
      "content===> Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarize_conversation node can be called after some number of messages have accumulated in the messages state key.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Kendra\n",
      "content===> Amazon Kendra is an intelligent search service\n",
      "provided by Amazon Web Services ( AWS ). It utilizes advanced natural language processing (NLP) and machine\n",
      "learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately,\n",
      "improving productivity and decision-making.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream custom data\n",
      "content===> To send custom user-defined data from inside a LangGraph node or tool, follow these steps:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Structured output\n",
      "content===> Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.\n",
      "\n",
      "LangChain’s create_agent handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it’s captured, validated, and returned in the 'structured_response' key of the agent’s state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use anonymizers to prevent logging of sensitive data in traces\n",
      "content===> You may want to mask sensitive data to prevent it from being logged to LangSmith. You can create anonymizers and apply them to\n",
      "your graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a basic agent\n",
      "content===> Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n",
      "\n",
      "For this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> With this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Approve or reject\n",
      "content===> One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tutorial: Semantic search\n",
      "content===> Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\n",
      "In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in production\n",
      "content===> In production, use a checkpointer backed by a database:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> We can access the memories and use them in our model call.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Code examples\n",
      "content===> Always test code examples before publishing. Never include real API keys or secrets.\n",
      "\n",
      "Requirements for code examples:\n",
      "\n",
      "Include complete, runnable examples that users can copy and execute without errors\n",
      "\n",
      "Use realistic data instead of placeholder values like “foo” or “example”\n",
      "\n",
      "Show proper error handling and edge case management\n",
      "\n",
      "Add explanatory comments for complex logic\n",
      "\n",
      "Example of a well-documented function:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> Send multiple requests to a model in a batch for more efficient processing.\n",
      "\n",
      "In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Environment variables\n",
      "content===> Create a .env file in the root of your project and fill in the necessary API keys. We’ll need to set the LANGSMITH_API_KEY environment variable to the API key you get from LangSmith .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/studio\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Content blocks are represented (either when creating a message or accessing the content_blocks property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use with chat models\n",
      "content===> Refer to the below guides to learn more:\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Async Support\n",
      "content===> All agentevals evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding async after create_ in the function name.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom tool name\n",
      "content===> By default, the tool name comes from the function name. Override it when you need something more descriptive:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Test\n",
      "content===> Agentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model’s black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.\n",
      "\n",
      "There are a few approaches to testing your agents:\n",
      "\n",
      "Unit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n",
      "\n",
      "Integration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\n",
      "\n",
      "Agentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Development environment\n",
      "content===> Set up a development environment for the package(s) you’re working on.\n",
      "\n",
      "For changes to langchain-core :\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> See the tools guide for details and other options for creating tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Output of the executed tool.\n",
      "\n",
      "Purpose: Provider-specific escape hatch\n",
      "\n",
      "Always \"non_standard\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Relationship to the LangChain ecosystem\n",
      "content===> Deep agents is built on top of:\n",
      "\n",
      "Deep agents applications can be deployed via LangSmith Deployment and monitored with LangSmith Observability .\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Data Steps\n",
      "content===> When a step needs to retrieve information from external sources:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static runtime context\n",
      "content===> See Agents for details.\n",
      "\n",
      "The Runtime object can be used to access static context and other utilities like the active store and stream writer.\n",
      "See the @[ Runtime ][langgraph.runtime.Runtime] documentation for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> High-level API\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Task delegation (subagents)\n",
      "content===> The harness allows the main agent to create ephemeral “subagents” for isolated multi-step tasks.\n",
      "\n",
      "Why it’s useful:\n",
      "\n",
      "How it works:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat Completions API\n",
      "content===> Certain model providers offer endpoints that are compatible with OpenAI’s Chat Completions API . In such case, you can use ChatOpenAI with a custom base_url to connect to these endpoints.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream multiple modes\n",
      "content===> You can specify multiple streaming modes by passing stream mode as a list: stream_mode=[\"updates\", \"custom\"] :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 5. Define end logic\n",
      "content===> The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Get state\n",
      "content===> When interacting with the saved graph state, you must specify a thread identifier . You can view the latest state of the graph by calling graph.get_state(config) . This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Actors\n",
      "content===> An actor is a PregelNode . It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. PregelNodes implement LangChain’s Runnable interface.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/pregel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Where to go from here\n",
      "content===> This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Connect to your agent\n",
      "content===> Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n",
      "\n",
      "Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud Storage\n",
      "content===> Load from a directory or a specific file:\n",
      "\n",
      "See directory usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Manage short-term memory\n",
      "content===> With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\n",
      "\n",
      "This allows the agent to keep track of the conversation without exceeding the LLM’s context window.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream graph state\n",
      "content===> Use this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI Vector Search\n",
      "content===> Alias for VectorSearchVectorStore storing documents/index in GCS.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> AgentEvals offers the create_trajectory_match_evaluator function to match your agent’s trajectory against a reference trajectory. There are four modes to choose from:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prebuilt memory tools\n",
      "content===> LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Environment variables\n",
      "content===> Create a .env file in the root of your project and fill in the necessary API keys. We’ll need to set the LANGSMITH_API_KEY environment variable to the API key you get from LangSmith .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How to propose changes to the documentation\n",
      "content===> With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prompt caching (Anthropic)\n",
      "content===> The harness enables Anthropic’s prompt caching feature to reduce redundant token processing.\n",
      "\n",
      "How it works:\n",
      "\n",
      "Why it’s useful:\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/harness\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Agent harness\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context management\n",
      "content===> File system tools ( ls , read_file , write_file , edit_file ) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use the same thread ID\n",
      "content===> When resuming, you must use the same config with the same thread_id :\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Delete messages\n",
      "content===> You can delete messages from the graph state to manage the message history.\n",
      "\n",
      "This is useful when you want to remove specific messages or clear the entire message history.\n",
      "\n",
      "To delete messages from the graph state, you can use the RemoveMessage .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> See also\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serialization withpickle\n",
      "content===> If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\n",
      "you can use the pickle_fallback argument of the JsonPlusSerializer :\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prompted output removed\n",
      "content===> Prompted output is no longer supported via the response_format argument. Compared to strategies\n",
      "like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text prompts\n",
      "content===> Text prompts are strings - ideal for straightforward generation tasks where you don’t need to retain conversation history.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Key Concepts\n",
      "content===> To deploy using the LangSmith, the following information should be provided:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Featured providers\n",
      "content===> While all these LangChain classes support the indicated advanced feature , you may have to open the provider-specific documentation to learn which hosted models or backends support the feature.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Rules of interrupts\n",
      "content===> When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there’s a few important rules to follow when working with interrupts to ensure they behave as expected.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic Usage\n",
      "content===> We use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory ( memory_id ) and the value (a dictionary) is the memory itself.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state via middleware\n",
      "content===> Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add policy hooks\n",
      "content===> Enforce enterprise rules by subclassing or wrapping a backend.\n",
      "\n",
      "Block writes/edits under selected prefixes (subclass):\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bing Search API\n",
      "content===> Microsoft Bing , commonly referred to as Bing or Bing Search ,\n",
      "is a web search engine owned and operated by Microsoft .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Drive\n",
      "content===> Tools for interacting with Google Drive.\n",
      "\n",
      "Install required packages:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon API Gateway\n",
      "content===> Amazon API Gateway is a fully managed service that makes it easy for\n",
      "developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the “front door”\n",
      "for applications to access data, business logic, or functionality from your backend services. Using API Gateway , you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication\n",
      "applications. API Gateway supports containerized and serverless workloads, as well as web applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Web Browsing\n",
      "content===> The following table shows tools that can be used to automate tasks in web browsers:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Seamless with LangChain v1\n",
      "content===> LangChain’s create_agent runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Invocation\n",
      "content===> A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream subgraph outputs\n",
      "content===> To include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Comprehend Moderation Chain\n",
      "content===> Amazon Comprehend is a natural-language processing (NLP) service that\n",
      "uses machine learning to uncover valuable insights and connections in text.\n",
      "\n",
      "We need to install the boto3 and nltk libraries.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupt decision types\n",
      "content===> When editing tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subgraphs\n",
      "content===> This guide explains the mechanics of using subgraphs. A subgraph is a graph that is used as a node in another graph.\n",
      "\n",
      "Subgraphs are useful for:\n",
      "\n",
      "When adding subgraphs, you need to define how the parent graph and the subgraph communicate:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Subgraphs\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Runtime context\n",
      "content===> The old config[\"configurable\"] pattern still works for backward compatibility, but using the new context parameter is recommended for new applications or applications migrating to v1.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Models\n",
      "content===> LLMs are powerful AI tools that can interpret and generate text like humans. They’re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\n",
      "\n",
      "In addition to text generation, many models support:\n",
      "\n",
      "Models are the reasoning engine of agents . They drive the agent’s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\n",
      "\n",
      "The quality and capabilities of the model you choose directly impact your agent’s reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\n",
      "\n",
      "LangChain’s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your case.\n",
      "\n",
      "For provider-specific integration information and capabilities, see the provider’s chat model page .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PII detection\n",
      "content===> See the middleware documentation for complete details on PII detection capabilities.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Access\n",
      "content===> When creating an agent with create_agent , you can specify a context_schema to define the structure of the context stored in the agent Runtime .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining tools\n",
      "content===> Each tool needs a clear name, description, argument names, and argument descriptions. These aren’t just metadata—they guide the model’s reasoning about when and how to use the tool.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> Set up your language model with the right parameters for your use case:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling\n",
      "content===> In tool calling , one agent (the “ controller ”) treats other agents as tools to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results.\n",
      "\n",
      "Flow:\n",
      "\n",
      "Agents used as tools are generally not expected to continue conversation with the user.\n",
      "Their role is to perform a task and return results to the controller agent.\n",
      "If you need subagents to be able to converse with the user, use handoffs instead.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom RAG Agent\n",
      "content===> Build a RAG agent using LangGraph primitives for fine-grained control.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add short-term memory\n",
      "content===> Short-term memory (thread-level persistence ) enables agents to track multi-turn conversations. To add short-term memory:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Bedrock Converse\n",
      "content===> AWS Bedrock maintains a Converse API that provides a unified conversational interface for Bedrock models. This API does not\n",
      "yet support custom models. You can see a list of all models that are supported here .\n",
      "\n",
      "We recommend the Converse API for users who do not need to use custom models. It can be accessed using ChatBedrockConverse .\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> See also\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Streaming\n",
      "content===> LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n",
      "\n",
      "What’s possible with LangGraph streaming:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Parameters\n",
      "content===> Each chat model integration may have additional params used to control provider-specific functionality. For example, ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom MCP servers\n",
      "content===> To create your own MCP servers, you can use the mcp library. This library provides a simple way to define tools and run them as servers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PII detection\n",
      "content===> Detect and handle Personally Identifiable Information in conversations.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model\n",
      "content===> Different models have different strengths, costs, and context windows. Select the right model for the task at hand, which\n",
      "might change during an agent run.\n",
      "\n",
      "Use different models based on conversation length from State:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom MCP servers\n",
      "content===> Use the following reference implementations to test your agent with MCP tool servers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Wrong subagent being selected\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use in production\n",
      "content===> You need to call checkpointer.setup() the first time you’re using Redis checkpointer\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarize messages\n",
      "content===> The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n",
      "\n",
      "Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the MessagesState to include a summary key:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dropped Python 3.9 support\n",
      "content===> All LangChain packages now require Python 3.10 or higher . Python 3.9 reached end of life in October 2025.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Zilliz\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Context still getting bloated\n",
      "content===> Problem : Context fills up despite using subagents.\n",
      "\n",
      "Solutions :\n",
      "\n",
      "Instruct subagent to return concise results:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Access memory\n",
      "content===> You can access and modify the short-term memory (state) of an agent in several ways:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Token usage\n",
      "content===> A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the AIMessage objects produced by the corresponding model. For more details, see the messages guide.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reliability, by default\n",
      "content===> Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langgraph-v1\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Length-based\n",
      "content===> An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn’t exceed a specified size limit. Key benefits of length-based splitting:\n",
      "\n",
      "Types of length-based splitting:\n",
      "\n",
      "Example implementation using LangChain’s CharacterTextSplitter with token-based splitting:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/splitters\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Text splitters\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic usage\n",
      "content===> Models can be utilized in two ways:\n",
      "\n",
      "The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deep Agents Middleware\n",
      "content===> Deep agents are built with a modular middleware architecture. Deep agents have access to:\n",
      "\n",
      "Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add metadata to traces\n",
      "content===> You can annotate your traces with custom metadata and tags:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Time travel\n",
      "content===> Rewind conversations to any point and explore alternate paths and prompts\n",
      "\n",
      "You don’t need to learn LangGraph to use these features—they work out of the box.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Full development IDE setup\n",
      "content===> For larger changes or ongoing contributions, it’s important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting.\n",
      "\n",
      "Please review the steps to set up your environment outlined in the docs repo README.md .\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> BigQuery Vector Search\n",
      "content===> Google Cloud BigQuery ,\n",
      "BigQuery is a serverless and cost-effective enterprise data warehouse in Google Cloud.\n",
      "\n",
      "Google Cloud BigQuery Vector Search BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.\n",
      "\n",
      "It can calculate Euclidean or Cosine distance. With LangChain, we default to use Euclidean distance.\n",
      "\n",
      "We need to install several python packages.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Connect to your agent\n",
      "content===> Agent Chat UI can connect to both local and deployed agents .\n",
      "\n",
      "After starting Agent Chat UI, you’ll need to configure it to connect to your agent:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Delete messages\n",
      "content===> For RemoveMessage to work, you need to use a state key with add_messages reducer .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dependencies\n",
      "content===> Any additional binaries or system libraries can be specified using dockerfile_lines key in the LangGraph configuration file .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> The /memories/ path convention\n",
      "content===> The key to long-term memory is the /memories/ path prefix:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Server-side tool use\n",
      "content===> This represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling .\n",
      "\n",
      "See the integration page for your given provider for available tools and usage details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Fault-tolerance\n",
      "content===> Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Filesystem middleware\n",
      "content===> Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web_search and rag), as long tool results can quickly fill your context window.\n",
      "\n",
      "FilesystemMiddleware provides four tools for interacting with both short-term and long-term memory:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/middleware\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agentic RAG\n",
      "content===> Agentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.\n",
      "\n",
      "The only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Common File Types\n",
      "content===> The below document loaders allow you to load data from common data formats.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Prebuilt middleware\n",
      "content===> LangChain provides a few prebuilt middlewares for common patterns, including:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Document Intelligence\n",
      "content===> Document Intelligence supports PDF , JPEG/JPG , PNG , BMP , TIFF , HEIF , DOCX , XLSX , PPTX and HTML .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Combine multiple guardrails\n",
      "content===> You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Standard content blocks\n",
      "content===> See the integrations guides to get started with the\n",
      "inference provider of your choice.\n",
      "\n",
      "Serializing standard content\n",
      "\n",
      "If an application outside of LangChain needs access to the standard content block\n",
      "representation, you can opt-in to storing content blocks in message content.\n",
      "\n",
      "To do this, you can set the LC_OUTPUT_VERSION environment variable to v1 . Or,\n",
      "initialize any chat model with output_version=\"v1\" :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Do not wrapinterruptcalls in try/except\n",
      "content===> The way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Durability modes\n",
      "content===> A higher durability mode adds more overhead to the workflow execution.\n",
      "\n",
      "Added in v0.6.0 Use the durability parameter instead of checkpoint_during (deprecated in v0.6.0) for persistence policy management:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI Search\n",
      "content===> Build generative AI powered search engines using Vertex AI Search .\n",
      "from Google Cloud allows developers to quickly build generative AI powered search engines for customers and employees.\n",
      "\n",
      "See a usage example .\n",
      "\n",
      "Note: GoogleVertexAISearchRetriever is deprecated. Use the components below from langchain-google-community .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Simplified namespace\n",
      "content===> For a complete list of changes, see the migration guide .\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> LangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain.\n",
      "\n",
      "Middleware allows you to hook into any step in the agent lifecycle and:\n",
      "\n",
      "Throughout this guide, you’ll see frequent use of the middleware API as a means to the context engineering end.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom updates\n",
      "content===> If you add get_stream_writer inside your tool, you won’t be able to invoke the tool outside of a LangGraph execution context.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Caching\n",
      "content===> In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see stores integrations for options.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Data\n",
      "content===> First, you need to install several python packages.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Environment variables\n",
      "content===> Be sure not to commit your .env to version control systems such as Git!\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Integration\n",
      "content===> Integrate more tools for web search, database queries, and API calls\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Finance\n",
      "content===> Query financial data. Requires google-search-results package and SerpApi key.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> See the LangGraph configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\n",
      "\n",
      "So far, our project structure looks like this:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 1. Define tools and model\n",
      "content===> In this example, we’ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> InMemorySaver Checkpointer\n",
      "content===> To enable persistence during testing, you can use the InMemorySaver checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Define tool node\n",
      "content===> The tool node is used to call the tools and return the results.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM-as-Judge Evaluator\n",
      "content===> You can also use an LLM to evaluate the agent’s execution path with the create_trajectory_llm_as_judge function. Unlike the trajectory match evaluators, it doesn’t require a reference trajectory, but one can be provided if available.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> In the background\n",
      "content===> Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\n",
      "\n",
      "However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\n",
      "\n",
      "See our memory-service template as an reference implementation.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Data\n",
      "content===> Azure AI Foundry (formerly Azure AI Studio provides the capability to upload data assets\n",
      "to cloud storage and register existing data assets from the following sources:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Streaming\n",
      "content===> We’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Log to a project\n",
      "content===> You can set the project name programmatically for specific operations:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> History\n",
      "content===> For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the langchain-classic package.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/philosophy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Philosophy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messages\n",
      "content===> For persistent updates that modify state (like the summarization example in Life-cycle Context ), use life-cycle hooks like before_model or after_model to permanently update the conversation history. See the middleware documentation for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Return concise results\n",
      "content===> Instruct subagents to return summaries, not raw data:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom HITL logic\n",
      "content===> For more specialized workflows, you can build custom HITL logic directly using the interrupt primitive and middleware abstraction.\n",
      "\n",
      "Review the execution lifecycle above to understand how to integrate interrupts into the agent’s operation.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human-in-the-loop\n",
      "content===> Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the interrupt_on parameter.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Identifier of the corresponding server tool call.\n",
      "\n",
      "Identifier associated with the server tool result.\n",
      "\n",
      "Execution status of the server-side tool. \"success\" or \"error\" .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PlayWright Browser Toolkit\n",
      "content===> Playwright is an open-source automation tool\n",
      "developed by Microsoft that allows you to programmatically control and automate\n",
      "web browsers. It is designed for end-to-end testing, scraping, and automating\n",
      "tasks across various web browsers such as Chromium , Firefox , and WebKit .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpointer interface\n",
      "content===> For running your graph asynchronously, you can use InMemorySaver , or async versions of Sqlite/Postgres checkpointers — AsyncSqliteSaver / AsyncPostgresSaver checkpointers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use decorators when\n",
      "content===> • You need a single hook • No complex configuration\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tokens\n",
      "content===> The streamed output from messages mode is a tuple (message_chunk, metadata) where:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool retry\n",
      "content===> Automatically retry failed tool calls with configurable exponential backoff.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Built on top of LangGraph\n",
      "content===> LangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 3. Install dependencies\n",
      "content===> In the root of your new LangGraph app, install the dependencies in edit mode so your local changes are used by the server:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/local-server\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Local server\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model\n",
      "content===> Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). create_react_agent released in v0.6 of langgraph-prebuilt supported dynamic model and tool selection via a callable passed to the model parameter.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> URL pointing to the audio location.\n",
      "\n",
      "Base64-encoded audio data.\n",
      "\n",
      "Reference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).\n",
      "\n",
      "Audio MIME type (e.g., audio/mpeg , audio/wav )\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> You can access the runtime information within tools and middleware .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Do not reorderinterruptcalls within a node\n",
      "content===> It’s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\n",
      "\n",
      "When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task’s resume list. Matching is strictly index-based , so the order of interrupt calls within the node is important.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Repository structure\n",
      "content===> Many partner packages are in external repositories. Please check the list of integrations for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory storage\n",
      "content===> LangGraph stores long-term memories as JSON documents in a store . Each memory is organized under a custom namespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ZHIPU AI\n",
      "content===> If you’d like to contribute an integration, see Contributing integrations .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/chat\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Chat models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI\n",
      "content===> Generate embeddings using models deployed on Vertex AI. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Access\n",
      "content===> When invoking the agent, pass the context argument with the relevant configuration for the run:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Search\n",
      "content===> Azure AI Search (formerly known as Azure Search or Azure Cognitive Search ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Replay\n",
      "content===> Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id . All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Google Cloud\n",
      "content===> Google Cloud integrations typically use Application Default Credentials (ADC). Refer to the Google Cloud authentication documentation for setup instructions (e.g., using gcloud auth application-default login ).\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Language and style\n",
      "content===> Use Google-style docstrings with complete type hints for all public functions.\n",
      "\n",
      "Follow these standards for all documentation:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Removal of deprecated APIs\n",
      "content===> Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the deprecation notices from previous versions for replacement APIs.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Route to different backends\n",
      "content===> Route parts of the namespace to different backends. Commonly used to persist /memories/* and keep everything else ephemeral.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/backends\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Backends\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> Inside your app’s directory, create a configuration file langgraph.json :\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AnthropicLLM\n",
      "content===> (Legacy) Anthropic text completion models.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/anthropic\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Anthropic (Claude)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> See also\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Mocking Chat Model\n",
      "content===> For logic not requiring API calls, you can use an in-memory stub for mocking responses.\n",
      "\n",
      "LangChain provides GenericFakeChatModel for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Get state\n",
      "content===> In our example, the output of get_state will look like this:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serper.dev\n",
      "content===> See a usage example and authorization instructions .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Parameters\n",
      "content===> To find all the parameters supported by a given chat model, head to the chat model integrations page.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> How it works\n",
      "content===> When you invoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking on_llm_new_token events in LangChain’s callback system.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> When using batch_as_completed() , results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> In production\n",
      "content===> In production, use a checkpointer backed by a database:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Selecting tools\n",
      "content===> Not every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\n",
      "\n",
      "Enable advanced tools only after certain conversation milestones:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Handling tool errors\n",
      "content===> You can now configure the handling of tool errors with middleware implementing the wrap_tool_call method.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpoints\n",
      "content===> After we run the graph, we expect to see exactly 4 checkpoints:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> State type restrictions\n",
      "content===> Simply inherit from langchain.agents.AgentState instead of BaseModel or decorating with dataclass .\n",
      "If you need to perform validation, handle it in middleware hooks instead.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> langchain-classic\n",
      "content===> If you were using any of the following from the langchain package, you’ll need to install langchain-classic and update your imports:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom strategies\n",
      "content===> Custom strategies (e.g., message filtering, etc.)\n",
      "\n",
      "This allows the agent to keep track of the conversation without exceeding the LLM’s context window.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vector Stores\n",
      "content===> Store and search vectors using Google Cloud databases and Vertex AI Vector Search.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Base URL or proxy\n",
      "content===> For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\n",
      "\n",
      "Many model providers offer OpenAI-compatible APIs (e.g., Together AI , vLLM ). You can use init_chat_model with these providers by specifying the appropriate base_url parameter:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quickstart\n",
      "content===> This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You’ll build a research agent that can conduct research and write reports.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/quickstart\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Code Interpreter\n",
      "content===> The following table shows tools that can be used as code interpreters:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AI Message\n",
      "content===> The text content of the message.\n",
      "\n",
      "The raw content of the message.\n",
      "\n",
      "The standardized content blocks of the message.\n",
      "\n",
      "The tool calls made by the model. Empty if no tools are called.\n",
      "\n",
      "A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n",
      "\n",
      "The usage metadata of the message, which can contain token counts when available.\n",
      "\n",
      "The response metadata of the message.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph\n",
      "content===> LangChain’s agent implementations use LangGraph primitives.\n",
      "If deeper customization is required, agents can be implemented directly in LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/learn\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Learn\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Pending writes\n",
      "content===> Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use anonymizers to prevent logging of sensitive data in traces\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tools and toolkits\n",
      "content===> Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n",
      "\n",
      "A toolkit is a collection of tools meant to be used together.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reasoning\n",
      "content===> Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical “tiers” of reasoning (e.g., 'low' or 'high' ) or integer token budgets.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Life-cycle Context\n",
      "content===> Control what happens between the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\n",
      "\n",
      "As you’ve seen in Model Context and Tool Context , middleware is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangSmith Integration\n",
      "content===> LangSmith offers two main approaches for running evaluations: pytest integration and the evaluate function.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI visual QnA\n",
      "content===> Chat implementation of a visual QnA model. Requires langchain-google-vertexai .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Do not return complex values ininterruptcalls\n",
      "content===> Depending on which checkpointer is used, complex values may not be serializable (e.g. you can’t serialize a function). To make your graphs adaptable to any deployment, it’s best practice to only use values that can be reasonably serialized.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupts\n",
      "content===> The thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Implementation\n",
      "content===> Below is a minimal example where the main agent is given access to a single subagent via a tool definition:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/multi-agent\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Multi-agent\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agentic RAG\n",
      "content===> This example implements an Agentic RAG system to assist users in querying LangGraph documentation. The agent begins by loading llms.txt , which lists available documentation URLs, and can then dynamically use a fetch_documentation tool to retrieve and process the relevant content based on the user’s question.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Built-in middleware\n",
      "content===> LangChain provides prebuilt middleware for common use cases:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Debug with LangSmith\n",
      "content===> Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/overview\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Localization\n",
      "content===> We are working to add support for other languages, thanks to our LangChain Ambassadors!\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Debugging with interrupts\n",
      "content===> Static interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Ads4GPTs\n",
      "content===> Advertising platform for GPT applications and AI services.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Common patterns\n",
      "content===> The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> YouTube Transcripts Loader\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Cloud SQL for SQL Server\n",
      "content===> Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document AI Warehouse\n",
      "content===> Search, store, and manage documents using Document AI Warehouse .\n",
      "\n",
      "Note: GoogleDocumentAIWarehouseRetriever (from langchain ) is deprecated. Use DocumentAIWarehouseRetriever from langchain-google-community .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Summarization\n",
      "content===> Model for generating summaries\n",
      "\n",
      "Token threshold for triggering summarization\n",
      "\n",
      "Recent messages to preserve\n",
      "\n",
      "Custom token counting function. Defaults to character-based counting.\n",
      "\n",
      "Custom prompt template. Uses built-in template if not specified.\n",
      "\n",
      "Prefix for summary messages\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> All providers\n",
      "content===> See all providers or search for a provider using the search field.\n",
      "\n",
      "If you’d like to contribute an integration, see our contributing guide .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/overview\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dropped Python 3.9 support\n",
      "content===> All LangChain packages now require Python 3.10 or higher . Python 3.9 reaches end of life in October 2025.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agents\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Dynamic model\n",
      "content===> Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\n",
      "\n",
      "To use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft OneDrive File\n",
      "content===> Microsoft OneDrive (formerly SkyDrive ) is a file-hosting service operated by Microsoft.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick fix: submit a bugfix\n",
      "content===> Fix the bug while following our code quality standards\n",
      "\n",
      "Include unit tests that fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n",
      "\n",
      "Ensure all tests pass locally before submitting your PR\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vertex AI Vector Search\n",
      "content===> Vector search using Datastore for document storage.\n",
      "\n",
      "See usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Deploy\n",
      "content===> LangSmith is the fastest way to turn agents into production systems. Traditional hosting platforms are built for stateless, short-lived web apps, while LangGraph is purpose-built for stateful, long-running agents , so you can go from repo to reliable cloud deployment in minutes.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/deploy\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Deploy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory storage\n",
      "content===> For more information about the memory store, see the Persistence guide.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> The agent loop\n",
      "content===> A typical agent loop consists of two main steps:\n",
      "\n",
      "This loop continues until the LLM decides to finish.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Encryption\n",
      "content===> When running on LangSmith, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Before agent guardrails\n",
      "content===> Use “before agent” hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Microsoft Word\n",
      "content===> Microsoft Word is a word processor developed by Microsoft.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Inside middleware\n",
      "content===> You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n",
      "\n",
      "Use request.runtime to access the Runtime object inside middleware decorators. The runtime object is available in the ModelRequest parameter passed to middleware functions.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Middleware\n",
      "content===> Great agents require context engineering : getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpoints\n",
      "content===> The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using in LangGraph\n",
      "content===> See the deployment guide for more details and configuration options.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Post-model hook\n",
      "content===> Common use cases include:\n",
      "\n",
      "v1 has a built in middleware for human in the loop approval for tool calls:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Add metadata to traces\n",
      "content===> This custom metadata and tags will be attached to the trace in LangSmith.\n",
      "\n",
      "To learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation .\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/observability\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Basic usage\n",
      "content===> The simplest way to use messages is to create message objects and pass them to a model when invoking .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Checkpoints\n",
      "content===> Checkpoints are persisted and can be used to restore the state of a thread at a later time.\n",
      "\n",
      "Let’s see what checkpoints are saved when a simple graph is invoked as follows:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Document loaders\n",
      "content===> Document loaders provide a standard interface for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s Document format.\n",
      "This ensures that data can be handled consistently regardless of the source.\n",
      "\n",
      "All document loaders implement the BaseLoader interface.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reasoning\n",
      "content===> For details, see the integrations page or reference for your respective chat model.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Example: Summarization\n",
      "content===> The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\n",
      "\n",
      "For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Human input is first-class\n",
      "content===> The interrupt() function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> HuggingFaceEndpointEmbeddings\n",
      "content===> We can use the HuggingFaceEndpointEmbeddings class to run open source embedding models via a dedicated Inference Endpoint .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/huggingface\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Hugging Face\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Full development setup\n",
      "content===> For ongoing development or larger contributions:\n",
      "\n",
      "Set up your environment following our setup guide below\n",
      "\n",
      "Understand the repository structure and package organization\n",
      "\n",
      "Learn our development workflow including testing and linting\n",
      "\n",
      "Review our contribution guidelines for features, bugfixes, and integrations\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream Writer\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream\n",
      "content===> As opposed to invoke() , which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> Add memory to your agent to maintain state across interactions. This allows\n",
      "the agent to remember previous conversations and context.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Create a LangGraph config file\n",
      "content===> create_agent automatically returns a compiled LangGraph graph that we can pass to the graphs key in our configuration file.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Accessing Context\n",
      "content===> Why this matters: Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n",
      "\n",
      "Tools can access runtime information through the ToolRuntime parameter, which provides:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick edit: fix a typo\n",
      "content===> GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist, if present\n",
      "\n",
      "Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers. Do not bump the PR unless you have new information to provide - maintainers will address it as their availability permits.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ZhipuAI\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/text_embedding\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Embedding models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LangGraph\n",
      "content===> If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example . Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.\n",
      "\n",
      "A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.\n",
      "\n",
      "If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/overview\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Write short-term memory from tools\n",
      "content===> To modify the agent’s short-term memory (state) during execution, you can return state updates directly from the tools.\n",
      "\n",
      "This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/short-term-memory\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Short-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent progress\n",
      "content===> To stream agent progress, use the stream or astream methods with stream_mode=\"updates\" . This emits an event after every agent step.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/streaming\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Simplified package\n",
      "content===> LangChain v1 streamlines the langchain package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stateful tool usage\n",
      "content===> For stateful servers that maintain context between tool calls, use client.session() to create a persistent ClientSession .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> The hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Acknowledgements\n",
      "content===> LangGraph is inspired by Pregel and Apache Beam . The public interface draws inspiration from NetworkX . LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/overview\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Message\n",
      "content===> The stringified output of the tool call.\n",
      "\n",
      "The ID of the tool call that this message is responding to. (this must match the ID of the tool call in the AIMessage )\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> config\n",
      "content===> The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent Chat UI\n",
      "content===> LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using create_agent . This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith ).\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Manage checkpoints\n",
      "content===> You can view and delete the information stored by the checkpointer.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/add-memory\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Chat models\n",
      "content===> Use the ChatGoogleGenerativeAI class to interact with Gemini models. See\n",
      "details in this guide .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Review and edit state\n",
      "content===> Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> UpstashRedisByteStore\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/stores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Key-value stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Import path\n",
      "content===> The import path for the agent prebuilt has changed from langgraph.prebuilt to langchain.agents .\n",
      "The name of the function has changed from create_react_agent to create_agent :\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Getting help\n",
      "content===> Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post .\n",
      "\n",
      "You’re now ready to contribute high-quality code to LangChain!\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Parallelization\n",
      "content===> With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n",
      "\n",
      "Some examples include:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM Steps\n",
      "content===> Use when you need to understand, analyze, generate text, or make reasoning decisions\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Error handling strategies\n",
      "content===> You can customize how errors are handled using the handle_errors parameter:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text property\n",
      "content===> Existing usage patterns (i.e., .text() ) will continue to function but now emit a warning. The method form will be removed in v2.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI\n",
      "content===> Azure AI Foundry provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the AzureAIChatCompletionsModel class.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Running tests locally\n",
      "content===> Before submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Retry Logic\n",
      "content===> Implement retry logic with exponential backoff for failed operations\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model fallback\n",
      "content===> First fallback model to try when the primary model fails. Can be a model string (e.g., \"openai:gpt-4o-mini\" ) or a BaseChatModel instance.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Starting Points for Resuming Workflows\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/durable-execution\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Durable execution\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Configuration file\n",
      "content===> The langgraph.json file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/application-structure\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Application structure\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Database for PostgreSQL\n",
      "content===> Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It’s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n",
      "\n",
      "See set up instructions for Azure Database for PostgreSQL.\n",
      "\n",
      "Simply use the connection string from your Azure Portal.\n",
      "\n",
      "Since Azure Database for PostgreSQL is open-source Postgres, you can use the LangChain’s Postgres support to connect to Azure Database for PostgreSQL.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Search\n",
      "content===> The following table shows tools that execute online searches in some shape or form:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> Optionally, define a structured response format if you need the agent responses to match\n",
      "a specific schema.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Implementing our email agent nodes\n",
      "content===> We’ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Custom tool description\n",
      "content===> Override the auto-generated tool description for clearer model guidance:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use classes when\n",
      "content===> • Multiple hooks needed • Complex configuration • Reuse across projects (config on init)\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Co-locate Python and JavaScript/TypeScript content\n",
      "content===> All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Base URL or proxy\n",
      "content===> Proxy support varies by integration. Check the specific model provider’s reference for proxy configuration options.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Model-based guardrails\n",
      "content===> Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\n",
      "\n",
      "LangChain provides both built-in guardrails (e.g., PII detection , human-in-the-loop ) and a flexible middleware system for building custom guardrails using either approach.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/guardrails\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Guardrails\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Advanced schema definition\n",
      "content===> Define complex inputs with Pydantic models or JSON schemas:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Orchestrator-worker\n",
      "content===> In an orchestrator-worker configuration, the orchestrator:\n",
      "\n",
      "Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization . This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Reads\n",
      "content===> Most real-world tools need more than just the LLM’s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\n",
      "\n",
      "Read from State to check current session information:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Documentation types\n",
      "content===> Where applicable, all documentation must have translations in both Python and JavaScript/TypeScript. See our localization guide for details.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool Message\n",
      "content===> For models that support tool calling , AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n",
      "\n",
      "Tools can generate ToolMessage objects directly. Below, we show a simple example. Read more in the tools guide .\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Vectara\n",
      "content===> Neural search platform with built-in understanding.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/all_providers\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> All providers\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interface\n",
      "content===> This abstraction lets you switch between different implementations without altering your application logic.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/vectorstores\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Vector stores\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Testing and validation\n",
      "content===> Before submitting documentation:\n",
      "\n",
      "Run all code examples to ensure they work correctly\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Observability\n",
      "content===> Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use LangSmith to visualize these execution steps. To use it, enable tracing for your application . This enables you to do the following:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/observability\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Observability\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Retrieval\n",
      "content===> Large language models (LLMs) are powerful, but they have two key limitations:\n",
      "\n",
      "Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of Retrieval-Augmented Generation (RAG) : enhancing an LLM’s answers with context-specific information.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/retrieval\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Retrieval\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling strategy\n",
      "content===> For models that don’t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.\n",
      "\n",
      "To use this strategy, configure a ToolStrategy :\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Handle errors appropriately\n",
      "content===> Different errors need different handling strategies:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool call limit\n",
      "content===> Limit the number of tool calls to specific tools or all tools.\n",
      "\n",
      "Perfect for:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Agent Chat UI\n",
      "content===> Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking.\n",
      "\n",
      "Agent Chat UI is open source and can be adapted to your application needs.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool error handling\n",
      "content===> The agent will return a ToolMessage with the custom error message when a tool fails:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Streaming\n",
      "content===> Stream tokens, tool calls, and reasoning traces in real-time\n",
      "side_link===> https://docs.langchain.com/oss/python/releases/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Release notes\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Workflows and agents\n",
      "content===> This guide reviews common workflow and agent patterns.\n",
      "\n",
      "LangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> Set up your conftest.py file to filter out sensitive information from the cassettes:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Memory\n",
      "content===> Defining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupts\n",
      "content===> Unlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic —they can be placed anywhere in your code and can be conditional based on your application logic.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Defining state viastate_schema\n",
      "content===> As of langchain 1.0 , custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/agents\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Collection\n",
      "content===> Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.\n",
      "\n",
      "Regardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses , which often leads to more personalized and relevant interactions.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Cosmos DB for MongoDB (vCore)\n",
      "content===> Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.\n",
      "\n",
      "With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.\n",
      "\n",
      "Sign Up for free to get started today.\n",
      "\n",
      "See a usage example .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Inside tools\n",
      "content===> You can access the runtime information inside tools to:\n",
      "\n",
      "Use the ToolRuntime parameter to access the Runtime object inside a tool.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Bedrock (Knowledge Bases)\n",
      "content===> Knowledge bases for Amazon Bedrock is an Amazon Web Services ( AWS ) offering which lets you quickly build RAG applications by using your\n",
      "private data to customize foundation model response.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. View your agent in Studio\n",
      "content===> Safari blocks localhost connections to Studio. To work around this, run the above command with --tunnel to access Studio via a secure tunnel.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/studio\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Studio\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Message prompts\n",
      "content===> Alternatively, you can pass in a list of messages to the model by providing a list of message objects.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Getting started\n",
      "content===> Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.\n",
      "\n",
      "The below example shows how this works with a simple, linear graph that progresses through node1 and node2 . Each node updates the single state key my_key :\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/test\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> YouTube Transcripts Loader\n",
      "content===> Load video transcripts. Requires youtube-transcript-api .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Migrate tocreate_agent\n",
      "content===> The table below outlines what functionality has changed from create_react_agent to create_agent :\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ScaNN (Local Index)\n",
      "content===> ScaNN includes search space pruning and quantization for Maximum Inner\n",
      "Product Search and also supports other distance functions such as\n",
      "Euclidean distance. The implementation is optimized for x86 processors\n",
      "with AVX2 support. See its Google Research github for more details.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool calling strategy\n",
      "content===> The schema defining the structured output format. Supports:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> CompiledSubAgent\n",
      "content===> For complex workflows, use a pre-built LangGraph graph:\n",
      "\n",
      "Fields:\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Conceptual guides\n",
      "content===> Explanations that provide deeper understanding and insights\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/documentation\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Documentation\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 4. Resume execution from the checkpoint\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/use-time-travel\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Time travel\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Serializer\n",
      "content===> When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\n",
      "\n",
      "langgraph_checkpoint defines protocol for implementing serializers provides a default implementation ( JsonPlusSerializer ) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/persistence\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Persistence\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content we’re excited to promote\n",
      "content===> Blogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as “here’s how to use integration XYZ”, but rather “here’s how to do ABC”, as we find that is more educational and helpful for developers.\n",
      "\n",
      "End-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use LangGraph as the orchestration framework. We get particularly excited about anything involving:\n",
      "\n",
      "We love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/comarketing\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Co-marketing\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure AI Services individual tools\n",
      "content===> The azure_ai_services toolkit includes the tools that queries the Azure Cognitive Services :\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Write clear descriptions\n",
      "content===> The main agent uses descriptions to decide which subagent to call. Be specific:\n",
      "\n",
      "✅ Good: \"Analyzes financial data and generates investment insights with confidence scores\"\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/subagents\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Subagents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Build a real-world agent\n",
      "content===> To learn how to trace your agent with LangSmith, see the LangSmith documentation .\n",
      "\n",
      "Congratulations! You now have an AI agent that can:\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/quickstart\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Blob Storage\n",
      "content===> Azure Blob Storage is Microsoft’s object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn’t adhere to a particular data model or definition, such as text or binary data.\n",
      "\n",
      "Azure Blob Storage is designed for:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> When processing a large number of inputs using batch() or batch_as_completed() , you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> PII detection\n",
      "content===> Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n",
      "\n",
      "Check user messages before model call\n",
      "\n",
      "Check AI messages after model call\n",
      "\n",
      "Check tool result messages after execution\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/middleware\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Middleware\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Routing\n",
      "content===> Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/workflows-agents\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Workflows + agents\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Post-model hook\n",
      "content===> Post-model hooks are now implemented as middleware with the after_model method.\n",
      "This new pattern is more extensible—you can define multiple middlewares to run after the model is called,\n",
      "reusing common patterns across different agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> AlloyDB for PostgreSQL\n",
      "content===> Google Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.\n",
      "\n",
      "Install the python package:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Recording & Replaying HTTP Calls\n",
      "content===> Then configure your project to recognise the vcr marker:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Features\n",
      "content===> Studio automatically renders tool calls and results in an intuitive interface.\n",
      "\n",
      "Navigate through conversation history and fork from any point\n",
      "\n",
      "View and modify agent state at any point during execution\n",
      "\n",
      "Built-in support for reviewing and responding to agent requests\n",
      "\n",
      "You can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph .\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/ui\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Advanced considerations\n",
      "content===> This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.\n",
      "\n",
      "You might wonder: why not combine Read Email and Classify Intent into one node?\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Azure Cosmos DB\n",
      "content===> AI agents can rely on Azure Cosmos DB as a unified memory system solution, enjoying speed, scale, and simplicity. This service successfully enabled OpenAI’s ChatGPT service to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world’s first globally distributed NoSQL , relational , and vector database service that offers a serverless mode.\n",
      "\n",
      "Below are two available Azure Cosmos DB APIs that can provide vector store functionalities.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/microsoft\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Microsoft\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Error handling strategies\n",
      "content===> If handle_errors is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Response Format\n",
      "content===> When a schema type is provided directly, LangChain automatically chooses:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/structured-output\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Structured output\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Overview\n",
      "content===> LangGraph exposes a Runtime object with the following information:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/runtime\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Runtime\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. Build and compile the agent\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Trajectory Match Evaluator\n",
      "content===> The superset and subset modes match partial trajectories. The superset mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The subset mode ensures the agent did not call any tools beyond those in the reference.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/test\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Test\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> SageMaker Endpoint\n",
      "content===> Amazon SageMaker is a system that can build, train, and deploy\n",
      "machine learning (ML) models with fully managed infrastructure, tools, and workflows.\n",
      "\n",
      "We use SageMaker to host our model and expose it as the SageMaker Endpoint .\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Use MCP tools\n",
      "content===> MultiServerMCPClient is stateless by default . Each tool invocation creates a fresh MCP ClientSession , executes the tool, and then cleans up.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/mcp\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Model Context Protocol (MCP)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Text-to-Speech\n",
      "content===> Google Cloud Text-to-Speech is a Google Cloud service that enables developers to\n",
      "synthesize natural-sounding speech with 100+ voices, available in multiple languages and variants.\n",
      "It applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks\n",
      "to deliver the highest fidelity possible.\n",
      "\n",
      "Install required packages:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Response Format\n",
      "content===> Structured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn’t sufficient.\n",
      "\n",
      "How it works: When you provide a schema as the response format, the model’s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/context-engineering\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Context engineering\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> ToolRuntime\n",
      "content===> The tool_runtime parameter is hidden from the model. For the example above, the model only sees pref_name in the tool schema - tool_runtime is not included in the request.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/tools\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Tools\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Self-improving instructions\n",
      "content===> Over time, the instructions file accumulates user preferences, helping the agent improve.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/long-term-memory\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Long-term memory\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Backwards compatibility\n",
      "content===> Maintain stable public interfaces and avoid breaking changes\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Multimodal\n",
      "content===> Not all models support all file types. Check the model provider’s reference for supported formats and size limits.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Content block reference\n",
      "content===> Name of the tool that failed to be called\n",
      "\n",
      "Arguments to pass to the tool\n",
      "\n",
      "Description of what went wrong\n",
      "\n",
      "Purpose: Tool call that is executed server-side.\n",
      "\n",
      "Always \"server_tool_call\"\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/messages\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Messages\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Batch\n",
      "content===> This section describes a chat model method batch() , which parallelizes model calls client-side.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/models\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Models\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Security guidelines\n",
      "content===> Security is paramount. Never introduce vulnerabilities or unsafe patterns.\n",
      "\n",
      "Security checklist:\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Tool and provider strategies\n",
      "content===> In v1, there are two new structured output strategies:\n",
      "side_link===> https://docs.langchain.com/oss/python/migrate/langchain-v1\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Migration guide\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Amazon Comprehend Moderation Chain\n",
      "content===> Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/aws\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> AWS (Amazon)\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Manual formatting and linting\n",
      "content===> Both commands will show you any formatting or linting issues that need to be addressed before committing.\n",
      "side_link===> https://docs.langchain.com/oss/python/contributing/code\n",
      "head_menu_name===> Contribute\n",
      "side_menu_name===> Code\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Productivity\n",
      "content===> The following table shows tools that can be used to automate tasks in productivity tools:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/tools\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Tools and toolkits\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Step 4: Build your nodes\n",
      "content===> Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Thinking in LangGraph\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Using LangGraph Studio\n",
      "content===> You can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> OllamaEmbeddings\n",
      "content===> Ollama embedding models.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/ollama\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Ollama\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Quick start\n",
      "content===> The fastest way to get started is using the hosted version:\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/ui\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Agent Chat UI\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Static runtime context\n",
      "content===> Static runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the context argument to invoke / stream . This data does not change during execution.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/context\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Context\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> 6. Build and compile the agent\n",
      "content===> The agent is built using the StateGraph class and compiled using the compile method.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/quickstart\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Quickstart\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> When to use deep agents\n",
      "content===> Use deep agents when you need agents that can:\n",
      "\n",
      "For simpler use cases, consider using LangChain’s create_agent or building a custom LangGraph workflow.\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/overview\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Overview\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Interrupts in tools\n",
      "content===> You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it’s called, and allows for human review and editing of the tool call before it is executed.\n",
      "\n",
      "First, define a tool that uses interrupt :\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/interrupts\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Interrupts\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Supported stream modes\n",
      "content===> Pass one or more of the following stream modes as a list to the stream or astream methods:\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Stream subgraph outputs\n",
      "content===> Note that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> BigQuery\n",
      "content===> Google Cloud BigQuery is a serverless data warehouse.\n",
      "\n",
      "Install with BigQuery dependencies:\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/providers/google\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Google\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Subagent interrupts\n",
      "content===> When a subagent triggers an interrupt, the handling is the same – check for __interrupt__ and resume with Command .\n",
      "side_link===> https://docs.langchain.com/oss/python/deepagents/human-in-the-loop\n",
      "head_menu_name===> Deep Agents\n",
      "side_menu_name===> Human-in-the-loop\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> History\n",
      "content===> LangChain releases 1.0 with two major changes:\n",
      "\n",
      "Complete revamp of all chains and agents in langchain . All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.\n",
      "side_link===> https://docs.langchain.com/oss/python/langchain/philosophy\n",
      "head_menu_name===> LangChain\n",
      "side_menu_name===> Philosophy\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Messaging Services\n",
      "content===> The below document loaders allow you to load data from different messaging platforms.\n",
      "side_link===> https://docs.langchain.com/oss/python/integrations/document_loaders\n",
      "head_menu_name===> Integrations\n",
      "side_menu_name===> Document loaders\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> LLM tokens\n",
      "content===> Manual config required for async in Python < 3.11 When using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.\n",
      "side_link===> https://docs.langchain.com/oss/python/langgraph/streaming\n",
      "head_menu_name===> LangGraph\n",
      "side_menu_name===> Streaming\n",
      "----------------------------------\n",
      "type===> text\n",
      "title===> Long-term memory\n",
      "content===> Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped , long-term memory is saved within custom “namespaces.”\n",
      "\n",
      "Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:\n",
      "\n",
      "Different applications require various types of memory. Although the analogy isn’t perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper ) have even mapped these human memory types to those used in AI agents.\n",
      "side_link===> https://docs.langchain.com/oss/python/concepts/memory\n",
      "head_menu_name===> Learn\n",
      "side_menu_name===> Memory\n",
      "----------------------------------\n",
      " text 의 갯수: 1294\n"
     ]
    }
   ],
   "source": [
    "# text 블록만 출력\n",
    "count = 0\n",
    "for i in unique_data:\n",
    "    if i['type'] == 'text':\n",
    "        print(f\"type===> {i['type']}\")\n",
    "        print(f\"title===> {i['title']}\")\n",
    "        print(f\"content===> {i['content']}\")\n",
    "        print(f\"side_link===> {i['side_link']}\")\n",
    "        print(f\"head_menu_name===> {i['head_menu_name']}\")\n",
    "        print(f\"side_menu_name===> {i['side_menu_name']}\")\n",
    "        count += 1\n",
    "        print(\"----------------------------------\")\n",
    "#print(f\"총 코드 블록 개수: {count}\")\n",
    "print(f\" text 의 갯수: {count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21584a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 json 형태로 저장\n",
    "with open('unique_meta.json', 'w', encoding= 'utf-8') as f:\n",
    "    json.dump(unique_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcdb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2366개의 데이터가 로드되었습니다.\n",
      "\n",
      "--- 로드된 데이터의 첫 번째 항목 ---\n",
      "{'title': 'File structure', 'type': 'text', 'content': 'Below are examples of directory structures for applications:', 'side_link': 'https://docs.langchain.com/oss/python/langgraph/application-structure', 'head_menu_name': 'LangGraph', 'side_menu_name': 'Application structure'}\n"
     ]
    }
   ],
   "source": [
    "#데이터 불러오기\n",
    "with open('unique_meta.json', 'r', encoding='utf-8') as f:\n",
    "    # json.load() 함수를 사용하여 파일의 내용을 파이썬 객체로 불러옵니다.\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "# 데이터가 잘 로드되었는지 확인합니다.\n",
    "print(f\"총 {len(loaded_data)}개의 데이터가 로드되었습니다.\")\n",
    "print(\"\\n--- 로드된 데이터의 첫 번째 항목 ---\")\n",
    "print(loaded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd2b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiboot_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
