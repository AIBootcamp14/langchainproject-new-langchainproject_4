"""
Vector DB ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
LangChain ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ í›„ Vector DBì— ì ì¬í•©ë‹ˆë‹¤.
"""

import os
import argparse
from typing import List, Optional, Dict, Any
from pathlib import Path
from datetime import datetime

# ì¨ë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter # ì„ì‹œ ìŠ¤í”Œë¦¬í„°

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ (ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ src.ìœ¼ë¡œ ì‹œì‘)
from src.modules.vector_database import VectorDatabaseClient
# from src.data_collector import DataCollector # ğŸ‘ˆ íŒ€ì› 4ì˜ DataCollector í´ë˜ìŠ¤ë¡œ ëŒ€ì²´ í•„ìš”
# from src.utils.chunking_strategy import StructuredTextSplitter # ğŸ‘ˆ íŒ€ì› 4ì˜ ì²­í‚¹ ì „ëµìœ¼ë¡œ ëŒ€ì²´ í•„ìš”


# ì„ì‹œ DataCollector í´ë˜ìŠ¤ (íŒ€ì› 4 íŒŒì¼ ì˜¬ ë•Œê¹Œì§€ ì‚¬ìš©)
class DummyDataCollector:
    """íŒ€ì› 4ì˜ DataCollectorê°€ ì˜¤ê¸° ì „ê¹Œì§€ ì‚¬ìš©í•˜ëŠ” ë”ë¯¸ í´ë˜ìŠ¤"""
    def collect_documents(self, urls: List[str], max_pages: int, delay: float) -> List[Document]:
        print("âš ï¸ ë”ë¯¸ DataCollector ì‚¬ìš©: ì‹¤ì œ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        # ë”ë¯¸ ë¬¸ì„œ ë°˜í™˜
        return [
            Document(page_content=f"ë”ë¯¸ ë¬¸ì„œ {i}: ì´ ë¬¸ì„œëŠ” {url}ì—ì„œ ì™”ìŠµë‹ˆë‹¤.", metadata={"source": url, "title": f"Doc {i}"})
            for i, url in enumerate(urls[:max_pages])
        ]

# ì„ì‹œ TextSplitter í´ë˜ìŠ¤ (íŒ€ì› 4 íŒŒì¼ ì˜¬ ë•Œê¹Œì§€ ì‚¬ìš©)
class DummyTextSplitter(RecursiveCharacterTextSplitter):
    """íŒ€ì› 4ì˜ StructuredTextSplitterê°€ ì˜¤ê¸° ì „ê¹Œì§€ ì‚¬ìš©í•˜ëŠ” ë”ë¯¸ í´ë˜ìŠ¤"""
    def split_documents(self, documents: List[Document]) -> List[Document]:
        print("âš ï¸ ë”ë¯¸ TextSplitter ì‚¬ìš©: RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return super().split_documents(documents)

# LangChain ë¬¸ì„œ URL ëª©ë¡ (í™•ì¥ ê°€ëŠ¥)
LANGCHAIN_URLS: List[str] = [
    # í•µì‹¬ ê°œë…
    "https://python.langchain.com/docs/introduction",
    "https://python.langchain.com/docs/get_started/quickstart",
    # ì£¼ìš” ëª¨ë“ˆ
    "https://python.langchain.com/docs/modules/model_io",
    "https://python.langchain.com/docs/modules/data_connection",
    "https://python.langchain.com/docs/modules/chains",
    "https://python.langchain.com/docs/modules/agents",
    "https://python.langchain.com/docs/modules/memory",
]


class VectorDBInitializer:
    """Vector DB ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤"""

    def __init__(
        self,
        docker_host: str = "localhost",
        docker_port: int = 8000,
        collection_name: str = "langchain_docs",
        embedding_model: str = "solar-embedding-1-large",
    ) -> None:
        """
        ì´ˆê¸°í™” (Docker ChromaDB ì „ìš©ìœ¼ë¡œ ë‹¨ìˆœí™”)
        """
        print(f"ğŸ³ Docker ChromaDB ì‚¬ìš©: {docker_host}:{docker_port}")
        
        # Vector DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í†µí•©ëœ VectorDatabaseClient ì‚¬ìš©)
        self.vector_db = VectorDatabaseClient(
            host=docker_host,
            port=docker_port,
            collection_name=collection_name,
            embedding_model=embedding_model,
        )

        # ë°ì´í„° ìˆ˜ì§‘ê¸° (ë”ë¯¸ ì‚¬ìš©)
        self.collector = DummyDataCollector() # ğŸ‘ˆ ë‚˜ì¤‘ì— DataCollectorë¡œ êµì²´

        # êµ¬ì¡° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸° (ë”ë¯¸ ì‚¬ìš©)
        # âš ï¸ íŒ€ì› 4ì˜ StructuredTextSplitterë¡œ êµì²´ ì˜ˆì •
        self.text_splitter = DummyTextSplitter(
            chunk_size=1500,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""],
        )

    def collect_documents(
        self,
        urls: Optional[List[str]] = None,
        max_pages: int = 50,
    ) -> List[Document]:
        """ë¬¸ì„œ ìˆ˜ì§‘"""
        print("\nğŸ“¥ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")

        if urls is None:
            urls = LANGCHAIN_URLS

        documents = self.collector.collect_documents(
            urls=urls[:max_pages],
            max_pages=max_pages,
            delay=1.0,  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        )

        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        return documents

    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """êµ¬ì¡° ê¸°ë°˜ ë¬¸ì„œ ì²­í‚¹"""
        print("\nâœ‚ï¸ êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ ì‹œì‘...")

        # âš ï¸ íŒ€ì› 4ì˜ ë¡œì§ì„ ì‚¬ìš©í•  ê²½ìš° í†µê³„ ê³„ì‚° í•„ìš”
        chunked_docs = self.text_splitter.split_documents(documents)

        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: ì´ ì²­í¬ {len(chunked_docs)}ê°œ")
        return chunked_docs

    def load_to_vector_db(
        self,
        documents: List[Document],
        batch_size: int = 100,
        reset: bool = False,
    ) -> int:
        """ë¬¸ì„œë¥¼ Vector DBì— ì ì¬"""
        print("\nğŸ’¾ Vector DB ì ì¬ ì‹œì‘...")

        # Docker ChromaDB í—¬ìŠ¤ì²´í¬
        if not self.vector_db.health_check():
            raise ConnectionError(
                "ChromaDB ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "Docker Compose ì‹¤í–‰: docker-compose up -d chromadb"
            )

        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vector_db.init_vectorstore(reset=reset)

        # ë¬¸ì„œ ì¶”ê°€
        ids = self.vector_db.add_documents(
            documents=documents,
            batch_size=batch_size,
            show_progress=True,
        )

        print(f"âœ… {len(ids)}ê°œ ë¬¸ì„œ Vector DB ì ì¬ ì™„ë£Œ")
        return len(ids)

    def run_full_pipeline(
        self,
        urls: Optional[List[str]] = None,
        max_pages: int = 30,
        reset_db: bool = False,
    ) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = datetime.now()
        print("=" * 60)
        print("ğŸš€ Vector DB ì´ˆê¸°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)

        try:
            # 1. ë¬¸ì„œ ìˆ˜ì§‘
            documents = self.collect_documents(urls, max_pages)

            # 2. êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹
            chunked_docs = self.chunk_documents(documents)

            # 3. Vector DB ì ì¬
            loaded_count = self.load_to_vector_db(
                chunked_docs,
                batch_size=100,
                reset=reset_db,
            )

            # 4. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
            print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰...")
            test_query = "LangChainì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•"
            results = self.vector_db.search(test_query, k=1) # 1ê°œë§Œ í…ŒìŠ¤íŠ¸
            print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ")

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = (datetime.now() - start_time).total_seconds()

            # ê²°ê³¼ ë°˜í™˜
            result = {
                "status": "success",
                "documents_collected": len(documents),
                "chunks_created": len(chunked_docs),
                "documents_loaded": loaded_count,
                "execution_time": f"{execution_time:.2f}ì´ˆ",
                "vector_db_type": "Docker ChromaDB",
            }

            print("\n" + "=" * 60)
            print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print("=" * 60)
            print(f"ì´ ì‹¤í–‰ ì‹œê°„: {result['execution_time']}")

            return result

        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"status": "failed", "error": str(e)}


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Vector DB ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”©"
    )

    # Docker ì„¤ì •ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ê³ ì • (í”„ë¡œì íŠ¸ ê¸°ì¤€)
    parser.add_argument(
        "--host",
        type=str,
        default=os.getenv("CHROMA_HOST", "localhost"), # ENV ë³€ìˆ˜ ìš°ì„  ì‚¬ìš©
        help="Docker ChromaDB í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸: CHROMA_HOST í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” localhost)"
    )

    parser.add_argument(
        "--collection",
        type=str,
        default="langchain_docs",
        help="ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸: langchain_docs)"
    )

    parser.add_argument(
        "--embedding",
        type=str,
        default="solar-embedding-1-large",
        # ì„ íƒì§€ ì •ë¦¬
        choices=[
            "solar-embedding-1-large",
            "solar-embedding-1-large-query",
            "solar-embedding-1-large-passage",
            "ko-sbert-multitask"
        ],
        help="ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸: solar-embedding-1-large)"
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        default=len(LANGCHAIN_URLS), # ê¸°ë³¸ê°’: ì „ì²´ URL ìˆ˜
        help="ìµœëŒ€ ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: ì „ì²´)"
    )

    parser.add_argument(
        "--reset",
        action="store_true",
        help="ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ìƒì„±"
    )

    parser.add_argument(
        "--test-only",
        action="store_true",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¼ë¶€ ë¬¸ì„œë§Œ)"
    )

    args = parser.parse_args()

    # ì´ˆê¸°í™”
    initializer = VectorDBInitializer(
        docker_host=args.host,
        collection_name=args.collection,
        embedding_model=args.embedding
    )

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test_only:
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì¼ë¶€ ë¬¸ì„œë§Œ ì²˜ë¦¬")
        urls = LANGCHAIN_URLS[:3]
        max_pages = 3
    else:
        urls = LANGCHAIN_URLS
        max_pages = args.max_pages

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    initializer.run_full_pipeline(
        urls=urls,
        max_pages=max_pages,
        reset_db=args.reset
    )


if __name__ == "__main__":
    main()