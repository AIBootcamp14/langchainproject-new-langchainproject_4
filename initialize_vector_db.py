# initialize_vector_db.py

"""
Vector DB ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢…)
LangChain ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ í›„ ChromaDB Vector DBì— ì ì¬í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
import time


# ğŸŒŸ ê²½ë¡œ ì¶”ê°€ ë° ë””ë²„ê¹… ì½”ë“œ ì‹œì‘ ğŸŒŸ
current_dir = str(Path(__file__).parent)
sys.path.append(current_dir) # <--- ì´ ë¶€ë¶„ì´ ê²½ë¡œë¥¼ ì¶”ê°€í•˜ëŠ” í•µì‹¬ì´ì•¼.

print("="*30)
print(f"í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ: {current_dir}")

is_src_exist = Path(current_dir, "src").is_dir()
print(f"í´ë” ì•ˆì— 'src' í´ë” ì¡´ì¬ ì—¬ë¶€: {is_src_exist}")

print("sys.pathì— ì¶”ê°€ëœ ê²½ë¡œë“¤:")
for p in sys.path:
    if "lang" in p: # í”„ë¡œì íŠ¸ í´ë” ì´ë¦„ìœ¼ë¡œ í•„í„°ë§
        print(f"  -> {p}")
print("="*30)
# ğŸŒŸ ê²½ë¡œ ì¶”ê°€ ë° ë””ë²„ê¹… ì½”ë“œ ë ğŸŒŸ

from dotenv import load_dotenv
from typing import List

from langchain_core.documents import Document

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
# íŒ€ì› 4ì™€ íŒ€ì› 1ì´ ì™„ì„±í•œ ì‹¤ì œ ëª¨ë“ˆ ì‚¬ìš©
from src.utils.data_collector import DataCollector
from src.utils.chunking_strategy import StructuredTextSplitter
from src.modules.vector_database import VectorDatabaseClient

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë° DB ì„¤ì •)
load_dotenv() 

# --- ì„¤ì • ê°’ ---
# í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
COLLECTION_NAME: str = os.getenv("CHROMA_COLLECTION", "langchain_docs")
MAX_PAGES_TO_CRAWL: int = 10 # ì „ì²´ ë¬¸ì„œë¥¼ ì ì¬í•˜ê¸° ì „ì— í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œë¡œ ì œí•œ
CRAWL_DELAY_SECONDS: float = 1.0
RESET_DB: bool = True # DBë¥¼ ìƒˆë¡œ ë§Œë“¤ì§€ ì—¬ë¶€ (í…ŒìŠ¤íŠ¸ ì‹œ True ê¶Œì¥)


def run_data_ingestion() -> None:
    """ì „ì²´ ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    start_time = time.time()
    
    print("=" * 60)
    print(f"ğŸš€ RAG ë°ì´í„° ì ì¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì»¬ë ‰ì…˜: {COLLECTION_NAME})")
    print("=" * 60)
    
    # 1. ë¬¸ì„œ ìˆ˜ì§‘ (DataCollector ì‚¬ìš©)
    print("\n[ë‹¨ê³„ 1/4] LangChain ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
    collector = DataCollector()
    
    # DataCollector ë‚´ë¶€ì˜ get_sample_urls ì‚¬ìš©
    urls_to_crawl = collector.get_sample_urls() 
    
    raw_documents: List[Document] = collector.collect_documents(
        urls=urls_to_crawl,
        max_pages=MAX_PAGES_TO_CRAWL,
        delay=CRAWL_DELAY_SECONDS,
    )
    
    if not raw_documents:
        print("ğŸ›‘ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ URL ë˜ëŠ” ì›¹ ì„œë²„ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”.")
        return
        
    print(f"âœ… ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(raw_documents)}ê°œ ë¬¸ì„œ")
    
    
    # 2. êµ¬ì¡° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í•  (StructuredTextSplitter ì‚¬ìš©)
    print("\n[ë‹¨ê³„ 2/4] êµ¬ì¡° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í•  ë° ì²­í‚¹ ì‹œì‘...")
    # íŒ€ì› 1ì˜ ê³ ê¸‰ ë¶„í• ê¸° ì‚¬ìš© (ì½”ë“œ ë¸”ë¡ ë³´ì¡´)
    text_splitter = StructuredTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        preserve_code_blocks=True,
    )
    
    chunks: List[Document] = text_splitter.split_documents(raw_documents)
    
    if not chunks:
        print("ğŸ›‘ ë¶„í• ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„í•  ë¡œì§ì„ ì ê²€í•˜ì„¸ìš”.")
        return
        
    print(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")


    # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° í™•ì¸
    print("\n[ë‹¨ê³„ 3/4] ChromaDB ì—°ê²° ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”...")
    vdb_client = VectorDatabaseClient(collection_name=COLLECTION_NAME)
    
    if not vdb_client.health_check():
        print("ğŸ›‘ ChromaDB ì„œë²„ ì—°ê²° ì‹¤íŒ¨. Docker Composeê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # reset ì„¤ì •ì— ë”°ë¼ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì´ˆê¸°í™”
    vectorstore = vdb_client.init_vectorstore(reset=RESET_DB)
    print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ (ì»¬ë ‰ì…˜: {COLLECTION_NAME})")


    # 4. ë²¡í„° ì €ì¥ì†Œì— ì²­í¬ ì ì¬
    print(f"\n[ë‹¨ê³„ 4/4] {len(chunks)}ê°œ ì²­í¬ë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì ì¬ ì‹œì‘...")
    
    try:
        # ChromaDBì— Document ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì¶”ê°€ (ìë™ìœ¼ë¡œ ì„ë² ë”© ë° ì €ì¥ ìˆ˜í–‰)
        vectorstore.add_documents(documents=chunks)
        
        # ì ì¬ í›„ ìµœì¢… ë¬¸ì„œ ìˆ˜ í™•ì¸
        final_count = vectorstore._collection.count()
        print(f"ğŸ‰ ëª¨ë“  ì²­í¬ ì ì¬ ì™„ë£Œ! (ì´ {final_count}ê°œ ì²­í¬)")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì ì¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    execution_time = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    print("=" * 60)


if __name__ == "__main__":
    run_data_ingestion()