"""
ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸.
(ChromaDB ì—°ê²° í¬íŠ¸ ë° í™˜ê²½ ë³€ìˆ˜ ìˆ˜ì •)
"""

import os
import argparse 
import time
from typing import List, Dict, Any, Optional, Final

# ì¨ë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
from langchain_community.vectorstores import Chroma

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.utils.data_collector import DataCollector
from src.utils.utils import ensure_directory, generate_document_hash
from src.utils.chunking_strategy import CodeBlockPreservingSplitter
from src.modules.vector_database import VectorDatabaseClient # ì´ ëª¨ë“ˆì„ ìˆ˜ì •í•´ì•¼ í•¨
from src.modules.llm import get_embeddings


# --- ì„¤ì • ë° ìƒìˆ˜ ---
SOURCE_DATA_DIR: Final[str] = "data/source_documents"
EMBEDDING_MODEL_NAME: Final[str] = "solar-embedding-1-large"
COLLECTION_NAME: Final[str] = "langchain_docs"

# --------------------


def initialize_db(
    documents: List[Document], 
    reset_db: bool = False
) -> None:
    """
    ìˆ˜ì§‘ëœ ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬í•œë‹¤.
    """
    
    print("=" * 60)
    print(f"2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ì ì¬ ì‹œì‘ (Reset: {reset_db})")
    print("=" * 60)
    
    # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •]: ë°ì´í„° ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ChromaDB ì—°ê²° ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œ
    # Docker-composeì˜ 8001:8000 ë§¤í•‘ì„ ì‚¬ìš© (FastAPI 8000ê³¼ ì¶©ëŒ ë°©ì§€)
    os.environ["CHROMA_HOST"] = "localhost"
    os.environ["CHROMA_PORT"] = "8001"
    
    # 1. DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²°
    vdb_client: VectorDatabaseClient = VectorDatabaseClient(
        collection_name=COLLECTION_NAME,
        embedding_model=EMBEDDING_MODEL_NAME
    )
    
    # ChromaDB ì—°ê²° í™•ì¸
    if not vdb_client.health_check():
        raise ConnectionError("ChromaDB ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

    # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (reset ì¸ì ì „ë‹¬)
    vectorstore: Chroma = vdb_client.init_vectorstore(reset=reset_db)

    if not documents:
        print("ê²½ê³ : ì ì¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DB ì´ˆê¸°í™”ë§Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # 2. ë¬¸ì„œ ë¶„í•  (Chunking)
    print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¶„í•  ì‹œì‘ (Custom Splitter ì‚¬ìš©)")

    # CodeBlockPreservingSplitter ì‚¬ìš©
    text_splitter: RecursiveCharacterTextSplitter = CodeBlockPreservingSplitter(
        chunk_size=1500, 
        chunk_overlap=200, 
    )

    chunks: List[Document] = text_splitter.split_documents(documents)
    print(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ. ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨.")

    # 3. ë¬¸ì„œ í•´ì‹œ ìƒì„± ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
    for chunk in chunks:
        # ì²­í¬ ë ˆë²¨ì—ì„œ ê³ ìœ  í•´ì‹œ ìƒì„±
        chunk.metadata["chunk_hash"] = generate_document_hash(
            chunk.page_content, 
            chunk.metadata.get("url")
        )

    # 4. ë²¡í„° DBì— ì²­í¬ ì ì¬
    print(f"ì´ {len(chunks)}ê°œ ì²­í¬ë¥¼ ë²¡í„° DBì— ì ì¬ ì¤‘...")
    
    start_time = time.time()
    ids: List[str] = vectorstore.add_documents(chunks)
    end_time = time.time()
    
    print(f"âœ… ì ì¬ ì™„ë£Œ! (ì´ {len(ids)}ê°œ ë¬¸ì„œ ì ì¬, ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")


def parse_arguments() -> argparse.Namespace:
    """ëª…ë ¹ì¤„ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤."""
    parser = argparse.ArgumentParser(
        description="LangChain ë¬¸ì„œë¥¼ í¬ë¡¤ë§í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(ChromaDB)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--reset",
        action="store_true", 
        help="ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=100,
        help="í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ ìš©). 0 ë˜ëŠ” Noneì´ë©´ ì „ì²´ í¬ë¡¤ë§."
    )
    
    return parser.parse_args()


def main():
    """ìŠ¤í¬ë¦½íŠ¸ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    load_dotenv()
    
    args: argparse.Namespace = parse_arguments()
    reset_db: bool = args.reset
    max_pages: Optional[int] = args.max_pages if args.max_pages > 0 else None 
    
    print("=" * 60)
    print("1. ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print(f"  - DB ì´ˆê¸°í™” ì—¬ë¶€ (--reset): {reset_db}")
    print(f"  - ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (--max-pages): {max_pages if max_pages is not None else 'ì „ì²´'}")
    print("=" * 60)
    
    try:
        collector: DataCollector = DataCollector()
        documents: List[Document] = collector.collect_documents(
            max_pages=max_pages, 
            delay=0.5
        )
        
        if not documents:
            print("ê²½ê³ : ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ì–´ ì ì¬ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        initialize_db(documents=documents, reset_db=reset_db)

    except ConnectionError as e:
        print(f"\nâŒ [ì¹˜ëª…ì  ì˜¤ë¥˜ - ì—°ê²° ì‹¤íŒ¨]: {e}")
        print("ChromaDB ì„œë²„(Docker ì»¨í…Œì´ë„ˆ)ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except ValueError as e:
        print(f"\nâŒ [ì¹˜ëª…ì  ì˜¤ë¥˜ - ì„¤ì • ì‹¤íŒ¨]: {e}")
    except Exception as e:
        print(f"\nâŒ [ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜]: {e.__class__.__name__} - {e}")
    finally:
        print("\n=== ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ì ì¬ ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ===")


if __name__ == "__main__":
    main()