import os
from dotenv import load_dotenv
from openai import OpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from jinja2 import Template

# ------------------------------------------------------
# 1ï¸âƒ£ í™˜ê²½ ì„¤ì •
# ------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHROMA_PATH = "./chroma_langchain_docs"

client = OpenAI(api_key=OPENAI_API_KEY)

# ------------------------------------------------------
# 2ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
# ------------------------------------------------------
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ LangChain ê³µì‹ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ AI ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´, ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µí•©ë‹ˆë‹¤.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ê³µì‹ ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•©ë‹ˆë‹¤.
3. ê´€ë ¨ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ê°„ë‹¨íˆ í‘œì‹œí•©ë‹ˆë‹¤.
4. ê°€ëŠ¥í•œ ê²½ìš° LangChain ë¬¸ì„œ ìŠ¤íƒ€ì¼ì˜ ì½”ë“œ ì˜ˆì‹œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
"""

USER_TEMPLATE = Template("""
ì§ˆë¬¸: {{ question }}

[ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½]
{% for doc in docs %}
ì¶œì²˜: {{ doc.metadata.get("source", "unknown") }}
ë‚´ìš© ìš”ì•½:
{{ doc.page_content[:800] }}
---
{% endfor %}

ìœ„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
""")

# ------------------------------------------------------
# 3ï¸âƒ£ Retriever í•¨ìˆ˜ ì •ì˜
# ------------------------------------------------------
def get_retriever():
    """ê¸°ì¡´ì— êµ¬ì¶•ëœ Chroma DBì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ ë°˜í™˜"""
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)
    return vectordb.as_retriever(search_kwargs={"k": 5})

# ------------------------------------------------------
# 4ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ë¹Œë”
# ------------------------------------------------------
def build_prompt(question: str, docs: list):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡(docs)ì„ í…œí”Œë¦¿ì— ì‚½ì…í•´ LLMì— ì „ë‹¬í•  ë©”ì‹œì§€ í¬ë§· ìƒì„±
    """
    user_prompt = USER_TEMPLATE.render(question=question, docs=docs)
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT.strip()},
        {"role": "user", "content": user_prompt.strip()},
    ]
    return messages

# ------------------------------------------------------
# 5ï¸âƒ£ LLM í˜¸ì¶œ í•¨ìˆ˜
# ------------------------------------------------------
def call_llm(messages):
    """OpenAI GPT í˜¸ì¶œ"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,
        max_tokens=700,
    )
    return response.choices[0].message.content.strip()

# ------------------------------------------------------
# 6ï¸âƒ£ RAG í†µí•© íŒŒì´í”„ë¼ì¸
# ------------------------------------------------------
def rag_answer(question: str):
    """
    ì „ì²´ RAG íë¦„:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ë°›ìŒ
    2. ê²€ìƒ‰ê¸°ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    4. LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
    """
    retriever = get_retriever()
    docs = retriever.get_relevant_documents(question)
    messages = build_prompt(question, docs)
    answer = call_llm(messages)
    return answer

# ------------------------------------------------------
# 7ï¸âƒ£ ì‹¤í–‰ ì˜ˆì‹œ
# ------------------------------------------------------
if __name__ == "__main__":
    print("ğŸ” LangChain ë¬¸ì„œ ê¸°ë°˜ RAG ì±—ë´‡\n")
    while True:
        query = input("ì§ˆë¬¸: ")
        if query.lower() in ["exit", "quit"]:
            break
        print("\nğŸ§  ë‹µë³€:")
        print(rag_answer(query))
        print("-" * 80)
