[
    {
        "title": "LangChain overview",
        "type": "text",
        "content": "LangChain v1.0 is now available!\n\nFor a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\n\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\n\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage."
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "pip install -U langchain\n"
    },
    {
        "title": "​Create an agent",
        "type": "code",
        "content": "# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n"
    },
    {
        "title": "Standard model interface",
        "type": "text",
        "content": "Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in."
    },
    {
        "title": "Easy to use, highly flexible agent",
        "type": "text",
        "content": "LangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires."
    },
    {
        "title": "Built on top of LangGraph",
        "type": "text",
        "content": "LangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more."
    },
    {
        "title": "Debug with LangSmith",
        "type": "text",
        "content": "Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "LangChain overview",
        "type": "text",
        "content": "LangChain v1.0 is now available!\n\nFor a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\n\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\n\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage."
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "pip install -U langchain\n"
    },
    {
        "title": "​Create an agent",
        "type": "code",
        "content": "# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n"
    },
    {
        "title": "Standard model interface",
        "type": "text",
        "content": "Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in."
    },
    {
        "title": "Easy to use, highly flexible agent",
        "type": "text",
        "content": "LangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires."
    },
    {
        "title": "Built on top of LangGraph",
        "type": "text",
        "content": "LangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more."
    },
    {
        "title": "Debug with LangSmith",
        "type": "text",
        "content": "Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "What's new in v1",
        "type": "text",
        "content": "LangChain v1 is a focused, production-ready foundation for building agents. We’ve streamlined the framework around three core improvements:"
    },
    {
        "title": "create_agent",
        "type": "text",
        "content": "The new standard for building agents in LangChain, replacing langgraph.prebuilt.create_react_agent ."
    },
    {
        "title": "create_agent",
        "type": "code",
        "content": "langgraph.prebuilt.create_react_agent"
    },
    {
        "title": "Standard content blocks",
        "type": "text",
        "content": "A new content_blocks property that provides unified access to modern LLM features across providers."
    },
    {
        "title": "Simplified namespace",
        "type": "text",
        "content": "The langchain namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic ."
    },
    {
        "title": "Simplified namespace",
        "type": "code",
        "content": "pip install -U langchain\n"
    },
    {
        "title": "Simplified namespace",
        "type": "text",
        "content": "For a complete list of changes, see the migration guide ."
    },
    {
        "title": "​create_agent",
        "type": "text",
        "content": "create_agent is the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent while offering greater customization potential by using middleware ."
    },
    {
        "title": "​create_agent",
        "type": "code",
        "content": "langgraph.prebuilt.create_react_agent"
    },
    {
        "title": "​create_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[search_web, analyze_data, send_email],\n    system_prompt=\"You are a helpful research assistant.\"\n)\n\nresult = agent.invoke({\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n    ]\n})\n"
    },
    {
        "title": "​create_agent",
        "type": "text",
        "content": "Under the hood, create_agent is built on the basic agent loop — calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:"
    },
    {
        "title": "​create_agent",
        "type": "text",
        "content": "For more information, see Agents .\n\nMiddleware is the defining feature of create_agent . It offers a highly customizable entry-point, raising the ceiling for what you can build."
    },
    {
        "title": "​create_agent",
        "type": "text",
        "content": "Great agents require context engineering : getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n\nLangChain provides a few prebuilt middlewares for common patterns, including:"
    },
    {
        "title": "​create_agent",
        "type": "code",
        "content": "SummarizationMiddleware"
    },
    {
        "title": "​create_agent",
        "type": "code",
        "content": "HumanInTheLoopMiddleware"
    },
    {
        "title": "​create_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    PIIMiddleware,\n    SummarizationMiddleware,\n    HumanInTheLoopMiddleware\n)\n\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[read_email, send_email],\n    middleware=[\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\n            \"phone_number\",\n            detector=(\n                r\"(?:\\+?\\d{1,3}[\\s.-]?)?\"\n                r\"(?:\\(?\\d{2,4}\\)?[\\s.-]?)?\"\n                r\"\\d{3,4}[\\s.-]?\\d{4}\"\n\t\t\t),\n\t\t\tstrategy=\"block\"\n        ),\n        SummarizationMiddleware(\n            model=\"claude-sonnet-4-5-20250929\",\n            max_tokens_before_summary=500\n        ),\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"send_email\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]\n                }\n            }\n        ),\n    ]\n)\n"
    },
    {
        "title": "​create_agent",
        "type": "text",
        "content": "You can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent’s execution:\n\nBuild custom middleware by implementing any of these hooks on a subclass of the AgentMiddleware class:"
    },
    {
        "title": "​create_agent",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom typing import Callable\n\nfrom langchain_openai import ChatOpenAI\n\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    ModelRequest\n)\nfrom langchain.agents.middleware.types import ModelResponse\n\n@dataclass\nclass Context:\n    user_expertise: str = \"beginner\"\n\nclass ExpertiseBasedToolMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        user_level = request.runtime.context.user_expertise\n\n        if user_level == \"expert\":\n            # More powerful model\n            model = ChatOpenAI(model=\"gpt-5\")\n            tools = [advanced_search, data_analysis]\n        else:\n            # Less powerful model\n            model = ChatOpenAI(model=\"gpt-5-nano\")\n            tools = [simple_search, basic_calculator]\n\n        request.model = model\n        request.tools = tools\n        return handler(request)\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[\n        simple_search,\n        advanced_search,\n        basic_calculator,\n        data_analysis\n    ],\n    middleware=[ExpertiseBasedToolMiddleware()],\n    context_schema=Context\n)\n"
    },
    {
        "title": "​create_agent",
        "type": "text",
        "content": "For more information, see the complete middleware guide .\n\nBecause create_agent is built on LangGraph , you automatically get built in support for long running and reliable agents via:"
    },
    {
        "title": "Persistence",
        "type": "text",
        "content": "Conversations automatically persist across sessions with built-in checkpointing"
    },
    {
        "title": "Streaming",
        "type": "text",
        "content": "Stream tokens, tool calls, and reasoning traces in real-time"
    },
    {
        "title": "Human-in-the-loop",
        "type": "text",
        "content": "Pause agent execution for human approval before sensitive actions"
    },
    {
        "title": "Time travel",
        "type": "text",
        "content": "Rewind conversations to any point and explore alternate paths and prompts\n\nYou don’t need to learn LangGraph to use these features—they work out of the box.\n\ncreate_agent has improved structured output generation:"
    },
    {
        "title": "Time travel",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    temperature: float\n    condition: str\n\ndef weather_tool(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"it's sunny and 70 degrees in {city}\"\n\nagent = create_agent(\n    \"gpt-4o-mini\",\n    tools=[weather_tool],\n    response_format=ToolStrategy(Weather)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n})\n\nprint(repr(result[\"structured_response\"]))\n# results in `Weather(temperature=70.0, condition='sunny')`\n"
    },
    {
        "title": "Time travel",
        "type": "text",
        "content": "Error handling : Control error handling via the handle_errors parameter to ToolStrategy :"
    },
    {
        "title": "​Standard content blocks",
        "type": "text",
        "content": "Content block support is currently only available for the following integrations:"
    },
    {
        "title": "​Standard content blocks",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "​Standard content blocks",
        "type": "text",
        "content": "Broader support for content blocks will be rolled out gradually across more providers.\n\nThe new content_blocks property introduces a standard representation for message content that works across providers:"
    },
    {
        "title": "​Standard content blocks",
        "type": "code",
        "content": "from langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\nresponse = model.invoke(\"What's the capital of France?\")\n\n# Unified access to content blocks\nfor block in response.content_blocks:\n    if block[\"type\"] == \"reasoning\":\n        print(f\"Model reasoning: {block['reasoning']}\")\n    elif block[\"type\"] == \"text\":\n        print(f\"Response: {block['text']}\")\n    elif block[\"type\"] == \"tool_call\":\n        print(f\"Tool call: {block['name']}({block['args']})\")\n"
    },
    {
        "title": "​Standard content blocks",
        "type": "text",
        "content": "For more information, see our guide on content blocks ."
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "LangChain v1 streamlines the langchain package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "langchain.chat_models"
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "Most of these are re-exported from langchain-core for convenience, which gives you a focused API surface for building agents."
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "# Agent building\nfrom langchain.agents import create_agent\n\n# Messages and content\nfrom langchain.messages import AIMessage, HumanMessage\n\n# Tools\nfrom langchain.tools import tool\n\n# Model initialization\nfrom langchain.chat_models import init_chat_model\nfrom langchain.embeddings import init_embeddings\n"
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "Legacy functionality has moved to langchain-classic to keep the core packages lean and focused."
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "If you use any of this functionality, install langchain-classic :"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "pip install langchain-classic\n"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "from langchain import ...\nfrom langchain_classic import ...\n\nfrom langchain.chains import ...\nfrom langchain_classic.chains import ...\n\nfrom langchain.retrievers import ...\nfrom langchain_classic.retrievers import ...\n\nfrom langchain import hub  \nfrom langchain_classic import hub  \n"
    },
    {
        "title": "​Migration guide",
        "type": "text",
        "content": "See our migration guide for help updating your code to LangChain v1."
    },
    {
        "title": "​Reporting issues",
        "type": "text",
        "content": "Please report any issues discovered with 1.0 on GitHub using the 'v1' label ."
    },
    {
        "title": "​See also",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "LangChain v1 migration guide",
        "type": "text",
        "content": "This guide outlines the major changes between LangChain v1 and previous versions."
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "The langchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality."
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "langchain.chat_models"
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "If you were using any of the following from the langchain package, you’ll need to install langchain-classic and update your imports:"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "CacheBackedEmbeddings"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "# Chains\nfrom langchain_classic.chains import LLMChain\n\n# Retrievers\nfrom langchain_classic.retrievers import ...\n\n# Indexing\nfrom langchain_classic.indexes import ...\n\n# Hub\nfrom langchain_classic import hub\n"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "pip install langchain-classic\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Prior to v1.0, we recommended using langgraph.prebuilt.create_react_agent to build agents. Now, we recommend you use langchain.agents.create_agent to build agents."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "langgraph.prebuilt.create_react_agent"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "langchain.agents.create_agent"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "The table below outlines what functionality has changed from create_react_agent to create_agent :"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "config[\"configurable\"]"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "The import path for the agent prebuilt has changed from langgraph.prebuilt to langchain.agents .\nThe name of the function has changed from create_react_agent to create_agent :"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langgraph.prebuilt import create_react_agent \nfrom langchain.agents import create_agent \n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "For more information, see Agents .\n\nThe prompt parameter has been renamed to system_prompt :"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather],\n    system_prompt=\"You are a helpful assistant\"\n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "If using SystemMessage objects in the system prompt, extract the string content:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather],\n    system_prompt=\"You are a helpful assistant\"\n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Dynamic prompts are a core context engineering pattern— they adapt what you tell the model based on the current conversation state. To do this, use the @dynamic_prompt decorator:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from dataclasses import dataclass\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:  \n    user_role: str = \"user\"\n\n@dynamic_prompt\ndef dynamic_prompt(request: ModelRequest) -> str:  \n    user_role = request.runtime.context.user_role\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        prompt = (\n            f\"{base_prompt} Provide detailed technical responses.\"\n        )\n    elif user_role == \"beginner\":\n        prompt = (\n            f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n        )\n    else:\n        prompt = base_prompt\n\n    return prompt  \n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=tools,\n    middleware=[dynamic_prompt],  \n    context_schema=Context\n)\n\n# Use with context\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n    context=Context(user_role=\"expert\")\n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Pre-model hooks are now implemented as middleware with the before_model method.\nThis new pattern is more extensible—you can define multiple middlewares to run before the model is called,\nreusing common patterns across different agents."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Common use cases include:\n\nv1 now has summarization middleware as a built in option:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=tools,\n    middleware=[\n        SummarizationMiddleware(  \n            model=\"claude-sonnet-4-5-20250929\",  \n            max_tokens_before_summary=1000\n        )  \n    ]  \n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Post-model hooks are now implemented as middleware with the after_model method.\nThis new pattern is more extensible—you can define multiple middlewares to run after the model is called,\nreusing common patterns across different agents."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Common use cases include:\n\nv1 has a built in middleware for human in the loop approval for tool calls:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[read_email, send_email],\n    middleware=[HumanInTheLoopMiddleware(\n        interrupt_on={\n            \"send_email\": True,\n            \"description\": \"Please review this email before sending\"\n        },\n    )]\n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Custom state extends the default agent state with additional fields. You can define custom state in two ways:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Defining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "state_schema is still supported for backwards compatibility on create_agent ."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Use the state_schema parameter when your custom state needs to be accessed by tools:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent, AgentState  \n\n\n# Define custom state extending AgentState\nclass CustomState(AgentState):\n    user_name: str\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomState]\n) -> str:\n    \"\"\"Use this to greet the user by name.\"\"\"\n    user_name = runtime.state.get(\"user_name\", \"Unknown\")  \n    return f\"Hello {user_name}!\"\n\nagent = create_agent(  \n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[greet],\n    state_schema=CustomState  \n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Middleware can also define custom state by setting the state_schema attribute.\nThis helps to keep state extensions conceptually scoped to the relevant middleware and tools."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState  \n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[...],\n    middleware=[CallCounterMiddleware()]  \n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "See the middleware documentation for more details on defining custom state via middleware.\n\ncreate_agent only supports TypedDict for state schemas. Pydantic models and dataclasses are no longer supported."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import AgentState, create_agent\n\n# AgentState is a TypedDict\nclass CustomAgentState(AgentState):  \n    user_id: str\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=tools,\n    state_schema=CustomAgentState  \n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Simply inherit from langchain.agents.AgentState instead of BaseModel or decorating with dataclass .\nIf you need to perform validation, handle it in middleware hooks instead."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "langchain.agents.AgentState"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). create_react_agent released in v0.6 of langgraph-prebuilt supported dynamic model and tool selection via a callable passed to the model parameter."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "This functionality has been ported to the middleware interface in v1."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    AgentMiddleware, ModelRequest, ModelRequestHandler\n)\nfrom langchain.messages import AIMessage\nfrom langchain_openai import ChatOpenAI\n\n\nbasic_model = ChatOpenAI(model=\"gpt-5-nano\")\nadvanced_model = ChatOpenAI(model=\"gpt-5\")\n\nclass DynamicModelMiddleware(AgentMiddleware):\n\n    def wrap_model_call(self, request: ModelRequest, handler: ModelRequestHandler) -> AIMessage:\n        if len(request.state.messages) > self.messages_threshold:\n            model = advanced_model\n        else:\n            model = basic_model\n\n        return handler(request.replace(model=model))\n\n    def __init__(self, messages_threshold: int) -> None:\n        self.messages_threshold = messages_threshold\n\nagent = create_agent(\n    model=basic_model,\n    tools=tools,\n    middleware=[DynamicModelMiddleware(messages_threshold=10)]\n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "To better support structured output, create_agent no longer accepts pre-bound models with tools or configuration:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "# No longer supported\nmodel_with_tools = ChatOpenAI().bind_tools([some_tool])\nagent = create_agent(model_with_tools, tools=[])\n\n# Use instead\nagent = create_agent(\"gpt-4o-mini\", tools=[some_tool])\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Dynamic model functions can return pre-bound models if structured output is not used.\n\nThe tools argument to create_agent accepts a list of:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "The argument will no longer accept ToolNode instances."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather, search_web]\n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "You can now configure the handling of tool errors with middleware implementing the wrap_tool_call method."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "# Example coming soon\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Structured output used to be generated in a separate node from the main agent. This is no longer the case.\nWe generate structured output in the main loop, reducing cost and latency.\n\nIn v1, there are two new structured output strategies:"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy, ProviderStrategy\nfrom pydantic import BaseModel\n\n\nclass OutputSchema(BaseModel):\n    summary: str\n    sentiment: str\n\n# Using ToolStrategy\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=tools,\n    # explicitly using tool strategy\n    response_format=ToolStrategy(OutputSchema)  \n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "Prompted output is no longer supported via the response_format argument. Compared to strategies\nlike artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "When streaming events from agents, the node name has changed from \"agent\" to \"model\" to better reflect the node’s purpose."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "When you invoke an agent, it’s often the case that you want to pass two types of data:\n\nIn v1, static context is supported by setting the context parameter to invoke and stream ."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "from dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_id: str\n    session_id: str\n\nagent = create_agent(\n    model=model,\n    tools=tools,\n    context_schema=Context  \n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n    context=Context(user_id=\"123\", session_id=\"abc\")  \n)\n"
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "text",
        "content": "The old config[\"configurable\"] pattern still works for backward compatibility, but using the new context parameter is recommended for new applications or applications migrating to v1."
    },
    {
        "title": "​Migrate tocreate_agent",
        "type": "code",
        "content": "config[\"configurable\"]"
    },
    {
        "title": "​Standard content",
        "type": "text",
        "content": "In v1, messages gain provider-agnostic standard content blocks. Access them via @[ message.content_blocks ][content_blocks] for a consistent, typed view across providers. The existing message.content field remains unchanged for strings or provider-native structures."
    },
    {
        "title": "​Standard content",
        "type": "code",
        "content": "message.content_blocks"
    },
    {
        "title": "​Standard content",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\nresponse = model.invoke(\"Explain AI\")\n\nfor block in response.content_blocks:\n    if block[\"type\"] == \"reasoning\":\n        print(block.get(\"reasoning\"))\n    elif block[\"type\"] == \"text\":\n        print(block.get(\"text\"))\n"
    },
    {
        "title": "​Standard content",
        "type": "code",
        "content": "from langchain.messages import HumanMessage\n\nmessage = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\nres = model.invoke([message])\n"
    },
    {
        "title": "​Standard content",
        "type": "code",
        "content": "# Text block\ntext_block = {\n    \"type\": \"text\",\n    \"text\": \"Hello world\",\n}\n\n# Image block\nimage_block = {\n    \"type\": \"image\",\n    \"url\": \"https://example.com/image.png\",\n    \"mime_type\": \"image/png\",\n}\n"
    },
    {
        "title": "​Standard content",
        "type": "text",
        "content": "See the content blocks reference for more details.\n\nStandard content blocks are not serialized into the content attribute by default. If you need to access standard content blocks in the content attribute (e.g., when sending messages to a client), you can opt-in to serializing them into content ."
    },
    {
        "title": "​Standard content",
        "type": "code",
        "content": "export LC_OUTPUT_VERSION=v1\n"
    },
    {
        "title": "​Standard content",
        "type": "text",
        "content": "Learn more: Messages , Standard content blocks , and Multimodal ."
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "The langchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality."
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "langchain.chat_models"
    },
    {
        "title": "​Simplified package",
        "type": "text",
        "content": "If you were using any of the following from the langchain package, you’ll need to install langchain-classic and update your imports:"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "CacheBackedEmbeddings"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "# Chains\nfrom langchain_classic.chains import LLMChain\n\n# Retrievers\nfrom langchain_classic.retrievers import ...\n\n# Indexing\nfrom langchain_classic.indexes import ...\n\n# Hub\nfrom langchain_classic import hub\n"
    },
    {
        "title": "​Simplified package",
        "type": "code",
        "content": "uv pip install langchain-classic\n"
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "All LangChain packages now require Python 3.10 or higher . Python 3.9 reaches end of life in October 2025.\n\nThe return type signature for chat model invocation has been fixed from BaseMessage to AIMessage . Custom chat models implementing bind_tools should update their return signature:"
    },
    {
        "title": "​Breaking changes",
        "type": "code",
        "content": "def bind_tools(\n        ...\n    ) -> Runnable[LanguageModelInput, AIMessage]:\n"
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "When interacting with the Responses API, langchain-openai now defaults to storing response items in message content . To restore previous behavior, set the LC_OUTPUT_VERSION environment variable to v0 , or specify output_version=\"v0\" when instantiating ChatOpenAI ."
    },
    {
        "title": "​Breaking changes",
        "type": "code",
        "content": "# Enforce previous behavior with output_version flag\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", output_version=\"v0\")\n"
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "The max_tokens parameter in langchain-anthropic now defaults to higher values based on the model chosen, rather than the previous default of 1024 . If you relied on the old default, explicitly set max_tokens=1024 ."
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "Existing functionality outside the focus of standard interfaces and agents has been moved to the langchain-classic package. See the Simplified namespace section for details on what’s available in the core langchain package and what moved to langchain-classic ."
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the deprecation notices from previous versions for replacement APIs.\n\nUse of the .text() method on message objects should drop the parentheses, as it is now a property:"
    },
    {
        "title": "​Breaking changes",
        "type": "code",
        "content": "# Property access\ntext = response.text\n\n# Deprecated method call\ntext = response.text()\n"
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "Existing usage patterns (i.e., .text() ) will continue to function but now emit a warning. The method form will be removed in v2."
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "The example parameter has been removed from AIMessage objects. We recommend migrating to use additional_kwargs for passing extra metadata as needed."
    },
    {
        "title": "​Minor changes",
        "type": "code",
        "content": "LanguageModelOutputVar"
    },
    {
        "title": "​Minor changes",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Install LangChain",
        "type": "code",
        "content": "pip install -U langchain\n"
    },
    {
        "title": "Install LangChain",
        "type": "text",
        "content": "LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:"
    },
    {
        "title": "Install LangChain",
        "type": "code",
        "content": "# Installing the OpenAI integration\npip install -U langchain-openai\n\n# Installing the Anthropic integration\npip install -U langchain-anthropic\n"
    },
    {
        "title": "Install LangChain",
        "type": "text",
        "content": "See the Integrations tab for a full list of available integrations.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Quickstart",
        "type": "text",
        "content": "This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes."
    },
    {
        "title": "​Build a basic agent",
        "type": "text",
        "content": "Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\nFor this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal."
    },
    {
        "title": "​Build a basic agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n"
    },
    {
        "title": "​Build a basic agent",
        "type": "text",
        "content": "To learn how to trace your agent with LangSmith, see the LangSmith documentation ."
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "Next, build a practical weather forecasting agent that demonstrates key production concepts:\n\nLet’s walk through each step:\n\nThe system prompt defines your agent’s role and behavior. Keep it specific and actionable:"
    },
    {
        "title": "​Build a real-world agent",
        "type": "code",
        "content": "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n"
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "Tools let a model interact with external systems by calling functions you define.\nTools can depend on runtime context and also interact with agent memory .\n\nNotice below how the get_user_location tool uses runtime context:"
    },
    {
        "title": "​Build a real-world agent",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather_for_location(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\n@dataclass\nclass Context:\n    \"\"\"Custom runtime context schema.\"\"\"\n    user_id: str\n\n@tool\ndef get_user_location(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    user_id = runtime.context.user_id\n    return \"Florida\" if user_id == \"1\" else \"SF\"\n"
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "Tools should be well-documented: their name, description, and argument names become part of the model’s prompt.\nLangChain’s @tool decorator adds metadata and enables runtime injection via the ToolRuntime parameter."
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "Set up your language model with the right parameters for your use case:"
    },
    {
        "title": "​Build a real-world agent",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    temperature=0.5,\n    timeout=10,\n    max_tokens=1000\n)\n"
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "Optionally, define a structured response format if you need the agent responses to match\na specific schema."
    },
    {
        "title": "​Build a real-world agent",
        "type": "code",
        "content": "from dataclasses import dataclass\n\n# We use a dataclass here, but Pydantic models are also supported.\n@dataclass\nclass ResponseFormat:\n    \"\"\"Response schema for the agent.\"\"\"\n    # A punny response (always required)\n    punny_response: str\n    # Any interesting information about the weather if available\n    weather_conditions: str | None = None\n"
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "Add memory to your agent to maintain state across interactions. This allows\nthe agent to remember previous conversations and context."
    },
    {
        "title": "​Build a real-world agent",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n"
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "In production, use a persistent checkpointer that saves to a database.\nSee Add and manage memory for more details.\n\nNow assemble your agent with all the components and run it!"
    },
    {
        "title": "​Build a real-world agent",
        "type": "code",
        "content": "agent = create_agent(\n    model=model,\n    system_prompt=SYSTEM_PROMPT,\n    tools=[get_user_location, get_weather_for_location],\n    context_schema=Context,\n    response_format=ResponseFormat,\n    checkpointer=checkpointer\n)\n\n# `thread_id` is a unique identifier for a given conversation.\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nprint(response['structured_response'])\n# ResponseFormat(\n#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n#     weather_conditions=\"It's always sunny in Florida!\"\n# )\n\n\n# Note that we can continue the conversation using the same `thread_id`.\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nprint(response['structured_response'])\n# ResponseFormat(\n#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n#     weather_conditions=None\n# )\n"
    },
    {
        "title": "​Build a real-world agent",
        "type": "text",
        "content": "To learn how to trace your agent with LangSmith, see the LangSmith documentation .\n\nCongratulations! You now have an AI agent that can:\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Philosophy",
        "type": "text",
        "content": "LangChain is driven by a few core beliefs:\n\nWith LangChain, we have two core focuses:\n\nDifferent providers expose different APIs, with different model parameters and different message formats.\nStandardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.\n\nModels should be used for more than just text generation - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define tools that LLMs can use dynamically, as well as help with parsing of and access to unstructured data."
    },
    {
        "title": "​History",
        "type": "text",
        "content": "LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.\n\nAs developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.\n\nModel APIs become more multimodal.\n\nModels started to accept files, images, videos, and more. We updated the langchain-core message format accordingly to allow developers to specify these multimodal inputs in a standard way."
    },
    {
        "title": "​History",
        "type": "text",
        "content": "LangChain releases 1.0 with two major changes:\n\nComplete revamp of all chains and agents in langchain . All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain."
    },
    {
        "title": "​History",
        "type": "text",
        "content": "For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the langchain-classic package."
    },
    {
        "title": "​History",
        "type": "text",
        "content": "A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Agents",
        "type": "text",
        "content": "Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\ncreate_agent provides a production-ready agent implementation."
    },
    {
        "title": "Agents",
        "type": "text",
        "content": "An LLM Agent runs tools in a loop to achieve a goal .\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\ncreate_agent builds a graph -based agent runtime using LangGraph . A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware."
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "The model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a model identifier string :"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=tools\n)\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "Model identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\" ). Refer to the reference to see a full list of model identifier string mappings."
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "For more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI . See Chat models for other available chat model classes."
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30\n    # ... (other params)\n)\nagent = create_agent(model, tools=tools)\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "Model instances give you complete control over configuration. Use them when you need to set specific parameters like temperature , max_tokens , timeouts , base_url , and other provider-specific settings. Refer to the reference to see available params and methods on your model."
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "from langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # Use an advanced model for longer conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    request.model = model\n    return handler(request)\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "Pre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound."
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "For model configuration details, see Models . For dynamic model selection patterns, see Dynamic model in middleware .\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\nFor more information, see Tools .\n\nPass a list of tools to the agent."
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "from langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain_core.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "The agent will return a ToolMessage with the custom error message when a tool fails:"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "[\n    ...\n    ToolMessage(\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\n        tool_call_id=\"...\"\n    ),\n    ...\n]\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "Agents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\nPrompt: Identify the current most popular wireless headphones and verify availability."
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "================================ Human Message =================================\n\nFind the most popular wireless headphones right now and check if they're in stock\n"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "search_products(\"wireless headphones\")"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "================================== Ai Message ==================================\nTool Calls:\n  search_products (call_abc123)\n Call ID: call_abc123\n  Args:\n    query: wireless headphones\n"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "================================= Tool Message =================================\n\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "check_inventory(\"WH-1000XM5\")"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "================================== Ai Message ==================================\nTool Calls:\n  check_inventory (call_def456)\n Call ID: call_def456\n  Args:\n    product_id: WH-1000XM5\n"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "================================= Tool Message =================================\n\nProduct WH-1000XM5: 10 units in stock\n"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "================================== Ai Message ==================================\n\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "To learn more about tools, see Tools .\n\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "agent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "When no system_prompt is provided, the agent will infer its task from the messages directly."
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware .\n\nThe @dynamic_prompt decorator creates middleware that generates system prompts dynamically based on the model request:"
    },
    {
        "title": "​Core components",
        "type": "code",
        "content": "from typing import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"Generate system prompt based on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# The system prompt will be set dynamically based on context\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n    context={\"user_role\": \"expert\"}\n)\n"
    },
    {
        "title": "​Core components",
        "type": "text",
        "content": "For more details on message types and formatting, see Messages . For comprehensive middleware documentation, see Middleware ."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "You can invoke an agent by passing an update to its State . All agents include a sequence of messages in their state; to invoke the agent, pass a new message:"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "result = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n)\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "For streaming steps and / or tokens from the agent, refer to the streaming guide.\n\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "ToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:"
    },
    {
        "title": "​Advanced concepts",
        "type": "code",
        "content": "from pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n"
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "ProviderStrategy uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):"
    },
    {
        "title": "​Advanced concepts",
        "type": "code",
        "content": "from langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n"
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "As of langchain 1.0 , simply passing a schema (e.g., response_format=ContactInfo ) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy ."
    },
    {
        "title": "​Advanced concepts",
        "type": "code",
        "content": "response_format=ContactInfo"
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "To learn about structured output, see Structured output .\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the short-term memory of the agent:\n\nCustom state schemas must extend AgentState as a TypedDict ."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "Defining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "state_schema is still supported for backwards compatibility on create_agent ."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware."
    },
    {
        "title": "​Advanced concepts",
        "type": "code",
        "content": "from langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n"
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "Use the state_schema parameter as a shortcut to define custom state that is only used in tools."
    },
    {
        "title": "​Advanced concepts",
        "type": "code",
        "content": "from langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n"
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "As of langchain 1.0 , custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "To learn more about memory, see Memory . For information on implementing long-term memory that persists across sessions, see Long-term memory .\n\nWe’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur."
    },
    {
        "title": "​Advanced concepts",
        "type": "code",
        "content": "for chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n}, stream_mode=\"values\"):\n    # Each chunk contains the full state at that point\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"Agent: {latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n"
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "For more details on streaming, see Streaming .\n\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\nMiddleware integrates seamlessly into the agent’s execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\nFor comprehensive middleware documentation including decorators like @before_model , @after_model , and @wrap_tool_call , see Middleware ."
    },
    {
        "title": "​Advanced concepts",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Models",
        "type": "text",
        "content": "LLMs are powerful AI tools that can interpret and generate text like humans. They’re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\n\nIn addition to text generation, many models support:\n\nModels are the reasoning engine of agents . They drive the agent’s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\n\nThe quality and capabilities of the model you choose directly impact your agent’s reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\n\nLangChain’s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your case.\n\nFor provider-specific integration information and capabilities, see the provider’s chat model page ."
    },
    {
        "title": "​Basic usage",
        "type": "text",
        "content": "Models can be utilized in two ways:\n\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\n\nThe easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):"
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "pip install -U \"langchain[openai]\"\n"
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n"
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "response = model.invoke(\"Why do parrots talk?\")\n"
    },
    {
        "title": "​Basic usage",
        "type": "text",
        "content": "See init_chat_model for more detail, including information on how to pass model parameters ."
    },
    {
        "title": "Invoke",
        "type": "text",
        "content": "The model takes messages as input and outputs messages after generating a complete response."
    },
    {
        "title": "Stream",
        "type": "text",
        "content": "Invoke the model, but stream the output as it is generated in real-time."
    },
    {
        "title": "Batch",
        "type": "text",
        "content": "Send multiple requests to a model in a batch for more efficient processing.\n\nIn addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details."
    },
    {
        "title": "​Parameters",
        "type": "text",
        "content": "A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n\nThe name or identifier of the specific model you want to use with a provider.\n\nThe key required for authenticating with the model’s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable .\n\nControls the randomness of the model’s output. A higher number makes responses more creative; lower ones make them more deterministic.\n\nThe maximum time (in seconds) to wait for a response from the model before canceling the request.\n\nLimits the total number of tokens in the response, effectively controlling how long the output can be.\n\nThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n\nUsing init_chat_model , pass these parameters as inline **kwargs :"
    },
    {
        "title": "​Parameters",
        "type": "code",
        "content": "model = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    # Kwargs passed to the model:\n    temperature=0.7,\n    timeout=30,\n    max_tokens=1000,\n)\n"
    },
    {
        "title": "​Parameters",
        "type": "text",
        "content": "Each chat model integration may have additional params used to control provider-specific functionality. For example, ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API."
    },
    {
        "title": "​Parameters",
        "type": "text",
        "content": "To find all the parameters supported by a given chat model, head to the chat model integrations page."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\n\nThe most straightforward way to call a model is to use invoke() with a single message or a list of messages."
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "response = model.invoke(\"Why do parrots have colorful feathers?\")\nprint(response)\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "A list of messages can be provided to a model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation. See the messages guide for more detail on roles, types, and content."
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nconversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore créer des applications.\")\n"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n\nconversation = [\n    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage(\"Translate: I love programming.\"),\n    AIMessage(\"J'adore la programmation.\"),\n    HumanMessage(\"Translate: I love building applications.\")\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore créer des applications.\")\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\n\nCalling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n    print(chunk.text, end=\"|\", flush=True)\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "As opposed to invoke() , which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "full = None  # None | AIMessageChunk\nfor chunk in model.stream(\"What color is the sky?\"):\n    full = chunk if full is None else full + chunk\n    print(full.text)\n\n# The\n# The sky\n# The sky is\n# The sky is typically\n# The sky is typically blue\n# ...\n\nprint(full.content_blocks)\n# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "The resulting message can be treated the same as a message that was generated with invoke() - for example, it can be aggregated into a message history and passed back to the model as conversational context."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn’t streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n\nLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\n\nIn LangGraph agents , for example, you can call model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "When you invoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking on_llm_new_token events in LangChain’s callback system."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "Callback events allow LangGraph stream() and astream_events() to surface the chat model’s output in real-time."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "LangChain chat models can also stream semantic events using astream_events() ."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example."
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "async for event in model.astream_events(\"Hello\"):\n\n    if event[\"event\"] == \"on_chat_model_start\":\n        print(f\"Input: {event['data']['input']}\")\n\n    elif event[\"event\"] == \"on_chat_model_stream\":\n        print(f\"Token: {event['data']['chunk'].text}\")\n\n    elif event[\"event\"] == \"on_chat_model_end\":\n        print(f\"Full message: {event['data']['output'].text}\")\n\n    else:\n        pass\n"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "Input: Hello\nToken: Hi\nToken:  there\nToken: !\nToken:  How\nToken:  can\nToken:  I\n...\nFull message: Hi there! How can I help today?\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "See the astream_events() reference for event types and other details."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "responses = model.batch([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n])\nfor response in responses:\n    print(response)\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "This section describes a chat model method batch() , which parallelizes model calls client-side."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "It is distinct from batch APIs supported by inference providers, such as OpenAI or Anthropic .\n\nBy default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed() :"
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "for response in model.batch_as_completed([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n]):\n    print(response)\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "When using batch_as_completed() , results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed."
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "When processing a large number of inputs using batch() or batch_as_completed() , you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary."
    },
    {
        "title": "​Invocation",
        "type": "code",
        "content": "model.batch(\n    list_of_inputs,\n    config={\n        'max_concurrency': 5,  # Limit to 5 parallel calls\n    }\n)\n"
    },
    {
        "title": "​Invocation",
        "type": "text",
        "content": "See the RunnableConfig reference for a full list of supported attributes."
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n\nYou may hear the term “function calling”. We use this interchangeably with “tool calling”.\n\nTo make tools that you have defined available for use by a model, you must bind them using bind_tools() . In subsequent invocations, the model can choose to call any of the bound tools as needed."
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI , ChatAnthropic ). Check the respective provider reference for details."
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "See the tools guide for details and other options for creating tools."
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "from langchain.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    return f\"It's sunny in {location}.\"\n\n\nmodel_with_tools = model.bind_tools([get_weather])  \n\nresponse = model_with_tools.invoke(\"What's the weather like in Boston?\")\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "When binding user-defined tools, the model’s response includes a request to execute a tool. When using a model separately from an agent , it is up to you to perform the requested action and return the result back to the model for use in subsequent reasoning. Note that when using an agent , the agent loop will handle the tool execution loop for you.\n\nBelow, we show some common ways you can use tool calling.\n\nWhen a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes agent abstractions that handle this orchestration for you.\n\nHere’s a simple example of how to do this:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "# Bind (potentially multiple) tools to the model\nmodel_with_tools = model.bind_tools([get_weather])\n\n# Step 1: Model generates tool calls\nmessages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\nai_msg = model_with_tools.invoke(messages)\nmessages.append(ai_msg)\n\n# Step 2: Execute tools and collect results\nfor tool_call in ai_msg.tool_calls:\n    # Execute the tool with the generated arguments\n    tool_result = get_weather.invoke(tool_call)\n    messages.append(tool_result)\n\n# Step 3: Pass results back to model for final response\nfinal_response = model_with_tools.invoke(messages)\nprint(final_response.text)\n# \"The current weather in Boston is 72°F and sunny.\"\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "Each ToolMessage returned by the tool includes a tool_call_id that matches the original tool call, helping the model correlate results with requests."
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "By default, the model has the freedom to choose which bound tool to use based on the user’s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or any tool from a given list:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously."
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "model_with_tools = model.bind_tools([get_weather])\n\nresponse = model_with_tools.invoke(\n    \"What's the weather in Boston and Tokyo?\"\n)\n\n\n# The model may generate multiple tool calls\nprint(response.tool_calls)\n# [\n#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n# ]\n\n\n# Execute all tools (can be done in parallel with async)\nresults = []\nfor tool_call in response.tool_calls:\n    if tool_call['name'] == 'get_weather':\n        result = get_weather.invoke(tool_call)\n    ...\n    results.append(result)\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.\n\nMost models supporting tool calling enable parallel tool calls by default. Some (including OpenAI and Anthropic ) allow you to disable this feature. To do this, set parallel_tool_calls=False :"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "parallel_tool_calls=False"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "model.bind_tools([get_weather], parallel_tool_calls=False)\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "When streaming responses, tool calls are progressively built through ToolCallChunk . This allows you to see tool calls as they’re being generated rather than waiting for the complete response."
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "for chunk in model_with_tools.stream(\n    \"What's the weather in Boston and Tokyo?\"\n):\n    # Tool call chunks arrive progressively\n    for tool_chunk in chunk.tool_call_chunks:\n        if name := tool_chunk.get(\"name\"):\n            print(f\"Tool: {name}\")\n        if id_ := tool_chunk.get(\"id\"):\n            print(f\"ID: {id_}\")\n        if args := tool_chunk.get(\"args\"):\n            print(f\"Args: {args}\")\n\n# Output:\n# Tool: get_weather\n# ID: call_SvMlU1TVIZugrFLckFE2ceRE\n# Args: {\"lo\n# Args: catio\n# Args: n\": \"B\n# Args: osto\n# Args: n\"}\n# Tool: get_weather\n# ID: call_QMZdy6qInx13oWKE7KhuhOLR\n# Args: {\"lo\n# Args: catio\n# Args: n\": \"T\n# Args: okyo\n# Args: \"}\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "You can accumulate chunks to build complete tool calls:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "gathered = None\nfor chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n    gathered = chunk if gathered is None else gathered + chunk\n    print(gathered.tool_calls)\n"
    },
    {
        "title": "​Structured outputs",
        "type": "text",
        "content": "Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured outputs.\n\nPydantic models provide the richest feature set with field validation, descriptions, and nested structures."
    },
    {
        "title": "​Structured outputs",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n    \"\"\"A movie with details.\"\"\"\n    title: str = Field(..., description=\"The title of the movie\")\n    year: int = Field(..., description=\"The year the movie was released\")\n    director: str = Field(..., description=\"The director of the movie\")\n    rating: float = Field(..., description=\"The movie's rating out of 10\")\n\nmodel_with_structure = model.with_structured_output(Movie)\nresponse = model_with_structure.invoke(\"Provide details about the movie Inception\")\nprint(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n"
    },
    {
        "title": "​Structured outputs",
        "type": "text",
        "content": "It can be useful to return the raw AIMessage object alongside the parsed representation to access response metadata such as token counts . To do this, set include_raw=True when calling with_structured_output :"
    },
    {
        "title": "​Structured outputs",
        "type": "code",
        "content": "with_structured_output"
    },
    {
        "title": "​Structured outputs",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n    \"\"\"A movie with details.\"\"\"\n    title: str = Field(..., description=\"The title of the movie\")\n    year: int = Field(..., description=\"The year the movie was released\")\n    director: str = Field(..., description=\"The director of the movie\")\n    rating: float = Field(..., description=\"The movie's rating out of 10\")\n\nmodel_with_structure = model.with_structured_output(Movie, include_raw=True)  \nresponse = model_with_structure.invoke(\"Provide details about the movie Inception\")\nresponse\n# {\n#     \"raw\": AIMessage(...),\n#     \"parsed\": Movie(title=..., year=..., ...),\n#     \"parsing_error\": None,\n# }\n"
    },
    {
        "title": "​Structured outputs",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\n\nclass Actor(BaseModel):\n    name: str\n    role: str\n\nclass MovieDetails(BaseModel):\n    title: str\n    year: int\n    cast: list[Actor]\n    genres: list[str]\n    budget: float | None = Field(None, description=\"Budget in millions USD\")\n\nmodel_with_structure = model.with_structured_output(MovieDetails)\n"
    },
    {
        "title": "​Supported models",
        "type": "text",
        "content": "LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page ."
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks .\n\nAll LangChain chat models with underlying multimodal capabilities support:\n\nSee the multimodal section of the messages guide for details.\n\nSome models can return multimodal data as part of their response. If invoked to do so, the resulting AIMessage will have content blocks with multimodal types."
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "response = model.invoke(\"Create a picture of a cat\")\nprint(response.content_blocks)\n# [\n#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n# ]\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "See the integrations page for details on specific providers.\n\nNewer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\n\nIf supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer."
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n    reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n    print(reasoning_steps if reasoning_steps else chunk.text)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical “tiers” of reasoning (e.g., 'low' or 'high' ) or integer token budgets."
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "For details, see the integrations page or reference for your respective chat model.\n\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\n\nOllama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page .\n\nMany providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit :"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "AnthropicPromptCachingMiddleware"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Prompt caching is often only engaged above a minimum input token threshold. See provider pages for details.\n\nCache usage will be reflected in the usage metadata of the model response.\n\nSome providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\n\nIf a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-4.1-mini\")\n\ntool = {\"type\": \"web_search\"}\nmodel_with_tools = model.bind_tools([tool])\n\nresponse = model_with_tools.invoke(\"What was a positive news story from today?\")\nresponse.content_blocks\n"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "[\n    {\n        \"type\": \"server_tool_call\",\n        \"name\": \"web_search\",\n        \"args\": {\n            \"query\": \"positive news stories today\",\n            \"type\": \"search\"\n        },\n        \"id\": \"ws_abc123\"\n    },\n    {\n        \"type\": \"server_tool_result\",\n        \"tool_call_id\": \"ws_abc123\",\n        \"status\": \"success\"\n    },\n    {\n        \"type\": \"text\",\n        \"text\": \"Here are some positive news stories from today...\",\n        \"annotations\": [\n            {\n                \"end_index\": 410,\n                \"start_index\": 337,\n                \"title\": \"article title\",\n                \"type\": \"citation\",\n                \"url\": \"...\"\n            }\n        ]\n    }\n]\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "This represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling .\n\nSee the integration page for your given provider for available tools and usage details.\n\nMany chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\nTo help manage rate limits, chat model integrations accept a rate_limiter parameter that can be provided during initialization to control the rate at which requests are made."
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "LangChain in comes with (an optional) built-in InMemoryRateLimiter . This limiter is thread safe and can be shared by multiple threads in the same process."
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "from langchain_core.rate_limiters import InMemoryRateLimiter\n\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=0.1,  # 1 request every 10s\n    check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n    max_bucket_size=10,  # Controls the maximum burst size.\n)\n\nmodel = init_chat_model(\n    model=\"gpt-5\",\n    model_provider=\"openai\",\n    rate_limiter=rate_limiter  \n)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n\nFor many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\n\nMany model providers offer OpenAI-compatible APIs (e.g., Together AI , vLLM ). You can use init_chat_model with these providers by specifying the appropriate base_url parameter:"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "model = init_chat_model(\n    model=\"MODEL_NAME\",\n    model_provider=\"openai\",\n    base_url=\"BASE_URL\",\n    api_key=\"YOUR_API_KEY\",\n)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective reference for details.\n\nFor deployments requiring HTTP proxies, some model integrations support proxy configuration:"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "from langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-4o\",\n    openai_proxy=\"http://proxy.example.com:8080\"\n)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Proxy support varies by integration. Check the specific model provider’s reference for proxy configuration options.\n\nCertain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the logprobs parameter when initializing the model:"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "model = init_chat_model(\n    model=\"gpt-4o\",\n    model_provider=\"openai\"\n).bind(logprobs=True)\n\nresponse = model.invoke(\"Why do parrots talk?\")\nprint(response.response_metadata[\"logprobs\"])\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the AIMessage objects produced by the corresponding model. For more details, see the messages guide."
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.\n\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\nfrom langchain_core.callbacks import UsageMetadataCallbackHandler\n\nmodel_1 = init_chat_model(model=\"gpt-4o-mini\")\nmodel_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\ncallback = UsageMetadataCallbackHandler()\nresult_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\nresult_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\ncallback.usage_metadata\n"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "{\n    'gpt-4o-mini-2024-07-18': {\n        'input_tokens': 8,\n        'output_tokens': 10,\n        'total_tokens': 18,\n        'input_token_details': {'audio': 0, 'cache_read': 0},\n        'output_token_details': {'audio': 0, 'reasoning': 0}\n    },\n    'claude-haiku-4-5-20251001': {\n        'input_tokens': 8,\n        'output_tokens': 21,\n        'total_tokens': 29,\n        'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n    }\n}\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "When invoking a model, you can pass additional configuration through the config parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking."
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "response = model.invoke(\n    \"Tell me a joke\",\n    config={\n        \"run_name\": \"joke_generation\",      # Custom name for this run\n        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n        \"callbacks\": [my_callback_handler], # Callback handlers\n    }\n)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "These configuration values are particularly useful when:\n\nIdentifies this specific invocation in logs and traces. Not inherited by sub-calls.\n\nLabels inherited by all sub-calls for filtering and organization in debugging tools.\n\nCustom key-value pairs for tracking additional context, inherited by all sub-calls.\n\nControls the maximum number of parallel calls when using batch() or batch_as_completed() ."
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Handlers for monitoring and responding to events during execution.\n\nMaximum recursion depth for chains to prevent infinite loops in complex pipelines.\n\nSee full RunnableConfig reference for all supported attributes."
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "You can also create a runtime-configurable model by specifying configurable_fields . If you don’t specify a model value, then 'model' and 'model_provider' will be configurable by default."
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nconfigurable_model = init_chat_model(temperature=0)\n\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n)\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},  # Run with Claude\n)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "first_model = init_chat_model(\n        model=\"gpt-4.1-mini\",\n        temperature=0,\n        configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n        config_prefix=\"first\",  # Useful when you have a chain with multiple models\n)\n\nfirst_model.invoke(\"what's your name\")\n"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "first_model.invoke(\n    \"what's your name\",\n    config={\n        \"configurable\": {\n            \"first_model\": \"claude-sonnet-4-5-20250929\",\n            \"first_temperature\": 0.5,\n            \"first_max_tokens\": 100,\n        }\n    },\n)\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "We can call declarative operations like bind_tools , with_structured_output , with_configurable , etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object."
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "with_structured_output"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\n\n\nclass GetWeather(BaseModel):\n    \"\"\"Get the current weather in a given location\"\"\"\n\n        location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\nclass GetPopulation(BaseModel):\n    \"\"\"Get the current population in a given location\"\"\"\n\n        location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\nmodel = init_chat_model(temperature=0)\nmodel_with_tools = model.bind_tools([GetWeather, GetPopulation])\n\nmodel_with_tools.invoke(\n    \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\n).tool_calls\n"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "[\n    {\n        'name': 'GetPopulation',\n        'args': {'location': 'Los Angeles, CA'},\n        'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\n        'type': 'tool_call'\n    },\n    {\n        'name': 'GetPopulation',\n        'args': {'location': 'New York, NY'},\n        'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n        'type': 'tool_call'\n    }\n]\n"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "model_with_tools.invoke(\n    \"what's bigger in 2024 LA or NYC\",\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},\n).tool_calls\n"
    },
    {
        "title": "​Advanced topics",
        "type": "code",
        "content": "[\n    {\n        'name': 'GetPopulation',\n        'args': {'location': 'Los Angeles, CA'},\n        'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\n        'type': 'tool_call'\n    },\n    {\n        'name': 'GetPopulation',\n        'args': {'location': 'New York City, NY'},\n        'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\n        'type': 'tool_call'\n    }\n]\n"
    },
    {
        "title": "​Advanced topics",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Messages",
        "type": "text",
        "content": "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n\nMessages are objects that contain:"
    },
    {
        "title": "Messages",
        "type": "text",
        "content": "LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called."
    },
    {
        "title": "​Basic usage",
        "type": "text",
        "content": "The simplest way to use messages is to create message objects and pass them to a model when invoking ."
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nsystem_msg = SystemMessage(\"You are a helpful assistant.\")\nhuman_msg = HumanMessage(\"Hello, how are you?\")\n\n# Use with chat models\nmessages = [system_msg, human_msg]\nresponse = model.invoke(messages)  # Returns AIMessage\n"
    },
    {
        "title": "​Basic usage",
        "type": "text",
        "content": "Text prompts are strings - ideal for straightforward generation tasks where you don’t need to retain conversation history."
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "response = model.invoke(\"Write a haiku about spring\")\n"
    },
    {
        "title": "​Basic usage",
        "type": "text",
        "content": "Use text prompts when:\n\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects."
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(\"You are a poetry expert\"),\n    HumanMessage(\"Write a haiku about spring\"),\n    AIMessage(\"Cherry blossoms bloom...\")\n]\nresponse = model.invoke(messages)\n"
    },
    {
        "title": "​Basic usage",
        "type": "text",
        "content": "Use message prompts when:\n\nYou can also specify messages directly in OpenAI chat completions format."
    },
    {
        "title": "​Basic usage",
        "type": "code",
        "content": "messages = [\n    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n]\nresponse = model.invoke(messages)\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "A SystemMessage represent an initial set of instructions that primes the model’s behavior. You can use a system message to set the tone, define the model’s role, and establish guidelines for responses."
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "system_msg = SystemMessage(\"You are a helpful coding assistant.\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "from langchain.messages import SystemMessage, HumanMessage\n\nsystem_msg = SystemMessage(\"\"\"\nYou are a senior Python developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n\"\"\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "A HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content ."
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "response = model.invoke([\n  HumanMessage(\"What is machine learning?\")\n])\n"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "human_msg = HumanMessage(\n    content=\"Hello!\",\n    name=\"alice\",  # Optional: identify different users\n    id=\"msg_123\",  # Optional: unique identifier for tracing\n)\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "The name field behavior varies by provider - some use it for user identification, others ignore it. To check, refer to the model provider’s reference ."
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "An AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access."
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "response = model.invoke(\"Explain AI\")\nprint(type(response))  # <class 'langchain_core.messages.AIMessage'>\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "AIMessage objects are returned by the model when calling it, which contains all of the associated metadata in the response."
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new AIMessage object and insert it into the message history as if it came from the model."
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "from langchain.messages import AIMessage, SystemMessage, HumanMessage\n\n# Create an AI message manually (e.g., for conversation history)\nai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n\n# Add to conversation history\nmessages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "The text content of the message.\n\nThe raw content of the message.\n\nThe standardized content blocks of the message.\n\nThe tool calls made by the model. Empty if no tools are called.\n\nA unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n\nThe usage metadata of the message, which can contain token counts when available.\n\nThe response metadata of the message.\n\nWhen models make tool calls , they’re included in the AIMessage :"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    ...\n\nmodel_with_tools = model.bind_tools([get_weather])\nresponse = model_with_tools.invoke(\"What's the weather in Paris?\")\n\nfor tool_call in response.tool_calls:\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n    print(f\"ID: {tool_call['id']}\")\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "Other structured data, such as reasoning or citations, can also appear in message content .\n\nAn AIMessage can hold token counts and other usage metadata in its usage_metadata field:"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nresponse = model.invoke(\"Hello!\")\nresponse.usage_metadata\n"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "{'input_tokens': 8,\n 'output_tokens': 304,\n 'total_tokens': 312,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 256}}\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "During streaming, you’ll receive AIMessageChunk objects that can be combined into a full message object:"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "chunks = []\nfull_message = None\nfor chunk in model.stream(\"Hi\"):\n    chunks.append(chunk)\n    print(chunk.text)\n    full_message = chunk if full_message is None else full_message + chunk\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "Learn more:\n\nFor models that support tool calling , AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n\nTools can generate ToolMessage objects directly. Below, we show a simple example. Read more in the tools guide ."
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "# After a model makes a tool call\nai_message = AIMessage(\n    content=[],\n    tool_calls=[{\n        \"name\": \"get_weather\",\n        \"args\": {\"location\": \"San Francisco\"},\n        \"id\": \"call_123\"\n    }]\n)\n\n# Execute tool and create result message\nweather_result = \"Sunny, 72°F\"\ntool_message = ToolMessage(\n    content=weather_result,\n    tool_call_id=\"call_123\"  # Must match the call ID\n)\n\n# Continue conversation\nmessages = [\n    HumanMessage(\"What's the weather in San Francisco?\"),\n    ai_message,  # Model's tool call\n    tool_message,  # Tool execution result\n]\nresponse = model.invoke(messages)  # Model processes the result\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "The stringified output of the tool call.\n\nThe ID of the tool call that this message is responding to. (this must match the ID of the tool call in the AIMessage )"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "The name of the tool that was called.\n\nAdditional data not sent to the model but can be accessed programmatically.\n\nThe artifact field stores supplementary data that won’t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model’s context."
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "For example, a retrieval tool could retrieve a passage from a document for reference by a model. Where message content contains text that the model will reference, an artifact can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:"
    },
    {
        "title": "​Message types",
        "type": "code",
        "content": "from langchain.messages import ToolMessage\n\n# Sent to model\nmessage_content = \"It was the best of times, it was the worst of times.\"\n\n# Artifact available downstream\nartifact = {\"document_id\": \"doc_123\", \"page\": 0}\n\ntool_message = ToolMessage(\n    content=message_content,\n    tool_call_id=\"call_123\",\n    name=\"search_books\",\n    artifact=artifact,\n)\n"
    },
    {
        "title": "​Message types",
        "type": "text",
        "content": "See the RAG tutorial for an end-to-end example of building retrieval agents with LangChain."
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "You can think of a message’s content as the payload of data that gets sent to the model. Messages have a content attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data."
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.\n\nLangChain chat models accept message content in the content attribute, and can contain:"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "from langchain.messages import HumanMessage\n\n# String content\nhuman_message = HumanMessage(\"Hello, how are you?\")\n\n# Provider-native format (e.g., OpenAI)\nhuman_message = HumanMessage(content=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n])\n\n# List of standard content blocks\nhuman_message = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Specifying content_blocks when initializing a message will still populate message content , but provides a type-safe interface for doing so."
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "LangChain provides a standard representation for message content that works across providers.\n\nMessage objects implement a content_blocks property that will lazily parse the content attribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking or reasoning blocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock representation:"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "ReasoningContentBlock"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "from langchain.messages import AIMessage\n\nmessage = AIMessage(\n    content=[\n        {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n        {\"type\": \"text\", \"text\": \"...\"},\n    ],\n    response_metadata={\"model_provider\": \"anthropic\"}\n)\nmessage.content_blocks\n"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "[{'type': 'reasoning',\n  'reasoning': '...',\n  'extras': {'signature': 'WaUjzkyp...'}},\n {'type': 'text', 'text': '...'}]\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "See the integrations guides to get started with the\ninference provider of your choice.\n\nSerializing standard content\n\nIf an application outside of LangChain needs access to the standard content block\nrepresentation, you can opt-in to storing content blocks in message content.\n\nTo do this, you can set the LC_OUTPUT_VERSION environment variable to v1 . Or,\ninitialize any chat model with output_version=\"v1\" :"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Multimodality refers to the ability to work with data that comes in different\nforms, such as text, audio, images, and video. LangChain includes standard types\nfor these data that can be used across providers.\n\nChat models can accept multimodal data as input and generate\nit as output. Below we show short examples of input messages featuring multimodal data.\n\nExtra keys can be included top-level in the content block or nested in \"extras\": {\"key\": value} ."
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "\"extras\": {\"key\": value}"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "OpenAI and AWS Bedrock Converse ,\nfor example, require a filename for PDFs. See the provider page for your chosen model for specifics."
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "# From URL\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n        {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n    ]\n}\n\n# From base64 data\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n        {\n            \"type\": \"image\",\n            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n            \"mime_type\": \"image/jpeg\",\n        },\n    ]\n}\n\n# From provider-managed File ID\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n        {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n    ]\n}\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Not all models support all file types. Check the model provider’s reference for supported formats and size limits.\n\nContent blocks are represented (either when creating a message or accessing the content_blocks property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "The text content\n\nList of annotations for the text\n\nAdditional provider-specific data\n\nExample:"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "{\n    \"type\": \"text\",\n    \"text\": \"Hello world\",\n    \"annotations\": []\n}\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "The reasoning content\n\nAdditional provider-specific data\n\nExample:"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "{\n    \"type\": \"reasoning\",\n    \"reasoning\": \"The user is asking about...\",\n    \"extras\": {\"signature\": \"abc123\"},\n}\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "URL pointing to the image location.\n\nBase64-encoded image data.\n\nReference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).\n\nImage MIME type (e.g., image/jpeg , image/png )"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "URL pointing to the audio location.\n\nBase64-encoded audio data.\n\nReference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).\n\nAudio MIME type (e.g., audio/mpeg , audio/wav )"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "URL pointing to the video location.\n\nBase64-encoded video data.\n\nReference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).\n\nVideo MIME type (e.g., video/mp4 , video/webm )"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "URL pointing to the file location.\n\nBase64-encoded file data.\n\nReference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).\n\nFile MIME type (e.g., application/pdf )"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "The text content\n\nMIME type of the text (e.g., text/plain , text/markdown )"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Name of the tool to call\n\nArguments to pass to the tool\n\nUnique identifier for this tool call\n\nExample:"
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "{\n    \"type\": \"tool_call\",\n    \"name\": \"search\",\n    \"args\": {\"query\": \"weather\"},\n    \"id\": \"call_123\"\n}\n"
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Purpose: Streaming tool call fragments\n\nAlways \"tool_call_chunk\""
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Name of the tool being called\n\nPartial tool arguments (may be incomplete JSON)\n\nTool call identifier\n\nPosition of this chunk in the stream\n\nPurpose: Malformed calls, intended to catch JSON parsing errors.\n\nAlways \"invalid_tool_call\""
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Name of the tool that failed to be called\n\nArguments to pass to the tool\n\nDescription of what went wrong\n\nPurpose: Tool call that is executed server-side.\n\nAlways \"server_tool_call\""
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "An identifier associated with the tool call.\n\nThe name of the tool to be called.\n\nPartial tool arguments (may be incomplete JSON)\n\nPurpose: Streaming server-side tool call fragments\n\nAlways \"server_tool_call_chunk\""
    },
    {
        "title": "​Message content",
        "type": "code",
        "content": "\"server_tool_call_chunk\""
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "An identifier associated with the tool call.\n\nName of the tool being called\n\nPartial tool arguments (may be incomplete JSON)\n\nPosition of this chunk in the stream\n\nPurpose: Search results\n\nAlways \"server_tool_result\""
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Identifier of the corresponding server tool call.\n\nIdentifier associated with the server tool result.\n\nExecution status of the server-side tool. \"success\" or \"error\" ."
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Output of the executed tool.\n\nPurpose: Provider-specific escape hatch\n\nAlways \"non_standard\""
    },
    {
        "title": "​Message content",
        "type": "text",
        "content": "Provider-specific data structure\n\nUsage: For experimental or provider-unique features\n\nAdditional provider-specific content types may be found within the reference documentation of each model provider.\n\nView the canonical type definitions in the API reference .\n\nContent blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the content property, but rather a new property that can be used to access the content of a message in a standardized format."
    },
    {
        "title": "​Use with chat models",
        "type": "text",
        "content": "Chat models accept a sequence of message objects as input and return an AIMessage as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages."
    },
    {
        "title": "​Use with chat models",
        "type": "text",
        "content": "Refer to the below guides to learn more:\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Tools",
        "type": "text",
        "content": "Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems—such as APIs, databases, or file systems—using structured input.\n\nTools are components that agents call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs. Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models , allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.\n\nServer-side tool use\n\nSome chat models (e.g., OpenAI , Anthropic , and Gemini ) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model."
    },
    {
        "title": "​Create tools",
        "type": "text",
        "content": "The simplest way to create a tool is with the @tool decorator. By default, the function’s docstring becomes the tool’s description that helps the model understand when to use it:"
    },
    {
        "title": "​Create tools",
        "type": "code",
        "content": "from langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n"
    },
    {
        "title": "​Create tools",
        "type": "text",
        "content": "Type hints are required as they define the tool’s input schema. The docstring should be informative and concise to help the model understand the tool’s purpose.\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:"
    },
    {
        "title": "​Create tools",
        "type": "code",
        "content": "@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n"
    },
    {
        "title": "​Create tools",
        "type": "text",
        "content": "Override the auto-generated tool description for clearer model guidance:"
    },
    {
        "title": "​Create tools",
        "type": "code",
        "content": "@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n"
    },
    {
        "title": "​Create tools",
        "type": "text",
        "content": "Define complex inputs with Pydantic models or JSON schemas:"
    },
    {
        "title": "​Create tools",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass WeatherInput(BaseModel):\n    \"\"\"Input for weather queries.\"\"\"\n    location: str = Field(description=\"City name or coordinates\")\n    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n        default=\"celsius\",\n        description=\"Temperature unit preference\"\n    )\n    include_forecast: bool = Field(\n        default=False,\n        description=\"Include 5-day forecast\"\n    )\n\n@tool(args_schema=WeatherInput)\ndef get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n    \"\"\"Get current weather and optional forecast.\"\"\"\n    temp = 22 if units == \"celsius\" else 72\n    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n    if include_forecast:\n        result += \"\\nNext 5 days: Sunny\"\n    return result\n"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Why this matters: Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n\nTools can access runtime information through the ToolRuntime parameter, which provides:"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Use ToolRuntime to access all runtime information in a single parameter. Simply add runtime: ToolRuntime to your tool signature, and it will be automatically injected without being exposed to the LLM."
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "ToolRuntime : A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate InjectedState , InjectedStore , get_runtime , and InjectedToolCallId annotations."
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Accessing state:\n\nTools can access the current graph state using ToolRuntime :"
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "from langchain.tools import tool, ToolRuntime\n\n# Access the current conversation state\n@tool\ndef summarize_conversation(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Summarize the conversation so far.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "The tool_runtime parameter is hidden from the model. For the example above, the model only sees pref_name in the tool schema - tool_runtime is not included in the request."
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Updating state:\n\nUse Command to update the agent’s state or control the graph’s execution flow:"
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "from langgraph.types import Command\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.tools import tool, ToolRuntime\n\n# Update the conversation history by removing all messages\n@tool\ndef clear_conversation() -> Command:\n    \"\"\"Clear the conversation history.\"\"\"\n\n    return Command(\n        update={\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n        }\n    )\n\n# Update the user_name in the agent state\n@tool\ndef update_user_name(\n    new_name: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Update the user's name.\"\"\"\n    return Command(update={\"user_name\": new_name})\n"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through runtime.context ."
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Tools can access runtime context through ToolRuntime :"
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Access persistent data across conversations using the store. The store is accessed via runtime.store and allows you to save and retrieve user-specific or application-specific data."
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Tools can access and update the store through ToolRuntime :"
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "from typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Stream custom updates from tools as they execute using runtime.stream_writer . This is useful for providing real-time feedback to users about what a tool is doing."
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "runtime.stream_writer"
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "from langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "If you use runtime.stream_writer inside your tool, the tool must be invoked within a LangGraph execution context. See Streaming for more details."
    },
    {
        "title": "​Accessing Context",
        "type": "code",
        "content": "runtime.stream_writer"
    },
    {
        "title": "​Accessing Context",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today’s LLMs; a full history may not fit inside an LLM’s context window, resulting in an context loss or errors.\n\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.\n\nChat models accept context using messages , which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or “forget” stale information."
    },
    {
        "title": "​Usage",
        "type": "text",
        "content": "To add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent."
    },
    {
        "title": "​Usage",
        "type": "text",
        "content": "LangChain’s agent manages short-term memory as a part of your agent’s state.\n\nBy storing these in the graph’s state, the agent can access the full context for a given conversation while maintaining separation between different threads.\n\nState is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n\nShort-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step."
    },
    {
        "title": "​Usage",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver  \n\n\nagent = create_agent(\n    \"gpt-5\",\n    [get_user_info],\n    checkpointer=InMemorySaver(),  \n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  \n)\n"
    },
    {
        "title": "​Usage",
        "type": "text",
        "content": "In production, use a checkpointer backed by a database:"
    },
    {
        "title": "​Usage",
        "type": "code",
        "content": "pip install langgraph-checkpoint-postgres\n"
    },
    {
        "title": "​Usage",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nfrom langgraph.checkpoint.postgres import PostgresSaver  \n\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    checkpointer.setup() # auto create tables in PostgresSql\n    agent = create_agent(\n        \"gpt-5\",\n        [get_user_info],\n        checkpointer=checkpointer,  \n    )\n"
    },
    {
        "title": "​Customizing agent memory",
        "type": "text",
        "content": "By default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key."
    },
    {
        "title": "​Customizing agent memory",
        "type": "text",
        "content": "You can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter."
    },
    {
        "title": "​Customizing agent memory",
        "type": "code",
        "content": "from langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):  \n    user_id: str\n    preferences: dict\n\nagent = create_agent(\n    \"gpt-5\",\n    [get_user_info],\n    state_schema=CustomAgentState,  \n    checkpointer=InMemorySaver(),\n)\n\n# Custom state can be passed in invoke\nresult = agent.invoke(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"user_id\": \"user_123\",  \n        \"preferences\": {\"theme\": \"dark\"}  \n    },\n    {\"configurable\": {\"thread_id\": \"1\"}})\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:"
    },
    {
        "title": "Trim messages",
        "type": "text",
        "content": "Remove first or last N messages (before calling LLM)"
    },
    {
        "title": "Summarize messages",
        "type": "text",
        "content": "Summarize earlier messages in the history and replace them with a summary"
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "Custom strategies (e.g., message filtering, etc.)\n\nThis allows the agent to keep track of the conversation without exceeding the LLM’s context window.\n\nMost LLMs have a maximum supported context window (denominated in tokens).\n\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens ) to use for handling the boundary."
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "To trim message history in an agent, use the @before_model middleware decorator:"
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n"
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "You can delete messages from the graph state to manage the message history.\n\nThis is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the RemoveMessage ."
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "For RemoveMessage to work, you need to use a state key with add_messages reducer ."
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage  \n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n"
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \n"
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:"
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\n\n\n@after_model\ndef delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n    return None\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    system_prompt=\"Please be concise and to the point.\",\n    middleware=[delete_old_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n"
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n"
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\nTo summarize message history in an agent, use the built-in SummarizationMiddleware :"
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "SummarizationMiddleware"
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n            messages_to_keep=20,  # Keep last 20 messages after summary\n        )\n    ],\n    checkpointer=checkpointer,\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob!\n\"\"\"\n"
    },
    {
        "title": "Custom strategies",
        "type": "text",
        "content": "See SummarizationMiddleware for more configuration options."
    },
    {
        "title": "Custom strategies",
        "type": "code",
        "content": "SummarizationMiddleware"
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "You can access and modify the short-term memory (state) of an agent in several ways:\n\nAccess short term memory (state) in a tool using the ToolRuntime parameter."
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "The tool_runtime parameter is hidden from the tool signature (so the model doesn’t see it), but the tool can access the state through it."
    },
    {
        "title": "​Access memory",
        "type": "code",
        "content": "from langchain.agents import create_agent, AgentState\nfrom langchain.tools import tool, ToolRuntime\n\n\nclass CustomState(AgentState):\n    user_id: str\n\n@tool\ndef get_user_info(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = runtime.state[\"user_id\"]\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_user_info],\n    state_schema=CustomState,\n)\n\nresult = agent.invoke({\n    \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n# > User is John Smith.\n"
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "To modify the agent’s short-term memory (state) during execution, you can return state updates directly from the tools.\n\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts."
    },
    {
        "title": "​Access memory",
        "type": "code",
        "content": "from langchain.tools import tool, ToolRuntime\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.messages import ToolMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.types import Command\nfrom pydantic import BaseModel\n\n\nclass CustomState(AgentState):  \n    user_name: str\n\nclass CustomContext(BaseModel):\n    user_id: str\n\n@tool\ndef update_user_info(\n    runtime: ToolRuntime[CustomContext, CustomState],\n) -> Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = runtime.context.user_id  \n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomContext, CustomState]\n) -> str:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = runtime.state[\"user_name\"]\n    return f\"Hello {user_name}!\"\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[update_user_info, greet],\n    state_schema=CustomState,\n    context_schema=CustomContext,  \n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    context=CustomContext(user_id=\"user_123\"),\n)\n"
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields."
    },
    {
        "title": "​Access memory",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom typing import TypedDict\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass CustomContext(TypedDict):\n    user_name: str\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a city.\"\"\"\n    return f\"The weather in {city} is always sunny!\"\n\n\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context[\"user_name\"]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n    middleware=[dynamic_system_prompt],\n    context_schema=CustomContext,\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    context=CustomContext(user_name=\"John Smith\"),\n)\nfor msg in result[\"messages\"]:\n    msg.pretty_print()\n\n"
    },
    {
        "title": "​Access memory",
        "type": "code",
        "content": "================================ Human Message =================================\n\nWhat is the weather in SF?\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)\n Call ID: call_WFQlOGn4b2yoJrv7cih342FG\n  Args:\n    city: San Francisco\n================================= Tool Message =================================\nName: get_weather\n\nThe weather in San Francisco is always sunny!\n================================== Ai Message ==================================\n\nHi John Smith, the weather in San Francisco is always sunny!\n"
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "Access short term memory (state) in @before_model middleware to process messages before model calls."
    },
    {
        "title": "​Access memory",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[trim_messages]\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n"
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "Access short term memory (state) in @after_model middleware to process messages after model calls."
    },
    {
        "title": "​Access memory",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.runtime import Runtime\n\n\n@after_model\ndef validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove messages containing sensitive words.\"\"\"\n    STOP_WORDS = [\"password\", \"secret\"]\n    last_message = state[\"messages\"][-1]\n    if any(word in last_message.content for word in STOP_WORDS):\n        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[],\n    middleware=[validate_response],\n    checkpointer=InMemorySaver(),\n)\n"
    },
    {
        "title": "​Access memory",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Streaming",
        "type": "text",
        "content": "LangChain implements a streaming system to surface real-time updates.\n\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "LangChain’s streaming system lets you surface live feedback from agent runs to your application.\n\nWhat’s possible with LangChain streaming:"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "\"Fetched 10/100 records\""
    },
    {
        "title": "​Agent progress",
        "type": "text",
        "content": "To stream agent progress, use the stream or astream methods with stream_mode=\"updates\" . This emits an event after every agent step."
    },
    {
        "title": "​Agent progress",
        "type": "code",
        "content": "stream_mode=\"updates\""
    },
    {
        "title": "​Agent progress",
        "type": "text",
        "content": "For example, if you have an agent that calls a tool once, you should see the following updates:"
    },
    {
        "title": "​Agent progress",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor chunk in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"updates\",\n):\n    for step, data in chunk.items():\n        print(f\"step: {step}\")\n        print(f\"content: {data['messages'][-1].content_blocks}\")\n"
    },
    {
        "title": "​Agent progress",
        "type": "code",
        "content": "step: model\ncontent: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\n\nstep: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\nstep: model\ncontent: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]\n"
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "To stream tokens as they are produced by the LLM, use stream_mode=\"messages\" . Below you can see the output of the agent streaming tool calls and the final response."
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "stream_mode=\"messages\""
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor token, metadata in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"messages\",\n):\n    print(f\"node: {metadata['langgraph_node']}\")\n    print(f\"content: {token.content_blocks}\")\n    print(\"\\n\")\n"
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "node: model\ncontent: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}]\n\n\nnode: model\ncontent: []\n\n\nnode: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\n\nnode: model\ncontent: []\n\n\nnode: model\ncontent: [{'type': 'text', 'text': 'Here'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ''s'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' what'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' I'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' got'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ':'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' \"'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': \"It's\"}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' always'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' sunny'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' in'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' San'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' Francisco'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\n"
    },
    {
        "title": "​Custom updates",
        "type": "text",
        "content": "To stream updates from tools as they are executed, you can use get_stream_writer ."
    },
    {
        "title": "​Custom updates",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer  \n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()  \n    # stream any arbitrary data\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"custom\"\n):\n    print(chunk)\n"
    },
    {
        "title": "​Custom updates",
        "type": "code",
        "content": "Looking up data for city: San Francisco\nAcquired data for city: San Francisco\n"
    },
    {
        "title": "​Custom updates",
        "type": "text",
        "content": "If you add get_stream_writer inside your tool, you won’t be able to invoke the tool outside of a LangGraph execution context."
    },
    {
        "title": "​Stream multiple modes",
        "type": "text",
        "content": "You can specify multiple streaming modes by passing stream mode as a list: stream_mode=[\"updates\", \"custom\"] :"
    },
    {
        "title": "​Stream multiple modes",
        "type": "code",
        "content": "stream_mode=[\"updates\", \"custom\"]"
    },
    {
        "title": "​Stream multiple modes",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\n\nfor stream_mode, chunk in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=[\"updates\", \"custom\"]\n):\n    print(f\"stream_mode: {stream_mode}\")\n    print(f\"content: {chunk}\")\n    print(\"\\n\")\n"
    },
    {
        "title": "​Disable streaming",
        "type": "text",
        "content": "In some applications you might need to disable streaming of individual tokens for a given model.\n\nThis is useful in multi-agent systems to control which agents stream their output.\n\nSee the Models guide to learn how to disable streaming.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Middleware",
        "type": "text",
        "content": "Middleware provides a way to more tightly control what happens inside the agent.\n\nThe core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\nMiddleware exposes hooks before and after each of those steps:"
    },
    {
        "title": "Monitor",
        "type": "text",
        "content": "Track agent behavior with logging, analytics, and debugging"
    },
    {
        "title": "Modify",
        "type": "text",
        "content": "Transform prompts, tool selection, and output formatting"
    },
    {
        "title": "Control",
        "type": "text",
        "content": "Add retries, fallbacks, and early termination logic"
    },
    {
        "title": "Enforce",
        "type": "text",
        "content": "Apply rate limits, guardrails, and PII detection\n\nAdd middleware by passing it to create_agent :"
    },
    {
        "title": "Enforce",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "LangChain provides prebuilt middleware for common use cases:\n\nAutomatically summarize conversation history when approaching token limits.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[weather_tool, calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n            messages_to_keep=20,  # Keep last 20 messages after summary\n            summary_prompt=\"Custom prompt for summarization...\",  # Optional\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Model for generating summaries\n\nToken threshold for triggering summarization\n\nRecent messages to preserve\n\nCustom token counting function. Defaults to character-based counting.\n\nCustom prompt template. Uses built-in template if not specified.\n\nPrefix for summary messages\n\nPause agent execution for human approval, editing, or rejection of tool calls before they execute.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[read_email_tool, send_email_tool],\n    checkpointer=InMemorySaver(),\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                # Require approval, editing, or rejection for sending emails\n                \"send_email_tool\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n                },\n                # Auto-approve reading emails\n                \"read_email_tool\": False,\n            }\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Mapping of tool names to approval configs. Values can be True (interrupt with default config), False (auto-approve), or an InterruptOnConfig object."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Prefix for action request descriptions\n\nInterruptOnConfig options:"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "List of allowed decisions: \"approve\" , \"edit\" , or \"reject\""
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Static string or callable function for custom description\n\nImportant: Human-in-the-loop middleware requires a checkpointer to maintain state across interruptions.\n\nSee the human-in-the-loop documentation for complete examples and integration patterns.\n\nReduce costs by caching repetitive prompt prefixes with Anthropic models.\n\nPerfect for:\n\nLearn more about Anthropic Prompt Caching strategies and limitations."
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain_anthropic import ChatAnthropic\nfrom langchain_anthropic.middleware import AnthropicPromptCachingMiddleware\nfrom langchain.agents import create_agent\n\n\nLONG_PROMPT = \"\"\"\nPlease be a helpful assistant.\n\n<Lots more context ...>\n\"\"\"\n\nagent = create_agent(\n    model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"),\n    system_prompt=LONG_PROMPT,\n    middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n)\n\n# cache store\nagent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]})\n\n# cache hit, system prompt is cached\nagent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]})\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Cache type. Only \"ephemeral\" is currently supported."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Time to live for cached content. Valid values: \"5m\" or \"1h\""
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Minimum number of messages before caching starts\n\nBehavior when using non-Anthropic models. Options: \"ignore\" , \"warn\" , or \"raise\""
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Limit the number of model calls to prevent infinite loops or excessive costs.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelCallLimitMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        ModelCallLimitMiddleware(\n            thread_limit=10,  # Max 10 calls per thread (across runs)\n            run_limit=5,  # Max 5 calls per run (single invocation)\n            exit_behavior=\"end\",  # Or \"error\" to raise exception\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Maximum model calls across all runs in a thread. Defaults to no limit.\n\nMaximum model calls per single invocation. Defaults to no limit.\n\nBehavior when limit is reached. Options: \"end\" (graceful termination) or \"error\" (raise exception)"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Limit the number of tool calls to specific tools or all tools.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\n\n\n# Limit all tool calls\nglobal_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n\n# Limit specific tool\nsearch_limiter = ToolCallLimitMiddleware(\n    tool_name=\"search\",\n    thread_limit=5,\n    run_limit=3,\n)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[global_limiter, search_limiter],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Specific tool to limit. If not provided, limits apply to all tools.\n\nMaximum tool calls across all runs in a thread. Defaults to no limit.\n\nMaximum tool calls per single invocation. Defaults to no limit.\n\nBehavior when limit is reached. Options: \"end\" (graceful termination) or \"error\" (raise exception)"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Automatically fallback to alternative models when the primary model fails.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",  # Primary model\n    tools=[...],\n    middleware=[\n        ModelFallbackMiddleware(\n            \"gpt-4o-mini\",  # Try first on error\n            \"claude-3-5-sonnet-20241022\",  # Then this\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "First fallback model to try when the primary model fails. Can be a model string (e.g., \"openai:gpt-4o-mini\" ) or a BaseChatModel instance."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Additional fallback models to try in order if previous models fail\n\nDetect and handle Personally Identifiable Information in conversations.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        # Redact emails in user input\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        # Mask credit cards (show last 4 digits)\n        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n        # Custom PII type with regex\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",  # Raise error if detected\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Type of PII to detect. Can be a built-in type ( email , credit_card , ip , mac_address , url ) or a custom type name."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n\nCheck user messages before model call\n\nCheck AI messages after model call\n\nCheck tool result messages after execution\n\nAdd todo list management capabilities for complex multi-step tasks.\n\nThis middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning."
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import TodoListMiddleware\nfrom langchain.messages import HumanMessage\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[TodoListMiddleware()],\n)\n\nresult = agent.invoke({\"messages\": [HumanMessage(\"Help me refactor my codebase\")]})\nprint(result[\"todos\"])  # Array of todo items with status tracking\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n\nCustom description for the write_todos tool. Uses built-in description if not specified."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Use an LLM to intelligently select relevant tools before calling the main model.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolSelectorMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[tool1, tool2, tool3, tool4, tool5, ...],  # Many tools\n    middleware=[\n        LLMToolSelectorMiddleware(\n            model=\"gpt-4o-mini\",  # Use cheaper model for selection\n            max_tools=3,  # Limit to 3 most relevant tools\n            always_include=[\"search\"],  # Always include certain tools\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Model for tool selection. Can be a model string or BaseChatModel instance. Defaults to the agent’s main model."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Instructions for the selection model. Uses built-in prompt if not specified.\n\nMaximum number of tools to select. Defaults to no limit.\n\nList of tool names to always include in the selection\n\nAutomatically retry failed tool calls with configurable exponential backoff.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolRetryMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,  # Retry up to 3 times\n            backoff_factor=2.0,  # Exponential backoff multiplier\n            initial_delay=1.0,  # Start with 1 second delay\n            max_delay=60.0,  # Cap delays at 60 seconds\n            jitter=True,  # Add random jitter to avoid thundering herd\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Maximum number of retry attempts after the initial call (3 total attempts with default)\n\nOptional list of tools or tool names to apply retry logic to. If None , applies to all tools."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Either a tuple of exception types to retry on, or a callable that takes an exception and returns True if it should be retried."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Multiplier for exponential backoff. Each retry waits initial_delay * (backoff_factor ** retry_number) seconds. Set to 0.0 for constant delay."
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "initial_delay * (backoff_factor ** retry_number)"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Initial delay in seconds before first retry\n\nMaximum delay in seconds between retries (caps exponential backoff growth)\n\nWhether to add random jitter (±25%) to delay to avoid thundering herd\n\nEmulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolEmulator\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[get_weather, search_database, send_email],\n    middleware=[\n        # Emulate all tools by default\n        LLMToolEmulator(),\n\n        # Or emulate specific tools\n        # LLMToolEmulator(tools=[\"get_weather\", \"search_database\"]),\n\n        # Or use a custom model for emulation\n        # LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\"),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "List of tool names (str) or BaseTool instances to emulate. If None (default), ALL tools will be emulated. If empty list, no tools will be emulated."
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Model to use for generating emulated tool responses. Can be a model identifier string or BaseChatModel instance.\n\nManage conversation context by trimming, summarizing, or clearing tool uses.\n\nPerfect for:"
    },
    {
        "title": "​Built-in middleware",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        ContextEditingMiddleware(\n            edits=[\n                ClearToolUsesEdit(trigger=1000),  # Clear old tool uses\n            ],\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Token counting method. Options: \"approximate\" or \"model\""
    },
    {
        "title": "​Built-in middleware",
        "type": "text",
        "content": "Token count that triggers the edit\n\nMinimum tokens to reclaim\n\nNumber of recent tool results to preserve\n\nWhether to clear tool call parameters\n\nList of tool names to exclude from clearing\n\nPlaceholder text for cleared outputs"
    },
    {
        "title": "​Custom middleware",
        "type": "text",
        "content": "Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\nYou can create middleware in two ways:"
    },
    {
        "title": "​Decorator-based middleware",
        "type": "text",
        "content": "For simple middleware that only needs a single hook, decorators provide the quickest way to add functionality:"
    },
    {
        "title": "​Decorator-based middleware",
        "type": "code",
        "content": "from langchain.agents.middleware import before_model, after_model, wrap_model_call\nfrom langchain.agents.middleware import AgentState, ModelRequest, ModelResponse, dynamic_prompt\nfrom langchain.messages import AIMessage\nfrom langchain.agents import create_agent\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\n\n# Node-style: logging before model calls\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"About to call model with {len(state['messages'])} messages\")\n    return None\n\n# Node-style: validation after model calls\n@after_model(can_jump_to=[\"end\"])\ndef validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    last_message = state[\"messages\"][-1]\n    if \"BLOCKED\" in last_message.content:\n        return {\n            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n            \"jump_to\": \"end\"\n        }\n    return None\n\n# Wrap-style: retry logic\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n\n# Wrap-style: dynamic prompts\n@dynamic_prompt\ndef personalized_prompt(request: ModelRequest) -> str:\n    user_id = request.runtime.context.get(\"user_id\", \"guest\")\n    return f\"You are a helpful assistant for user {user_id}. Be concise and friendly.\"\n\n# Use decorators in agent\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],\n    tools=[...],\n)\n"
    },
    {
        "title": "Use decorators when",
        "type": "text",
        "content": "• You need a single hook • No complex configuration"
    },
    {
        "title": "Use classes when",
        "type": "text",
        "content": "• Multiple hooks needed • Complex configuration • Reuse across projects (config on init)"
    },
    {
        "title": "Node-style hooks",
        "type": "text",
        "content": "Run sequentially at specific execution points. Use for logging, validation, and state updates."
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "Intercept execution with full control over handler calls. Use for retries, caching, and transformation.\n\nRun at specific points in the execution flow:"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentMiddleware, AgentState\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\nclass LoggingMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"About to call model with {len(state['messages'])} messages\")\n        return None\n\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentMiddleware, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\nclass MessageLimitMiddleware(AgentMiddleware):\n    def __init__(self, max_messages: int = 50):\n        super().__init__()\n        self.max_messages = max_messages\n\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        if len(state[\"messages\"]) == self.max_messages:\n            return {\n                \"messages\": [AIMessage(\"Conversation limit reached.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "Intercept execution and control when the handler is called:"
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n\nExample: Model retry middleware"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom typing import Callable\n\nclass RetryMiddleware(AgentMiddleware):\n    def __init__(self, max_retries: int = 3):\n        super().__init__()\n        self.max_retries = max_retries\n\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        for attempt in range(self.max_retries):\n            try:\n                return handler(request)\n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\n\nclass DynamicModelMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Use different model based on conversation length\n        if len(request.messages) > 10:\n            request.model = init_chat_model(\"gpt-4o\")\n        else:\n            request.model = init_chat_model(\"gpt-4o-mini\")\n\n        return handler(request)\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.tools.tool_node import ToolCallRequest\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n\nclass ToolMonitoringMiddleware(AgentMiddleware):\n    def wrap_tool_call(\n        self,\n        request: ToolCallRequest,\n        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n    ) -> ToolMessage | Command:\n        print(f\"Executing tool: {request.tool_call['name']}\")\n        print(f\"Arguments: {request.tool_call['args']}\")\n\n        try:\n            result = handler(request)\n            print(f\"Tool completed successfully\")\n            return result\n        except Exception as e:\n            print(f\"Tool failed: {e}\")\n            raise\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "Middleware can extend the agent’s state with custom properties. Define a custom state type and set it as the state_schema :"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n    user_id: NotRequired[str]\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        # Access custom state properties\n        count = state.get(\"model_call_count\", 0)\n\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        # Update custom state\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "agent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[CallCounterMiddleware()],\n    tools=[...],\n)\n\n# Invoke with custom state\nresult = agent.invoke({\n    \"messages\": [HumanMessage(\"Hello\")],\n    \"model_call_count\": 0,\n    \"user_id\": \"user-123\",\n})\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "When using multiple middleware, understanding execution order is important:"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "agent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[middleware1, middleware2, middleware3],\n    tools=[...],\n)\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware1.before_agent()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware2.before_agent()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware3.before_agent()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware1.before_model()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware2.before_model()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware3.before_model()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware1.wrap_model_call()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware2.wrap_model_call()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware3.wrap_model_call()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware3.after_model()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware2.after_model()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware1.after_model()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware3.after_agent()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware2.after_agent()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "middleware1.after_agent()"
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "To exit early from middleware, return a dictionary with jump_to :"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "class EarlyExitMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n        # Check some condition\n        if should_exit(state):\n            return {\n                \"messages\": [AIMessage(\"Exiting early due to condition.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n"
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "Important: When jumping from before_model or after_model , jumping to \"model\" will cause all before_model middleware to run again."
    },
    {
        "title": "Wrap-style hooks",
        "type": "text",
        "content": "To enable jumping, decorate your hook with @hook_config(can_jump_to=[...]) :"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "@hook_config(can_jump_to=[...])"
    },
    {
        "title": "Wrap-style hooks",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentMiddleware, hook_config\nfrom typing import Any\n\nclass ConditionalMiddleware(AgentMiddleware):\n    @hook_config(can_jump_to=[\"end\", \"tools\"])\n    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n        if some_condition(state):\n            return {\"jump_to\": \"end\"}\n        return None\n"
    },
    {
        "title": "​Examples",
        "type": "text",
        "content": "Select relevant tools at runtime to improve performance and accuracy.\n\nBenefits:"
    },
    {
        "title": "​Examples",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest\nfrom typing import Callable\n\n\nclass ToolSelectorMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n        # Select a small, relevant subset of tools based on state/context\n        relevant_tools = select_relevant_tools(request.state, request.runtime)\n        request.tools = relevant_tools\n        return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=all_tools,  # All available tools need to be registered upfront\n    # Middleware can be used to select a smaller subset that's relevant for the given run.\n    middleware=[ToolSelectorMiddleware()],\n)\n"
    },
    {
        "title": "​Examples",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom typing import Literal, Callable\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain_core.tools import tool\n\n\n@tool\ndef github_create_issue(repo: str, title: str) -> dict:\n    \"\"\"Create an issue in a GitHub repository.\"\"\"\n    return {\"url\": f\"https://github.com/{repo}/issues/1\", \"title\": title}\n\n@tool\ndef gitlab_create_issue(project: str, title: str) -> dict:\n    \"\"\"Create an issue in a GitLab project.\"\"\"\n    return {\"url\": f\"https://gitlab.com/{project}/-/issues/1\", \"title\": title}\n\nall_tools = [github_create_issue, gitlab_create_issue]\n\n@dataclass\nclass Context:\n    provider: Literal[\"github\", \"gitlab\"]\n\nclass ToolSelectorMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Select tools based on the VCS provider.\"\"\"\n        provider = request.runtime.context.provider\n\n        if provider == \"gitlab\":\n            selected_tools = [t for t in request.tools if t.name == \"gitlab_create_issue\"]\n        else:\n            selected_tools = [t for t in request.tools if t.name == \"github_create_issue\"]\n\n        request.tools = selected_tools\n        return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=all_tools,\n    middleware=[ToolSelectorMiddleware()],\n    context_schema=Context,\n)\n\n# Invoke with GitHub context\nagent.invoke(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`\"}]\n    },\n    context=Context(provider=\"github\"),\n)\n"
    },
    {
        "title": "​Additional resources",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Structured output",
        "type": "text",
        "content": "Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.\n\nLangChain’s create_agent handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it’s captured, validated, and returned in the 'structured_response' key of the agent’s state."
    },
    {
        "title": "Structured output",
        "type": "code",
        "content": "'structured_response'"
    },
    {
        "title": "Structured output",
        "type": "code",
        "content": "def create_agent(\n    ...\n    response_format: Union[\n        ToolStrategy[StructuredResponseT],\n        ProviderStrategy[StructuredResponseT],\n        type[StructuredResponseT],\n    ]\n"
    },
    {
        "title": "​Response Format",
        "type": "code",
        "content": "ToolStrategy[StructuredResponseT]"
    },
    {
        "title": "​Response Format",
        "type": "code",
        "content": "ProviderStrategy[StructuredResponseT]"
    },
    {
        "title": "​Response Format",
        "type": "code",
        "content": "type[StructuredResponseT]"
    },
    {
        "title": "​Response Format",
        "type": "text",
        "content": "When a schema type is provided directly, LangChain automatically chooses:"
    },
    {
        "title": "​Response Format",
        "type": "text",
        "content": "The structured response is returned in the structured_response key of the agent’s final state."
    },
    {
        "title": "​Provider strategy",
        "type": "text",
        "content": "Some model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available.\n\nTo use this strategy, configure a ProviderStrategy :"
    },
    {
        "title": "​Provider strategy",
        "type": "code",
        "content": "class ProviderStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n"
    },
    {
        "title": "​Provider strategy",
        "type": "text",
        "content": "The schema defining the structured output format. Supports:"
    },
    {
        "title": "​Provider strategy",
        "type": "text",
        "content": "LangChain automatically uses ProviderStrategy when you pass a schema type directly to create_agent.response_format and the model supports native structured output:"
    },
    {
        "title": "​Provider strategy",
        "type": "code",
        "content": "create_agent.response_format"
    },
    {
        "title": "​Provider strategy",
        "type": "code",
        "content": "from pydantic import BaseModel\nfrom langchain.agents import create_agent\n\n\nclass ContactInfo(BaseModel):\n    \"\"\"Contact information for a person.\"\"\"\n    name: str = Field(description=\"The name of the person\")\n    email: str = Field(description=\"The email address of the person\")\n    phone: str = Field(description=\"The phone number of the person\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=tools,\n    response_format=ContactInfo  # Auto-selects ProviderStrategy\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n"
    },
    {
        "title": "​Provider strategy",
        "type": "text",
        "content": "Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\n\nIf the provider natively supports structured output for your model choice, it is functionally equivalent to write response_format=ProductReview instead of response_format=ToolStrategy(ProductReview) . In either case, if structured output is not supported, the agent will fall back to a tool calling strategy."
    },
    {
        "title": "​Provider strategy",
        "type": "code",
        "content": "response_format=ProductReview"
    },
    {
        "title": "​Provider strategy",
        "type": "code",
        "content": "response_format=ToolStrategy(ProductReview)"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "For models that don’t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.\n\nTo use this strategy, configure a ToolStrategy :"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "class ToolStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    tool_message_content: str | None\n    handle_errors: Union[\n        bool,\n        str,\n        type[Exception],\n        tuple[type[Exception], ...],\n        Callable[[Exception], str],\n    ]\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "The schema defining the structured output format. Supports:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "Custom content for the tool message returned when structured output is generated.\nIf not provided, defaults to a message showing the structured response data.\n\nError handling strategy for structured output validation failures. Defaults to True ."
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "tuple[type[Exception], ...]"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "Callable[[Exception], str]"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductReview(BaseModel):\n    \"\"\"Analysis of a product review.\"\"\"\n    rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n    key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=tools,\n    response_format=ToolStrategy(ProductReview)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n})\nresult[\"structured_response\"]\n# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "The tool_message_content parameter allows you to customize the message that appears in the conversation history when structured output is generated:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass MeetingAction(BaseModel):\n    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n    task: str = Field(description=\"The specific task to be completed\")\n    assignee: str = Field(description=\"Person responsible for the task\")\n    priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n        schema=MeetingAction,\n        tool_message_content=\"Action item captured and added to meeting notes!\"\n    )\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n})\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================ Human Message =================================\n\nFrom our meeting: Sarah needs to update the project timeline as soon as possible\n================================== Ai Message ==================================\nTool Calls:\n  MeetingAction (call_1)\n Call ID: call_1\n  Args:\n    task: Update the project timeline\n    assignee: Sarah\n    priority: high\n================================= Tool Message =================================\nName: MeetingAction\n\nAction item captured and added to meeting notes!\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "Without tool_message_content , our final ToolMessage would be:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================= Tool Message =================================\nName: MeetingAction\n\nReturning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.\n\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in a ToolMessage and prompts the model to retry:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\nfrom typing import Union\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Person's name\")\n    email: str = Field(description=\"Email address\")\n\nclass EventDetails(BaseModel):\n    event_name: str = Field(description=\"Name of the event\")\n    date: str = Field(description=\"Event date\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================ Human Message =================================\n\nExtract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\nNone\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_1)\n Call ID: call_1\n  Args:\n    name: John Doe\n    email: john@email.com\n  EventDetails (call_2)\n Call ID: call_2\n  Args:\n    event_name: Tech Conference\n    date: March 15th\n================================= Tool Message =================================\nName: ContactInfo\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================= Tool Message =================================\nName: EventDetails\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_3)\n Call ID: call_3\n  Args:\n    name: John Doe\n    email: john@email.com\n================================= Tool Message =================================\nName: ContactInfo\n\nReturning structured response: {'name': 'John Doe', 'email': 'john@email.com'}\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "When structured output doesn’t match the expected schema, the agent provides specific error feedback:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductRating(BaseModel):\n    rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5)\n    comment: str = Field(description=\"Review comment\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n    system_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}]\n})\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================ Human Message =================================\n\nParse this: Amazing product, 10/10!\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_1)\n Call ID: call_1\n  Args:\n    rating: 10\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nError: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_2)\n Call ID: call_2\n  Args:\n    rating: 5\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nReturning structured response: {'rating': 5, 'comment': 'Amazing product'}\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "You can customize how errors are handled using the handle_errors parameter:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "ToolStrategy(\n    schema=ProductRating,\n    handle_errors=\"Please provide a valid rating between 1-5 and include a comment.\"\n)\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "If handle_errors is a string, the agent will always prompt the model to re-try with a fixed tool message:"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================= Tool Message =================================\nName: ProductRating\n\nPlease provide a valid rating between 1-5 and include a comment.\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "ToolStrategy(\n    schema=ProductRating,\n    handle_errors=ValueError  # Only retry on ValueError, raise others\n)\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "If handle_errors is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised."
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "ToolStrategy(\n    schema=ProductRating,\n    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\n)\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "If handle_errors is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised."
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "def custom_error_handler(error: Exception) -> str:\n    if isinstance(error, StructuredOutputValidationError):\n        return \"There was an issue with the format. Try again.\n    elif isinstance(error, MultipleStructuredOutputsError):\n        return \"Multiple structured outputs were returned. Pick the most relevant one.\"\n    else:\n        return f\"Error: {str(error)}\"\n\nToolStrategy(\n    schema=ToolStrategy(Union[ContactInfo, EventDetails]),\n    handle_errors=custom_error_handler\n)\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "StructuredOutputValidationError"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================= Tool Message =================================\nName: ToolStrategy\n\nThere was an issue with the format. Try again.\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "MultipleStructuredOutputsError"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================= Tool Message =================================\nName: ToolStrategy\n\nMultiple structured outputs were returned. Pick the most relevant one.\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "================================= Tool Message =================================\nName: ToolStrategy\n\nError: <error message>\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "code",
        "content": "response_format = ToolStrategy(\n    schema=ProductRating,\n    handle_errors=False  # All errors raised\n)\n"
    },
    {
        "title": "​Tool calling strategy",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Guardrails",
        "type": "text",
        "content": "Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent’s execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\n\nCommon use cases include:\n\nYou can implement guardrails using middleware to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\n\nGuardrails can be implemented using two complementary approaches:"
    },
    {
        "title": "Deterministic guardrails",
        "type": "text",
        "content": "Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations."
    },
    {
        "title": "Model-based guardrails",
        "type": "text",
        "content": "Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\n\nLangChain provides both built-in guardrails (e.g., PII detection , human-in-the-loop ) and a flexible middleware system for building custom guardrails using either approach."
    },
    {
        "title": "​Built-in guardrails",
        "type": "text",
        "content": "LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\n\nPII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\n\nThe PII middleware supports multiple strategies for handling detected PII:"
    },
    {
        "title": "​Built-in guardrails",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[customer_service_tool, email_tool],\n    middleware=[\n        # Redact emails in user input before sending to model\n        PIIMiddleware(\n            \"email\",\n            strategy=\"redact\",\n            apply_to_input=True,\n        ),\n        # Mask credit cards in user input\n        PIIMiddleware(\n            \"credit_card\",\n            strategy=\"mask\",\n            apply_to_input=True,\n        ),\n        # Block API keys - raise error if detected\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n            apply_to_input=True,\n        ),\n    ],\n)\n\n# When user provides PII, it will be handled according to the strategy\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 4532-1234-5678-9010\"}]\n})\n"
    },
    {
        "title": "​Built-in guardrails",
        "type": "code",
        "content": "apply_to_tool_results"
    },
    {
        "title": "​Built-in guardrails",
        "type": "text",
        "content": "See the middleware documentation for complete details on PII detection capabilities.\n\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\n\nHuman-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact."
    },
    {
        "title": "​Built-in guardrails",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command\n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, send_email_tool, delete_database_tool],\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                # Require approval for sensitive operations\n                \"send_email\": True,\n                \"delete_database\": True,\n                # Auto-approve safe operations\n                \"search\": False,\n            }\n        ),\n    ],\n    # Persist the state across interrupts\n    checkpointer=InMemorySaver(),\n)\n\n# Human-in-the-loop requires a thread ID for persistence\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Agent will pause and wait for approval before executing sensitive tools\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n    config=config\n)\n\nresult = agent.invoke(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config  # Same thread ID to resume the paused conversation\n)\n"
    },
    {
        "title": "​Built-in guardrails",
        "type": "text",
        "content": "See the human-in-the-loop documentation for complete details on implementing approval workflows."
    },
    {
        "title": "​Custom guardrails",
        "type": "text",
        "content": "For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\n\nUse “before agent” hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins."
    },
    {
        "title": "​Custom guardrails",
        "type": "code",
        "content": "from typing import Any\n\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\n\nclass ContentFilterMiddleware(AgentMiddleware):\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n\n    def __init__(self, banned_keywords: list[str]):\n        super().__init__()\n        self.banned_keywords = [kw.lower() for kw in banned_keywords]\n\n    @hook_config(can_jump_to=[\"end\"])\n    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        # Get the first user message\n        if not state[\"messages\"]:\n            return None\n\n        first_message = state[\"messages\"][0]\n        if first_message.type != \"human\":\n            return None\n\n        content = first_message.content.lower()\n\n        # Check for banned keywords\n        for keyword in self.banned_keywords:\n            if keyword in content:\n                # Block execution before any processing\n                return {\n                    \"messages\": [{\n                        \"role\": \"assistant\",\n                        \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                    }],\n                    \"jump_to\": \"end\"\n                }\n\n        return None\n\n# Use the custom guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, calculator_tool],\n    middleware=[\n        ContentFilterMiddleware(\n            banned_keywords=[\"hack\", \"exploit\", \"malware\"]\n        ),\n    ],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n})\n"
    },
    {
        "title": "​Custom guardrails",
        "type": "text",
        "content": "Use “after agent” hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response."
    },
    {
        "title": "​Custom guardrails",
        "type": "code",
        "content": "from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\nfrom langchain_core.messages import AIMessage\nfrom langchain.chat_models import init_chat_model\nfrom typing import Any\n\nclass SafetyGuardrailMiddleware(AgentMiddleware):\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.safety_model = init_chat_model(\"gpt-4o-mini\")\n\n    @hook_config(can_jump_to=[\"end\"])\n    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        # Get the final AI response\n        if not state[\"messages\"]:\n            return None\n\n        last_message = state[\"messages\"][-1]\n        if not isinstance(last_message, AIMessage):\n            return None\n\n        # Use a model to evaluate safety\n        safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n        Respond with only 'SAFE' or 'UNSAFE'.\n\n        Response: {last_message.content}\"\"\"\n\n        result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n        if \"UNSAFE\" in result.content:\n            return {\n                \"messages\": [{\n                    \"role\": \"assistant\",\n                    \"content\": \"I cannot provide that response. Please rephrase your request.\"\n                }],\n                \"jump_to\": \"end\"\n            }\n\n        return None\n\n# Use the safety guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, calculator_tool],\n    middleware=[SafetyGuardrailMiddleware()],\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n})\n"
    },
    {
        "title": "​Custom guardrails",
        "type": "text",
        "content": "You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:"
    },
    {
        "title": "​Custom guardrails",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, send_email_tool],\n    middleware=[\n        # Layer 1: Deterministic input filter (before agent)\n        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\n\n        # Layer 2: PII protection (before and after model)\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n\n        # Layer 3: Human approval for sensitive tools\n        HumanInTheLoopMiddleware(interrupt_on={\"send_email\": True}),\n\n        # Layer 4: Model-based safety check (after agent)\n        SafetyGuardrailMiddleware(),\n    ],\n)\n"
    },
    {
        "title": "​Additional resources",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "LangChain’s create_agent runs on LangGraph’s runtime under the hood."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "LangGraph exposes a Runtime object with the following information:"
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "You can access the runtime information within tools and middleware ."
    },
    {
        "title": "​Access",
        "type": "text",
        "content": "When creating an agent with create_agent , you can specify a context_schema to define the structure of the context stored in the agent Runtime ."
    },
    {
        "title": "​Access",
        "type": "text",
        "content": "When invoking the agent, pass the context argument with the relevant configuration for the run:"
    },
    {
        "title": "​Access",
        "type": "code",
        "content": "from dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_name: str\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    context_schema=Context  \n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")  \n)\n"
    },
    {
        "title": "​Access",
        "type": "text",
        "content": "You can access the runtime information inside tools to:\n\nUse the ToolRuntime parameter to access the Runtime object inside a tool."
    },
    {
        "title": "​Access",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime  \n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  \n    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n    user_id = runtime.context.user_id  \n\n    preferences: str = \"The user prefers you to write a brief and polite email.\"\n    if runtime.store:  \n        if memory := runtime.store.get((\"users\",), user_id):  \n            preferences = memory.value[\"preferences\"]\n\n    return preferences\n"
    },
    {
        "title": "​Access",
        "type": "text",
        "content": "You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n\nUse request.runtime to access the Runtime object inside middleware decorators. The runtime object is available in the ModelRequest parameter passed to middleware functions."
    },
    {
        "title": "​Access",
        "type": "code",
        "content": "from dataclasses import dataclass\n\nfrom langchain.messages import AnyMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:\n    user_name: str\n\n# Dynamic prompts\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context.user_name  \n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n# Before model hook\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  \n    print(f\"Processing request for user: {runtime.context.user_name}\")  \n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  \n    print(f\"Completed request for user: {runtime.context.user_name}\")  \n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  \n    context_schema=Context\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")\n)\n"
    },
    {
        "title": "​Access",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "The hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n\nWhen agents fail, it’s usually because the LLM call inside the agent took the wrong action / didn’t do what we expected. LLMs fail for one of two reasons:\n\nMore often than not - it’s actually the second reason that causes agents to not be reliable.\n\nContext engineering is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of “right” context is the number one blocker for more reliable agents, and LangChain’s agent abstractions are uniquely designed to facilitate context engineering.\n\nNew to context engineering? Start with the conceptual overview to understand the different types of context and when to use them.\n\nA typical agent loop consists of two main steps:\n\nThis loop continues until the LLM decides to finish.\n\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps."
    },
    {
        "title": "Transient context",
        "type": "text",
        "content": "What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what’s saved in state."
    },
    {
        "title": "Persistent context",
        "type": "text",
        "content": "What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n\nThroughout this process, your agent accesses (reads / writes) different sources of data:\n\nLangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain.\n\nMiddleware allows you to hook into any step in the agent lifecycle and:\n\nThroughout this guide, you’ll see frequent use of the middleware API as a means to the context engineering end."
    },
    {
        "title": "​Model Context",
        "type": "text",
        "content": "Control what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost."
    },
    {
        "title": "Messages",
        "type": "text",
        "content": "The full list of messages (conversation history) sent to the LLM."
    },
    {
        "title": "Model",
        "type": "text",
        "content": "The actual model (including configuration) to be called."
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "Schema specification for the model’s final response.\n\nAll of these types of model context can draw from state (short-term memory), store (long-term memory), or runtime context (static configuration).\n\nThe system prompt sets the LLM’s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n\nAccess message count or conversation context from state:"
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n@dynamic_prompt\ndef state_aware_prompt(request: ModelRequest) -> str:\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)\n\n    base = \"You are a helpful assistant.\"\n\n    if message_count > 10:\n        base += \"\\nThis is a long conversation - be extra concise.\"\n\n    return base\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[state_aware_prompt]\n)\n"
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "Messages make up the prompt that is sent to the LLM.\nIt’s critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n\nInject uploaded file context from State when relevant to current query:"
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n@wrap_model_call\ndef inject_file_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Inject context about files user has uploaded this session.\"\"\"\n    # Read from State: get uploaded files metadata\n    uploaded_files = request.state.get(\"uploaded_files\", [])  \n\n    if uploaded_files:\n        # Build context about available files\n        file_descriptions = []\n        for file in uploaded_files:\n            file_descriptions.append(\n                f\"- {file['name']} ({file['type']}): {file['summary']}\"\n            )\n\n        file_context = f\"\"\"Files you have access to in this conversation:\n{chr(10).join(file_descriptions)}\n\nReference these files when answering questions.\"\"\"\n\n        # Inject file context before recent messages\n        messages = [  \n            *request.messages,\n            {\"role\": \"user\", \"content\": file_context},\n        ]\n        request = request.override(messages=messages)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[inject_file_context]\n)\n"
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "Transient vs Persistent Message Updates:\n\nThe examples above use wrap_model_call to make transient updates - modifying what messages are sent to the model for a single call without changing what’s saved in state."
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "For persistent updates that modify state (like the summarization example in Life-cycle Context ), use life-cycle hooks like before_model or after_model to permanently update the conversation history. See the middleware documentation for more details."
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "Tools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren’t just metadata—they guide the model’s reasoning about when and how to use the tool."
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from langchain.tools import tool\n\n@tool(parse_docstring=True)\ndef search_orders(\n    user_id: str,\n    status: str,\n    limit: int = 10\n) -> str:\n    \"\"\"Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.\n\n    Args:\n        user_id: Unique identifier for the user\n        status: Order status: 'pending', 'shipped', or 'delivered'\n        limit: Maximum number of results to return\n    \"\"\"\n    # Implementation here\n    pass\n"
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "Not every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\n\nEnable advanced tools only after certain conversation milestones:"
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n@wrap_model_call\ndef state_based_tools(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Filter tools based on conversation State.\"\"\"\n    # Read from State: check if user has authenticated\n    state = request.state  \n    is_authenticated = state.get(\"authenticated\", False)  \n    message_count = len(state[\"messages\"])\n\n    # Only enable sensitive tools after authentication\n    if not is_authenticated:\n        tools = [t for t in request.tools if t.name.startswith(\"public_\")]\n        request = request.override(tools=tools)  \n    elif message_count < 5:\n        # Limit tools early in conversation\n        tools = [t for t in request.tools if t.name != \"advanced_search\"]\n        request = request.override(tools=tools)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[public_search, private_search, advanced_search],\n    middleware=[state_based_tools]\n)\n"
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "See Dynamically selecting tools for more examples.\n\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which\nmight change during an agent run.\n\nUse different models based on conversation length from State:"
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\n\n# Initialize models once outside the middleware\nlarge_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\nstandard_model = init_chat_model(\"gpt-4o\")\nefficient_model = init_chat_model(\"gpt-4o-mini\")\n\n@wrap_model_call\ndef state_based_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Select model based on State conversation length.\"\"\"\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)  \n\n    if message_count > 20:\n        # Long conversation - use model with larger context window\n        model = large_model\n    elif message_count > 10:\n        # Medium conversation\n        model = standard_model\n    else:\n        # Short conversation - use efficient model\n        model = efficient_model\n\n    request = request.override(model=model)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[...],\n    middleware=[state_based_model]\n)\n"
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "See Dynamic model for more examples.\n\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn’t sufficient.\n\nHow it works: When you provide a schema as the response format, the model’s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to."
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from pydantic import BaseModel, Field\n\nclass CustomerSupportTicket(BaseModel):\n    \"\"\"Structured ticket information extracted from customer message.\"\"\"\n\n    category: str = Field(\n        description=\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n    )\n    priority: str = Field(\n        description=\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n    )\n    summary: str = Field(\n        description=\"One-sentence summary of the customer's issue\"\n    )\n    customer_sentiment: str = Field(\n        description=\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n    )\n"
    },
    {
        "title": "Response Format",
        "type": "text",
        "content": "Dynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.\n\nConfigure structured output based on conversation state:"
    },
    {
        "title": "Response Format",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom pydantic import BaseModel, Field\nfrom typing import Callable\n\nclass SimpleResponse(BaseModel):\n    \"\"\"Simple response for early conversation.\"\"\"\n    answer: str = Field(description=\"A brief answer\")\n\nclass DetailedResponse(BaseModel):\n    \"\"\"Detailed response for established conversation.\"\"\"\n    answer: str = Field(description=\"A detailed answer\")\n    reasoning: str = Field(description=\"Explanation of reasoning\")\n    confidence: float = Field(description=\"Confidence score 0-1\")\n\n@wrap_model_call\ndef state_based_output(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Select output format based on State.\"\"\"\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)  \n\n    if message_count < 3:\n        # Early conversation - use simple format\n        request = request.override(response_format=SimpleResponse)  \n    else:\n        # Established conversation - use detailed format\n        request = request.override(response_format=DetailedResponse)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[state_based_output]\n)\n"
    },
    {
        "title": "​Tool Context",
        "type": "text",
        "content": "Tools are special in that they both read and write context.\n\nIn the most basic case, when a tool executes, it receives the LLM’s request parameters and returns a tool message back. The tool does its work and produces a result.\n\nTools can also fetch important information for the model that allows it to perform and complete tasks.\n\nMost real-world tools need more than just the LLM’s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\n\nRead from State to check current session information:"
    },
    {
        "title": "​Tool Context",
        "type": "code",
        "content": "from langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent\n\n@tool\ndef check_authentication(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Check if user is authenticated.\"\"\"\n    # Read from State: check current auth status\n    current_state = runtime.state\n    is_authenticated = current_state.get(\"authenticated\", False)\n\n    if is_authenticated:\n        return \"User is authenticated\"\n    else:\n        return \"User is not authenticated\"\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[check_authentication]\n)\n"
    },
    {
        "title": "​Tool Context",
        "type": "text",
        "content": "Tool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\n\nWrite to State to track session-specific information using Command:"
    },
    {
        "title": "​Tool Context",
        "type": "code",
        "content": "from langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent\nfrom langgraph.types import Command\n\n@tool\ndef authenticate_user(\n    password: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Authenticate user and update State.\"\"\"\n    # Perform authentication (simplified)\n    if password == \"correct\":\n        # Write to State: mark as authenticated using Command\n        return Command(\n            update={\"authenticated\": True},\n        )\n    else:\n        return Command(update={\"authenticated\": False})\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[authenticate_user]\n)\n"
    },
    {
        "title": "​Tool Context",
        "type": "text",
        "content": "See Tools for comprehensive examples of accessing state, store, and runtime context in tools."
    },
    {
        "title": "​Life-cycle Context",
        "type": "text",
        "content": "Control what happens between the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\n\nAs you’ve seen in Model Context and Tool Context , middleware is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\n\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context , summarization persistently updates state - permanently replacing old messages with a summary that’s saved for all future turns.\n\nLangChain offers built-in middleware for this:"
    },
    {
        "title": "​Life-cycle Context",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n            messages_to_keep=20,  # Keep last 20 messages after summary\n        ),\n    ],\n)\n"
    },
    {
        "title": "​Life-cycle Context",
        "type": "text",
        "content": "When the conversation exceeds the token limit, SummarizationMiddleware automatically:"
    },
    {
        "title": "​Life-cycle Context",
        "type": "code",
        "content": "SummarizationMiddleware"
    },
    {
        "title": "​Life-cycle Context",
        "type": "text",
        "content": "The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\n\nFor a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation ."
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "SummarizationMiddleware"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "LLMToolSelectorMiddleware"
    },
    {
        "title": "​Related resources",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Model Context Protocol (MCP)",
        "type": "text",
        "content": "Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the langchain-mcp-adapters library."
    },
    {
        "title": "Model Context Protocol (MCP)",
        "type": "code",
        "content": "langchain-mcp-adapters"
    },
    {
        "title": "​Install",
        "type": "text",
        "content": "Install the langchain-mcp-adapters library to use MCP tools in LangGraph:"
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "langchain-mcp-adapters"
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "pip install langchain-mcp-adapters\n"
    },
    {
        "title": "​Transport types",
        "type": "text",
        "content": "MCP supports different transport mechanisms for client-server communication:"
    },
    {
        "title": "​Use MCP tools",
        "type": "text",
        "content": "langchain-mcp-adapters enables agents to use tools defined across one or more MCP server."
    },
    {
        "title": "​Use MCP tools",
        "type": "code",
        "content": "langchain-mcp-adapters"
    },
    {
        "title": "​Use MCP tools",
        "type": "code",
        "content": "from langchain_mcp_adapters.client import MultiServerMCPClient  \nfrom langchain.agents import create_agent\n\n\nclient = MultiServerMCPClient(  \n    {\n        \"math\": {\n            \"transport\": \"stdio\",  # Local subprocess communication\n            \"command\": \"python\",\n            # Absolute path to your math_server.py file\n            \"args\": [\"/path/to/math_server.py\"],\n        },\n        \"weather\": {\n            \"transport\": \"streamable_http\",  # HTTP-based remote server\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n\ntools = await client.get_tools()  \nagent = create_agent(\n    \"claude-sonnet-4-5-20250929\",\n    tools  \n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n"
    },
    {
        "title": "​Use MCP tools",
        "type": "text",
        "content": "MultiServerMCPClient is stateless by default . Each tool invocation creates a fresh MCP ClientSession , executes the tool, and then cleans up."
    },
    {
        "title": "​Custom MCP servers",
        "type": "text",
        "content": "To create your own MCP servers, you can use the mcp library. This library provides a simple way to define tools and run them as servers."
    },
    {
        "title": "​Custom MCP servers",
        "type": "text",
        "content": "Use the following reference implementations to test your agent with MCP tool servers."
    },
    {
        "title": "​Custom MCP servers",
        "type": "code",
        "content": "from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n"
    },
    {
        "title": "​Custom MCP servers",
        "type": "code",
        "content": "from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -> str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n"
    },
    {
        "title": "​Stateful tool usage",
        "type": "text",
        "content": "For stateful servers that maintain context between tool calls, use client.session() to create a persistent ClientSession ."
    },
    {
        "title": "​Stateful tool usage",
        "type": "code",
        "content": "from langchain_mcp_adapters.tools import load_mcp_tools\n\nclient = MultiServerMCPClient({...})\nasync with client.session(\"math\") as session:\n    tools = await load_mcp_tools(session)\n"
    },
    {
        "title": "​Additional resources",
        "type": "code",
        "content": "langchain-mcp-adapters"
    },
    {
        "title": "​Additional resources",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Human-in-the-loop",
        "type": "text",
        "content": "The Human-in-the-Loop (HITL) middleware lets you add human oversight to agent tool calls.\nWhen a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.\n\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an interrupt that halts execution. The graph state is saved using LangGraph’s persistence layer , so execution can pause safely and resume later.\n\nA human decision then determines what happens next: the action can be approved as-is ( approve ), modified before running ( edit ), or rejected with feedback ( reject )."
    },
    {
        "title": "​Interrupt decision types",
        "type": "text",
        "content": "The middleware defines three built-in ways a human can respond to an interrupt:"
    },
    {
        "title": "​Interrupt decision types",
        "type": "text",
        "content": "The available decision types for each tool depend on the policy you configure in interrupt_on .\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request."
    },
    {
        "title": "​Interrupt decision types",
        "type": "text",
        "content": "When editing tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions."
    },
    {
        "title": "​Configuring interrupts",
        "type": "text",
        "content": "To use HITL, add the middleware to the agent’s middleware list when creating the agent."
    },
    {
        "title": "​Configuring interrupts",
        "type": "text",
        "content": "You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping."
    },
    {
        "title": "​Configuring interrupts",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \nfrom langgraph.checkpoint.memory import InMemorySaver \n\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n    middleware=[\n        HumanInTheLoopMiddleware( \n            interrupt_on={\n                \"write_file\": True,  # All decisions (approve, edit, reject) allowed\n                \"execute_sql\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing allowed\n                # Safe operation, no approval needed\n                \"read_data\": False,\n            },\n            # Prefix for interrupt messages - combined with tool name and args to form the full message\n            # e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n            # Individual tools can override this by specifying a \"description\" in their interrupt config\n            description_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    # Human-in-the-loop requires checkpointing to handle interrupts.\n    # In production, use a persistent checkpointer like AsyncPostgresSaver.\n    checkpointer=InMemorySaver(),  \n)\n"
    },
    {
        "title": "​Configuring interrupts",
        "type": "text",
        "content": "You must configure a checkpointer to persist the graph state across interrupts.\nIn production, use a persistent checkpointer like AsyncPostgresSaver . For testing or prototyping, use InMemorySaver ."
    },
    {
        "title": "​Configuring interrupts",
        "type": "text",
        "content": "When invoking the agent, pass a config that includes the thread ID to associate execution with a conversation thread.\nSee the LangGraph interrupts documentation for details."
    },
    {
        "title": "​Responding to interrupts",
        "type": "text",
        "content": "When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in interrupt_on . In that case, the invocation result will include an __interrupt__ field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided."
    },
    {
        "title": "​Responding to interrupts",
        "type": "code",
        "content": "from langgraph.types import Command\n\n# Human-in-the-loop leverages LangGraph's persistence layer.\n# You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}} \n# Run the graph until the interrupt is hit.\nresult = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Delete old records from the database\",\n            }\n        ]\n    },\n    config=config \n)\n\n# The interrupt contains the full HITL request with action_requests and review_configs\nprint(result['__interrupt__'])\n# > [\n# >    Interrupt(\n# >       value={\n# >          'action_requests': [\n# >             {\n# >                'name': 'execute_sql',\n# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n# >             }\n# >          ],\n# >          'review_configs': [\n# >             {\n# >                'action_name': 'execute_sql',\n# >                'allowed_decisions': ['approve', 'reject']\n# >             }\n# >          ]\n# >       }\n# >    )\n# > ]\n\n\n# Resume with approval decision\nagent.invoke(\n    Command( \n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"edit\", \"reject\"\n    ), \n    config=config # Same thread ID to resume the paused conversation\n)\n"
    },
    {
        "title": "​Responding to interrupts",
        "type": "text",
        "content": "Use approve to approve the tool call as-is and execute it without changes."
    },
    {
        "title": "​Responding to interrupts",
        "type": "code",
        "content": "agent.invoke(\n    Command(\n        # Decisions are provided as a list, one per action under review.\n        # The order of decisions must match the order of actions\n        # listed in the `__interrupt__` request.\n        resume={\n            \"decisions\": [\n                {\n                    \"type\": \"approve\",\n                }\n            ]\n        }\n    ),\n    config=config  # Same thread ID to resume the paused conversation\n)\n"
    },
    {
        "title": "​Execution lifecycle",
        "type": "text",
        "content": "The middleware defines an after_model hook that runs after the model generates a response but before any tool calls are executed:"
    },
    {
        "title": "​Custom HITL logic",
        "type": "text",
        "content": "For more specialized workflows, you can build custom HITL logic directly using the interrupt primitive and middleware abstraction.\n\nReview the execution lifecycle above to understand how to integrate interrupts into the agent’s operation.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Multi-agent",
        "type": "text",
        "content": "Multi-agent systems break a complex application into multiple specialized agents that work together to solve problems.\nInstead of relying on a single agent to handle every step, multi-agent architectures allow you to compose smaller, focused agents into a coordinated workflow.\n\nMulti-agent systems are useful when:"
    },
    {
        "title": "Tutorial: Build a supervisor agent",
        "type": "text",
        "content": "Learn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents.\nThis tutorial demonstrates:"
    },
    {
        "title": "​Choosing a pattern",
        "type": "text",
        "content": "You can mix both patterns — use handoffs for agent switching, and have each agent call subagents as tools for specialized tasks."
    },
    {
        "title": "​Customizing agent context",
        "type": "text",
        "content": "At the heart of multi-agent design is context engineering - deciding what information each agent sees. LangChain gives you fine-grained control over:\n\nThe quality of your system heavily depends on context engineering. The goal is to ensure that each agent has access to the correct data it needs to perform its task, whether it’s acting as a tool or as an active agent."
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "In tool calling , one agent (the “ controller ”) treats other agents as tools to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results.\n\nFlow:\n\nAgents used as tools are generally not expected to continue conversation with the user.\nTheir role is to perform a task and return results to the controller agent.\nIf you need subagents to be able to converse with the user, use handoffs instead.\n\nBelow is a minimal example where the main agent is given access to a single subagent via a tool definition:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "from langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\nsubagent1 = create_agent(model=\"...\", tools=[...])\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(query: str):\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return result[\"messages\"][-1].content\n\nagent = create_agent(model=\"...\", tools=[call_subagent1])\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "There are several points where you can control how context is passed between the main agent and its subagents:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "\"subagent1_description\""
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "There are two main levers to control the input that the main agent passes to a subagent:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "from langchain.agents import AgentState\nfrom langchain.tools import tool, ToolRuntime\n\nclass CustomState(AgentState):\n    example_state_key: str\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(query: str, runtime: ToolRuntime[None, CustomState]):\n    # Apply any logic needed to transform the messages into a suitable input\n    subagent_input = some_logic(query, runtime.state[\"messages\"])\n    result = subagent1.invoke({\n        \"messages\": subagent_input,\n        # You could also pass other state keys here as needed.\n        # Make sure to define these in both the main and subagent's\n        # state schemas.\n        \"example_state_key\": runtime.state[\"example_state_key\"]\n    })\n    return result[\"messages\"][-1].content\n"
    },
    {
        "title": "​Tool calling",
        "type": "text",
        "content": "Two common strategies for shaping what the main agent receives back from a subagent:"
    },
    {
        "title": "​Tool calling",
        "type": "code",
        "content": "from typing import Annotated\nfrom langchain.agents import AgentState\nfrom langchain.tools import InjectedToolCallId\nfrom langgraph.types import Command\n\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\n# We need to pass the `tool_call_id` to the sub agent so it can use it to respond with the tool call result\ndef call_subagent1(\n    query: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n# You need to return a `Command` object to include more than just a final tool call\n) -> Command:\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return Command(update={\n        # This is the example state key we are passing back\n        \"example_state_key\": result[\"example_state_key\"],\n        \"messages\": [\n            ToolMessage(\n                content=result[\"messages\"][-1].content,\n                # We need to include the tool call id so it matches up with the right tool call\n                tool_call_id=tool_call_id\n            )\n        ]\n    })\n"
    },
    {
        "title": "​Handoffs",
        "type": "text",
        "content": "In handoffs , agents can directly pass control to each other. The “active” agent changes, and the user interacts with whichever agent currently has control.\n\nFlow:\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Retrieval",
        "type": "text",
        "content": "Large language models (LLMs) are powerful, but they have two key limitations:\n\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of Retrieval-Augmented Generation (RAG) : enhancing an LLM’s answers with context-specific information."
    },
    {
        "title": "​Building a knowledge base",
        "type": "text",
        "content": "A knowledge base is a repository of documents or structured data used during retrieval.\n\nIf you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.\n\nIf you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\n\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:"
    },
    {
        "title": "Tutorial: Semantic search",
        "type": "text",
        "content": "Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\nIn this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\n\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they integrate retrieval with generation to produce grounded, context-aware answers.\n\nThis is the core idea behind Retrieval-Augmented Generation (RAG) . The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\n\nA typical retrieval workflow looks like this:\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic."
    },
    {
        "title": "Document loaders",
        "type": "text",
        "content": "Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized Document objects."
    },
    {
        "title": "Text splitters",
        "type": "text",
        "content": "Break large docs into smaller chunks that will be retrievable individually and fit within a model’s context window."
    },
    {
        "title": "Embedding models",
        "type": "text",
        "content": "An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space."
    },
    {
        "title": "Vector stores",
        "type": "text",
        "content": "Specialized databases for storing and searching embeddings."
    },
    {
        "title": "Retrievers",
        "type": "text",
        "content": "A retriever is an interface that returns documents given an unstructured query."
    },
    {
        "title": "​RAG Architectures",
        "type": "text",
        "content": "RAG can be implemented in multiple ways, depending on your system’s needs. We outline each type in the sections below.\n\nLatency : Latency is generally more predictable in 2-Step RAG , as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.\n\nIn 2-Step RAG , the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer."
    },
    {
        "title": "Tutorial: Retrieval-Augmented Generation (RAG)",
        "type": "text",
        "content": "See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\n\nAgentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.\n\nThe only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge — such as documentation loaders, web APIs, or database queries."
    },
    {
        "title": "Tutorial: Retrieval-Augmented Generation (RAG)",
        "type": "code",
        "content": "import requests\nfrom langchain.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom langchain.agents import create_agent\n\n\n@tool\ndef fetch_url(url: str) -> str:\n    \"\"\"Fetch text content from a URL\"\"\"\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.text\n\nsystem_prompt = \"\"\"\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n\"\"\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[fetch_url], # A tool for retrieval\n    system_prompt=system_prompt,\n)\n"
    },
    {
        "title": "Tutorial: Retrieval-Augmented Generation (RAG)",
        "type": "text",
        "content": "This example implements an Agentic RAG system to assist users in querying LangGraph documentation. The agent begins by loading llms.txt , which lists available documentation URLs, and can then dynamically use a fetch_documentation tool to retrieve and process the relevant content based on the user’s question."
    },
    {
        "title": "Tutorial: Retrieval-Augmented Generation (RAG)",
        "type": "text",
        "content": "See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\n\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n\nTypical components include:\n\nThe architecture often supports multiple iterations between these steps:\n\nThis architecture is suitable for:"
    },
    {
        "title": "Tutorial: Agentic RAG with Self-Correction",
        "type": "text",
        "content": "An example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "LangChain agents use LangGraph persistence to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use."
    },
    {
        "title": "​Memory storage",
        "type": "text",
        "content": "LangGraph stores long-term memories as JSON documents in a store .\n\nEach memory is organized under a custom namespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information."
    },
    {
        "title": "​Memory storage",
        "type": "text",
        "content": "This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters."
    },
    {
        "title": "​Memory storage",
        "type": "code",
        "content": "from langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) \nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context) \nstore.put( \n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\") \n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search( \n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n"
    },
    {
        "title": "​Memory storage",
        "type": "text",
        "content": "For more information about the memory store, see the Persistence guide."
    },
    {
        "title": "​Read long-term memory in tools",
        "type": "code",
        "content": "from dataclasses import dataclass\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n@dataclass\nclass Context:\n    user_id: str\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() \n\n# Write sample data to the store using the put method\nstore.put( \n    (\"users\",),  # Namespace to group related data together (users namespace for user data)\n    \"user_123\",  # Key within the namespace (user ID as key)\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    }  # Data to store for the given user\n)\n\n@tool\ndef get_user_info(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store \n    user_id = runtime.context.user_id\n    # Retrieve data from store - returns StoreValue object with value and metadata\n    user_info = store.get((\"users\",), user_id) \n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_user_info],\n    # Pass store to agent - enables agent to access store when running tools\n    store=store, \n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    context=Context(user_id=\"user_123\") \n)\n"
    },
    {
        "title": "​Write long-term memory from tools",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() \n\n@dataclass\nclass Context:\n    user_id: str\n\n# TypedDict defines the structure of user information for the LLM\nclass UserInfo(TypedDict):\n    name: str\n\n# Tool that allows agent to update user information (useful for chat applications)\n@tool\ndef save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store \n    user_id = runtime.context.user_id \n    # Store data in the store (namespace, key, data)\n    store.put((\"users\",), user_id, user_info) \n    return \"Successfully saved user info.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[save_user_info],\n    store=store, \n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # user_id passed in context to identify whose information is being updated\n    context=Context(user_id=\"user_123\") \n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n"
    },
    {
        "title": "​Write long-term memory from tools",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Studio",
        "type": "text",
        "content": "This guide will walk you through how to use Studio to visualize, interact, and debug your agent locally.\n\nStudio is our free-to-use, powerful agent IDE that integrates with LangSmith to enable tracing, evaluation, and prompt engineering. See exactly how your agent thinks, trace every decision, and ship smarter, more reliable agents."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "# Python >= 3.11 is required.\npip install --upgrade \"langgraph-cli[inmem]\"\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "We’ll use the following simple agent as an example:"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email\"\"\"\n    email = {\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body\n    }\n    # ... email sending logic\n\n    return f\"Email sent to {to}\"\n\nagent = create_agent(\n    \"gpt-4o\",\n    tools=[send_email],\n    system_prompt=\"You are an email assistant. Always use the send_email tool.\",\n)\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Create a .env file in the root of your project and fill in the necessary API keys. We’ll need to set the LANGSMITH_API_KEY environment variable to the API key you get from LangSmith ."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Be sure not to commit your .env to version control systems such as Git!"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "LANGSMITH_API_KEY=lsv2...\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Inside your app’s directory, create a configuration file langgraph.json :"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.py:agent\"\n  },\n  \"env\": \".env\"\n}\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "create_agent automatically returns a compiled LangGraph graph that we can pass to the graphs key in our configuration file."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "See the LangGraph configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\n\nSo far, our project structure looks like this:"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "my-app/\n├── src\n│   └── agent.py\n├── .env\n└── langgraph.json\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "In the root of your new LangGraph app, install the dependencies:"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Safari blocks localhost connections to Studio. To work around this, run the above command with --tunnel to access Studio via a secure tunnel."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Your agent will be accessible via API ( http://127.0.0.1:2024 ) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 :"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "http://127.0.0.1:2024"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Studio makes each step of your agent easily observable. Replay any input and inspect the exact prompt, tool arguments, return values, and token/latency metrics. If a tool throws an exception, Studio records it with surrounding state so you can spend less time debugging.\n\nKeep your dev server running, edit prompts or tool signatures, and watch Studio hot-reload. Re-run the conversation thread from any step to verify behavior changes. See Manage threads for more details.\n\nAs your agent grows, the same view scales from a single-tool demo to multi-node graphs, keeping decisions legible and reproducible.\n\nFor an in-depth look at Studio, check out the overview page .\n\nFor more information about local and deployed agents, see Set up local LangGraph Server and Deploy .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Test",
        "type": "text",
        "content": "Agentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model’s black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.\n\nThere are a few approaches to testing your agents:\n\nUnit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n\nIntegration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\n\nAgentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs."
    },
    {
        "title": "​Unit Testing",
        "type": "text",
        "content": "For logic not requiring API calls, you can use an in-memory stub for mocking responses.\n\nLangChain provides GenericFakeChatModel for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage."
    },
    {
        "title": "​Unit Testing",
        "type": "code",
        "content": "from langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n\nmodel = GenericFakeChatModel(messages=iter([\n    AIMessage(content=\"\", tool_calls=[ToolCall(name=\"foo\", args={\"bar\": \"baz\"}, id=\"call_1\")]),\n    \"bar\"\n]))\n\nmodel.invoke(\"hello\")\n# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])\n"
    },
    {
        "title": "​Unit Testing",
        "type": "text",
        "content": "If we invoke the model again, it will return the next item in the iterator:"
    },
    {
        "title": "​Unit Testing",
        "type": "code",
        "content": "model.invoke(\"hello, again!\")\n# AIMessage(content='bar', ...)\n"
    },
    {
        "title": "​Unit Testing",
        "type": "text",
        "content": "To enable persistence during testing, you can use the InMemorySaver checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:"
    },
    {
        "title": "​Unit Testing",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=[],\n    checkpointer=InMemorySaver()\n)\n\n# First invocation\nagent.invoke(HumanMessage(content=\"I live in Sydney, Australia.\"))\n\n# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time\nagent.invoke(HumanMessage(content=\"What's my local time?\"))\n"
    },
    {
        "title": "​Integration Testing",
        "type": "text",
        "content": "Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain’s agentevals package provides evaluators specifically designed for testing agent trajectories with live models."
    },
    {
        "title": "​Integration Testing",
        "type": "text",
        "content": "AgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a trajectory match or by using an LLM judge :"
    },
    {
        "title": "Trajectory match",
        "type": "text",
        "content": "Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.\n\nIdeal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn’t require additional LLM calls."
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "Use a LLM to qualitatively validate your agent’s execution trajectory. The “judge” LLM reviews the agent’s decisions against a prompt rubric (which can include a reference trajectory).\n\nMore flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent’s trajectory without strict tool call or ordering requirements."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "pip install agentevals\n"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "Or, clone the AgentEvals repository directly.\n\nAgentEvals offers the create_trajectory_match_evaluator function to match your agent’s trajectory against a reference trajectory. There are four modes to choose from:"
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "create_trajectory_match_evaluator"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "The strict mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\nagent = create_agent(\"gpt-4o\", tools=[get_weather])\n\nevaluator = create_trajectory_match_evaluator(  \n    trajectory_match_mode=\"strict\",  \n)  \n\ndef test_weather_tool_called_strict():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]\n    })\n\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in San Francisco?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"),\n    ]\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory\n    )\n    # {\n    #     'key': 'trajectory_strict_match',\n    #     'score': True,\n    #     'comment': None,\n    # }\n    assert evaluation[\"score\"] is True\n"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "The unordered mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don’t care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn’t matter."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\n@tool\ndef get_events(city: str):\n    \"\"\"Get events happening in a city.\"\"\"\n    return f\"Concert at the park in {city} tonight.\"\n\nagent = create_agent(\"gpt-4o\", tools=[get_weather, get_events])\n\nevaluator = create_trajectory_match_evaluator(  \n    trajectory_match_mode=\"unordered\",  \n)  \n\ndef test_multiple_tools_any_order():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's happening in SF today?\")]\n    })\n\n    # Reference shows tools called in different order than actual execution\n    reference_trajectory = [\n        HumanMessage(content=\"What's happening in SF today?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_events\", \"args\": {\"city\": \"SF\"}},\n            {\"id\": \"call_2\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n        ]),\n        ToolMessage(content=\"Concert at the park in SF tonight.\", tool_call_id=\"call_1\"),\n        ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_2\"),\n        AIMessage(content=\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n    ]\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory,\n    )\n    # {\n    #     'key': 'trajectory_unordered_match',\n    #     'score': True,\n    # }\n    assert evaluation[\"score\"] is True\n"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "The superset and subset modes match partial trajectories. The superset mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The subset mode ensures the agent did not call any tools beyond those in the reference."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\n@tool\ndef get_detailed_forecast(city: str):\n    \"\"\"Get detailed weather forecast for a city.\"\"\"\n    return f\"Detailed forecast for {city}: sunny all week.\"\n\nagent = create_agent(\"gpt-4o\", tools=[get_weather, get_detailed_forecast])\n\nevaluator = create_trajectory_match_evaluator(  \n    trajectory_match_mode=\"superset\",  \n)  \n\ndef test_agent_calls_required_tools_plus_extra():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in Boston?\")]\n    })\n\n    # Reference only requires get_weather, but agent may call additional tools\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in Boston?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"Boston\"}},\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in Boston.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in Boston is 75 degrees and sunny.\"),\n    ]\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory,\n    )\n    # {\n    #     'key': 'trajectory_superset_match',\n    #     'score': True,\n    #     'comment': None,\n    # }\n    assert evaluation[\"score\"] is True\n"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "You can also set the tool_args_match_mode property and/or tool_args_match_overrides to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "tool_args_match_overrides"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "You can also use an LLM to evaluate the agent’s execution path with the create_trajectory_llm_as_judge function. Unlike the trajectory match evaluators, it doesn’t require a reference trajectory, but one can be provided if available."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "create_trajectory_llm_as_judge"
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\nagent = create_agent(\"gpt-4o\", tools=[get_weather])\n\nevaluator = create_trajectory_llm_as_judge(  \n    model=\"openai:o3-mini\",  \n    prompt=TRAJECTORY_ACCURACY_PROMPT,  \n)  \n\ndef test_trajectory_quality():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n    })\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n    )\n    # {\n    #     'key': 'trajectory_accuracy',\n    #     'score': True,\n    #     'comment': 'The provided agent trajectory is reasonable...'\n    # }\n    assert evaluation[\"score\"] is True\n"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE prompt and configure the reference_outputs variable:"
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE"
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "evaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n)\nevaluation = judge_with_reference(\n    outputs=result[\"messages\"],\n    reference_outputs=reference_trajectory,\n)\n"
    },
    {
        "title": "LLM-as-judge",
        "type": "text",
        "content": "For more configurability over how the LLM evaluates the trajectory, visit the repository .\n\nAll agentevals evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding async after create_ in the function name."
    },
    {
        "title": "LLM-as-judge",
        "type": "code",
        "content": "from agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\nfrom agentevals.trajectory.match import create_async_trajectory_match_evaluator\n\nasync_judge = create_async_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\nasync_evaluator = create_async_trajectory_match_evaluator(\n    trajectory_match_mode=\"strict\",\n)\n\nasync def test_async_evaluation():\n    result = await agent.ainvoke({\n        \"messages\": [HumanMessage(content=\"What's the weather?\")]\n    })\n\n    evaluation = await async_judge(outputs=result[\"messages\"])\n    assert evaluation[\"score\"] is True\n"
    },
    {
        "title": "​LangSmith Integration",
        "type": "text",
        "content": "For tracking experiments over time, you can log evaluator results to LangSmith , a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.\n\nFirst, set up LangSmith by setting the required environment variables:"
    },
    {
        "title": "​LangSmith Integration",
        "type": "code",
        "content": "export LANGSMITH_API_KEY=\"your_langsmith_api_key\"\nexport LANGSMITH_TRACING=\"true\"\n"
    },
    {
        "title": "​LangSmith Integration",
        "type": "text",
        "content": "LangSmith offers two main approaches for running evaluations: pytest integration and the evaluate function."
    },
    {
        "title": "​LangSmith Integration",
        "type": "code",
        "content": "import pytest\nfrom langsmith import testing as t\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\ntrajectory_evaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\n@pytest.mark.langsmith\ndef test_trajectory_accuracy():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in SF?\")]\n    })\n\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in SF?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in SF is 75 degrees and sunny.\"),\n    ]\n\n    # Log inputs, outputs, and reference outputs to LangSmith\n    t.log_inputs({})\n    t.log_outputs({\"messages\": result[\"messages\"]})\n    t.log_reference_outputs({\"messages\": reference_trajectory})\n\n    trajectory_evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory\n    )\n"
    },
    {
        "title": "​LangSmith Integration",
        "type": "code",
        "content": "pytest test_trajectory.py --langsmith-output\n"
    },
    {
        "title": "​LangSmith Integration",
        "type": "text",
        "content": "Results will be automatically logged to LangSmith.\n\nAlternatively, you can create a dataset in LangSmith and use the evaluate function:"
    },
    {
        "title": "​LangSmith Integration",
        "type": "code",
        "content": "from langsmith import Client\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\nclient = Client()\n\ntrajectory_evaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\ndef run_agent(inputs):\n    \"\"\"Your agent function that returns trajectory messages.\"\"\"\n    return agent.invoke(inputs)[\"messages\"]\n\nexperiment_results = client.evaluate(\n    run_agent,\n    data=\"your_dataset_name\",\n    evaluators=[trajectory_evaluator]\n)\n"
    },
    {
        "title": "​LangSmith Integration",
        "type": "text",
        "content": "Results will be automatically logged to LangSmith.\n\nTo learn more about evaluating your agent, see the LangSmith docs ."
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "Integration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls.\n\nYou can use vcrpy to achieve this. If you’re using pytest , the pytest-recording plugin provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs."
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "Set up your conftest.py file to filter out sensitive information from the cassettes:"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "code",
        "content": "import pytest\n\n@pytest.fixture(scope=\"session\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\n            (\"authorization\", \"XXXX\"),\n            (\"x-api-key\", \"XXXX\"),\n            # ... other headers you want to mask\n        ],\n        \"filter_query_parameters\": [\n            (\"api_key\", \"XXXX\"),\n            (\"key\", \"XXXX\"),\n        ],\n    }\n"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "Then configure your project to recognise the vcr marker:"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "code",
        "content": "[pytest]\nmarkers =\n    vcr: record/replay HTTP via VCR\naddopts = --record-mode=once\n"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "The --record-mode=once option records HTTP interactions on the first run and replays them on subsequent runs."
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "Now, simply decorate your tests with the vcr marker:"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "code",
        "content": "@pytest.mark.vcr()\ndef test_agent_trajectory():\n    # ...\n"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "The first time you run this test, your agent will make real network calls and pytest will generate a cassette file test_agent_trajectory.yaml in the tests/cassettes directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent’s requests don’t change from the previous run. If they do, the test will fail and you’ll need to delete the cassette and rerun the test to record fresh interactions."
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "code",
        "content": "test_agent_trajectory.yaml"
    },
    {
        "title": "​Recording & Replaying HTTP Calls",
        "type": "text",
        "content": "When you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests will fail . You should delete the corresponding cassette files and rerun the tests to record fresh interactions.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Deploy",
        "type": "text",
        "content": "LangSmith is the fastest way to turn agents into production systems. Traditional hosting platforms are built for stateless, short-lived web apps, while LangGraph is purpose-built for stateful, long-running agents , so you can go from repo to reliable cloud deployment in minutes."
    },
    {
        "title": "​Deploy your agent",
        "type": "text",
        "content": "Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide . Then, push your code to the repository.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Agent Chat UI",
        "type": "text",
        "content": "LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using create_agent . This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith )."
    },
    {
        "title": "​Agent Chat UI",
        "type": "text",
        "content": "Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking.\n\nAgent Chat UI is open source and can be adapted to your application needs.\n\nStudio automatically renders tool calls and results in an intuitive interface.\n\nNavigate through conversation history and fork from any point\n\nView and modify agent state at any point during execution\n\nBuilt-in support for reviewing and responding to agent requests\n\nYou can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph .\n\nThe fastest way to get started is using the hosted version:\n\nFor customization or local development, you can run Agent Chat UI locally:"
    },
    {
        "title": "​Agent Chat UI",
        "type": "code",
        "content": "# Create a new Agent Chat UI project\nnpx create-agent-chat-app --project-name my-chat-ui\ncd my-chat-ui\n\n# Install dependencies and start\npnpm install\npnpm dev\n"
    },
    {
        "title": "​Agent Chat UI",
        "type": "text",
        "content": "Agent Chat UI can connect to both local and deployed agents .\n\nAfter starting Agent Chat UI, you’ll need to configure it to connect to your agent:"
    },
    {
        "title": "​Agent Chat UI",
        "type": "code",
        "content": "http://localhost:2024"
    },
    {
        "title": "​Agent Chat UI",
        "type": "text",
        "content": "Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n\nAgent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "LangGraph overview",
        "type": "text",
        "content": "LangGraph v1.0 is now available!\n\nFor a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\n\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\n\nTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent orchestration . Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools .\n\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more."
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "pip install -U langgraph\n"
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "from langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n"
    },
    {
        "title": "​Core benefits",
        "type": "text",
        "content": "LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:"
    },
    {
        "title": "​LangGraph ecosystem",
        "type": "text",
        "content": "While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:"
    },
    {
        "title": "​Acknowledgements",
        "type": "text",
        "content": "LangGraph is inspired by Pregel and Apache Beam . The public interface draws inspiration from NetworkX . LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "LangGraph overview",
        "type": "text",
        "content": "LangGraph v1.0 is now available!\n\nFor a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide .\n\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content .\n\nTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent orchestration . Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools .\n\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more."
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "pip install -U langgraph\n"
    },
    {
        "title": "​Install",
        "type": "code",
        "content": "from langgraph.graph import StateGraph, MessagesState, START, END\n\ndef mock_llm(state: MessagesState):\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(START, \"mock_llm\")\ngraph.add_edge(\"mock_llm\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n"
    },
    {
        "title": "​Core benefits",
        "type": "text",
        "content": "LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:"
    },
    {
        "title": "​LangGraph ecosystem",
        "type": "text",
        "content": "While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:"
    },
    {
        "title": "​Acknowledgements",
        "type": "text",
        "content": "LangGraph is inspired by Pregel and Apache Beam . The public interface draws inspiration from NetworkX . LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "What's new in v1",
        "type": "text",
        "content": "LangGraph v1 is a stability-focused release for the agent runtime. It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.\n\nIt’s designed to work hand-in-hand with LangChain v1 (whose create_agent is built on LangGraph) so you can start high-level and drop down to granular control when needed."
    },
    {
        "title": "Stable core APIs",
        "type": "text",
        "content": "Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward."
    },
    {
        "title": "Reliability, by default",
        "type": "text",
        "content": "Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class."
    },
    {
        "title": "Seamless with LangChain v1",
        "type": "text",
        "content": "LangChain’s create_agent runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration."
    },
    {
        "title": "Seamless with LangChain v1",
        "type": "code",
        "content": "pip install -U langgraph\n"
    },
    {
        "title": "​Deprecation ofcreate_react_agent",
        "type": "text",
        "content": "The LangGraph create_react_agent prebuilt has been deprecated in favor of LangChain’s create_agent . It provides a simpler interface, and offers greater customization potential through the introduction of middleware."
    },
    {
        "title": "​Reporting issues",
        "type": "text",
        "content": "Please report any issues discovered with 1.0 on GitHub using the 'v1' label ."
    },
    {
        "title": "​See also",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "LangGraph v1 migration guide",
        "type": "text",
        "content": "This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the what’s new page.\n\nTo upgrade:"
    },
    {
        "title": "LangGraph v1 migration guide",
        "type": "code",
        "content": "pip install -U langgraph langchain-core\n"
    },
    {
        "title": "​Summary of changes",
        "type": "text",
        "content": "LangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of create_react_agent in favor of LangChain’s new create_agent function."
    },
    {
        "title": "​Deprecations",
        "type": "text",
        "content": "The following table lists all items deprecated in LangGraph v1:"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.create_agent"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.AgentState"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.AgentState"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "AgentStateWithStructuredResponse"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.AgentState"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "AgentStateWithStructuredResponsePydantic"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.AgentState"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.middleware.human_in_the_loop.InterruptOnConfig"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.middleware.human_in_the_loop.InterruptOnConfig"
    },
    {
        "title": "​Deprecations",
        "type": "code",
        "content": "langchain.agents.middleware.human_in_the_loop.HITLRequest"
    },
    {
        "title": "​create_react_agent→create_agent",
        "type": "text",
        "content": "LangGraph v1 deprecates the create_react_agent prebuilt. Use LangChain’s create_agent , which runs on LangGraph and adds a flexible middleware system."
    },
    {
        "title": "​create_react_agent→create_agent",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\nagent = create_agent(  \n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant.\",\n)\n"
    },
    {
        "title": "​Breaking changes",
        "type": "text",
        "content": "All LangChain packages now require Python 3.10 or higher . Python 3.9 reached end of life in October 2025.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Install LangGraph",
        "type": "code",
        "content": "pip install -U langgraph\n"
    },
    {
        "title": "Install LangGraph",
        "type": "text",
        "content": "To use LangGraph you will usually want to access LLMs and define tools.\nYou can do this however you see fit.\n\nOne way to do this (which we will use in the docs) is to use LangChain .\n\nInstall LangChain with:"
    },
    {
        "title": "Install LangGraph",
        "type": "code",
        "content": "pip install -U langchain\n"
    },
    {
        "title": "Install LangGraph",
        "type": "text",
        "content": "To work with specific LLM provider packages, you will need install them separately.\n\nRefer to the integrations page for provider-specific installation instructions.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Quickstart",
        "type": "text",
        "content": "This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\nFor conceptual information, see Graph API overview and Functional API overview .\n\nFor this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal."
    },
    {
        "title": "​1. Define tools and model",
        "type": "text",
        "content": "In this example, we’ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division."
    },
    {
        "title": "​1. Define tools and model",
        "type": "code",
        "content": "from langchain.tools import tool\nfrom langchain.chat_models import init_chat_model\n\n\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    temperature=0\n)\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nmodel_with_tools = model.bind_tools(tools)\n"
    },
    {
        "title": "​2. Define state",
        "type": "text",
        "content": "The graph’s state is used to store the messages and the number of LLM calls.\n\nState in LangGraph persists throughout the agent’s execution.\n\nThe Annotated type with operator.add ensures that new messages are appended to the existing list rather than replacing it."
    },
    {
        "title": "​2. Define state",
        "type": "code",
        "content": "from langchain.messages import AnyMessage\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\n\nclass MessagesState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    llm_calls: int\n"
    },
    {
        "title": "​3. Define model node",
        "type": "text",
        "content": "The model node is used to call the LLM and decide whether to call a tool or not."
    },
    {
        "title": "​3. Define model node",
        "type": "code",
        "content": "from langchain.messages import SystemMessage\n\n\ndef llm_call(state: dict):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n    return {\n        \"messages\": [\n            model_with_tools.invoke(\n                [\n                    SystemMessage(\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                    )\n                ]\n                + state[\"messages\"]\n            )\n        ],\n        \"llm_calls\": state.get('llm_calls', 0) + 1\n    }\n"
    },
    {
        "title": "​4. Define tool node",
        "type": "text",
        "content": "The tool node is used to call the tools and return the results."
    },
    {
        "title": "​4. Define tool node",
        "type": "code",
        "content": "from langchain.messages import ToolMessage\n\n\ndef tool_node(state: dict):\n    \"\"\"Performs the tool call\"\"\"\n\n    result = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool = tools_by_name[tool_call[\"name\"]]\n        observation = tool.invoke(tool_call[\"args\"])\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n    return {\"messages\": result}\n"
    },
    {
        "title": "​5. Define end logic",
        "type": "text",
        "content": "The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call."
    },
    {
        "title": "​5. Define end logic",
        "type": "code",
        "content": "from typing import Literal\nfrom langgraph.graph import StateGraph, START, END\n\n\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n\n    # If the LLM makes a tool call, then perform an action\n    if last_message.tool_calls:\n        return \"tool_node\"\n\n    # Otherwise, we stop (reply to the user)\n    return END\n"
    },
    {
        "title": "​6. Build and compile the agent",
        "type": "text",
        "content": "The agent is built using the StateGraph class and compiled using the compile method."
    },
    {
        "title": "​6. Build and compile the agent",
        "type": "code",
        "content": "# Build workflow\nagent_builder = StateGraph(MessagesState)\n\n# Add nodes\nagent_builder.add_node(\"llm_call\", llm_call)\nagent_builder.add_node(\"tool_node\", tool_node)\n\n# Add edges to connect nodes\nagent_builder.add_edge(START, \"llm_call\")\nagent_builder.add_conditional_edges(\n    \"llm_call\",\n    should_continue,\n    [\"tool_node\", END]\n)\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n# Compile the agent\nagent = agent_builder.compile()\n\n# Show the agent\nfrom IPython.display import Image, display\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n# Invoke\nfrom langchain.messages import HumanMessage\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nmessages = agent.invoke({\"messages\": messages})\nfor m in messages[\"messages\"]:\n    m.pretty_print()\n"
    },
    {
        "title": "​6. Build and compile the agent",
        "type": "text",
        "content": "To learn how to trace your agent with LangSmith, see the LangSmith documentation .\n\nCongratulations! You’ve built your first agent using the LangGraph Graph API."
    },
    {
        "title": "​6. Build and compile the agent",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Run a local server",
        "type": "text",
        "content": "This guide shows you how to run a LangGraph application locally."
    },
    {
        "title": "​1. Install the LangGraph CLI",
        "type": "code",
        "content": "# Python >= 3.11 is required.\npip install -U \"langgraph-cli[inmem]\"\n"
    },
    {
        "title": "​2. Create a LangGraph app 🌱",
        "type": "text",
        "content": "Create a new app from the new-langgraph-project-python template . This template demonstrates a single-node application you can extend with your own logic."
    },
    {
        "title": "​2. Create a LangGraph app 🌱",
        "type": "code",
        "content": "new-langgraph-project-python"
    },
    {
        "title": "​2. Create a LangGraph app 🌱",
        "type": "code",
        "content": "langgraph new path/to/your/app --template new-langgraph-project-python\n"
    },
    {
        "title": "​2. Create a LangGraph app 🌱",
        "type": "text",
        "content": "Additional templates If you use langgraph new without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates."
    },
    {
        "title": "​3. Install dependencies",
        "type": "text",
        "content": "In the root of your new LangGraph app, install the dependencies in edit mode so your local changes are used by the server:"
    },
    {
        "title": "​3. Install dependencies",
        "type": "code",
        "content": "cd path/to/your/app\npip install -e .\n"
    },
    {
        "title": "​4. Create a.envfile",
        "type": "text",
        "content": "You will find a .env.example in the root of your new LangGraph app. Create a .env file in the root of your new LangGraph app and copy the contents of the .env.example file into it, filling in the necessary API keys:"
    },
    {
        "title": "​4. Create a.envfile",
        "type": "code",
        "content": "LANGSMITH_API_KEY=lsv2...\n"
    },
    {
        "title": "​5. Launch LangGraph server 🚀",
        "type": "code",
        "content": ">    Ready!\n>\n>    - API: [http://localhost:2024](http://localhost:2024/)\n>\n>    - Docs: http://localhost:2024/docs\n>\n>    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n"
    },
    {
        "title": "​5. Launch LangGraph server 🚀",
        "type": "text",
        "content": "The langgraph dev command starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see the Platform setup overview ."
    },
    {
        "title": "​6. Test your application in Studio",
        "type": "text",
        "content": "Studio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the langgraph dev command:"
    },
    {
        "title": "​6. Test your application in Studio",
        "type": "code",
        "content": ">    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n"
    },
    {
        "title": "​6. Test your application in Studio",
        "type": "text",
        "content": "For a LangGraph Server running on a custom host/port, update the baseURL parameter.\n\nUse the --tunnel flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:"
    },
    {
        "title": "​6. Test your application in Studio",
        "type": "code",
        "content": "langgraph dev --tunnel\n"
    },
    {
        "title": "​7. Test the API",
        "type": "code",
        "content": "pip install langgraph-sdk\n"
    },
    {
        "title": "​7. Test the API",
        "type": "code",
        "content": "from langgraph_sdk import get_client\nimport asyncio\n\nclient = get_client(url=\"http://localhost:2024\")\n\nasync def main():\n    async for chunk in client.runs.stream(\n        None,  # Threadless run\n        \"agent\", # Name of assistant. Defined in langgraph.json.\n        input={\n        \"messages\": [{\n            \"role\": \"human\",\n            \"content\": \"What is LangGraph?\",\n            }],\n        },\n    ):\n        print(f\"Receiving new event of type: {chunk.event}...\")\n        print(chunk.data)\n        print(\"\\n\\n\")\n\nasyncio.run(main())\n"
    },
    {
        "title": "​Next steps",
        "type": "text",
        "content": "Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:\n\nDeployment quickstart : Deploy your LangGraph app using LangSmith.\n\nLangSmith : Learn about foundational LangSmith concepts.\n\nPython SDK Reference : Explore the Python SDK API Reference.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Thinking in LangGraph",
        "type": "text",
        "content": "LangGraph can change how you think about the agents you build. When you build an agent with LangGraph, you will first break it apart into discrete steps called nodes . Then, you will describe the different decisions and transitions for each of your nodes. Finally, you will connect your nodes together through a shared state that each node can read from and write to. In this tutorial, we’ll guide you through the thought process of building a customer support email agent with LangGraph."
    },
    {
        "title": "​Start with the process you want to automate",
        "type": "text",
        "content": "Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:\n\nThe agent should:\n\nExample scenarios to handle:\n\nTo implement an agent in LangGraph, you will usually follow the same five steps."
    },
    {
        "title": "​Step 1: Map out your workflow as discrete steps",
        "type": "text",
        "content": "Start by identifying the distinct steps in your process. Each step will become a node (a function that does one specific thing). Then sketch how these steps connect to each other.\n\nThe arrows show possible paths, but the actual decision of which path to take happens inside each node.\n\nNow that you’ve identified the components in your workflow, let’s understand what each node needs to do:\n\nNotice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply)."
    },
    {
        "title": "​Step 2: Identify what each step needs to do",
        "type": "text",
        "content": "For each node in your graph, determine what type of operation it represents and what context it needs to work properly."
    },
    {
        "title": "LLM Steps",
        "type": "text",
        "content": "Use when you need to understand, analyze, generate text, or make reasoning decisions"
    },
    {
        "title": "Data Steps",
        "type": "text",
        "content": "Use when you need to retrieve information from external sources"
    },
    {
        "title": "User Input Steps",
        "type": "text",
        "content": "Use when you need human intervention\n\nWhen a step needs to understand, analyze, generate text, or make reasoning decisions:\n\nWhen a step needs to retrieve information from external sources:\n\nWhen a step needs to perform an external action:\n\nWhen a step needs human intervention:"
    },
    {
        "title": "​Step 3: Design your state",
        "type": "text",
        "content": "State is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.\n\nAsk yourself these questions about each piece of data:"
    },
    {
        "title": "Include in State",
        "type": "text",
        "content": "Does it need to persist across steps? If yes, it goes in state."
    },
    {
        "title": "Don't Store",
        "type": "text",
        "content": "Can you derive it from other data? If yes, compute it when needed instead of storing it in state.\n\nFor our email agent, we need to track:\n\nA key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.\n\nThis separation means:\n\nLet’s define our state:"
    },
    {
        "title": "Don't Store",
        "type": "code",
        "content": "from typing import TypedDict, Literal\n\n# Define the structure for email classification\nclass EmailClassification(TypedDict):\n    intent: Literal[\"question\", \"bug\", \"billing\", \"feature\", \"complex\"]\n    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n    topic: str\n    summary: str\n\nclass EmailAgentState(TypedDict):\n    # Raw email data\n    email_content: str\n    sender_email: str\n    email_id: str\n\n    # Classification result\n    classification: EmailClassification | None\n\n    # Raw search/API results\n    search_results: list[str] | None  # List of raw document chunks\n    customer_history: dict | None  # Raw customer data from CRM\n\n    # Generated content\n    draft_response: str | None\n    messages: list[str] | None\n"
    },
    {
        "title": "Don't Store",
        "type": "text",
        "content": "Notice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM."
    },
    {
        "title": "​Step 4: Build your nodes",
        "type": "text",
        "content": "Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.\n\nDifferent errors need different handling strategies:"
    },
    {
        "title": "​Step 4: Build your nodes",
        "type": "text",
        "content": "Add a retry policy to automatically retry network issues and rate limits:"
    },
    {
        "title": "​Step 4: Build your nodes",
        "type": "code",
        "content": "from langgraph.types import RetryPolicy\n\nworkflow.add_node(\n    \"search_documentation\",\n    search_documentation,\n    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)\n)\n"
    },
    {
        "title": "​Step 4: Build your nodes",
        "type": "text",
        "content": "We’ll implement each node as a simple function. Remember: nodes take state, do work, and return updates."
    },
    {
        "title": "​Step 4: Build your nodes",
        "type": "code",
        "content": "from typing import Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import interrupt, Command, RetryPolicy\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nllm = ChatOpenAI(model=\"gpt-4\")\n\ndef read_email(state: EmailAgentState) -> dict:\n    \"\"\"Extract and parse email content\"\"\"\n    # In production, this would connect to your email service\n    return {\n        \"messages\": [HumanMessage(content=f\"Processing email: {state['email_content']}\")]\n    }\n\ndef classify_intent(state: EmailAgentState) -> Command[Literal[\"search_documentation\", \"human_review\", \"draft_response\", \"bug_tracking\"]]:\n    \"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\"\n\n    # Create structured LLM that returns EmailClassification dict\n    structured_llm = llm.with_structured_output(EmailClassification)\n\n    # Format the prompt on-demand, not stored in state\n    classification_prompt = f\"\"\"\n    Analyze this customer email and classify it:\n\n    Email: {state['email_content']}\n    From: {state['sender_email']}\n\n    Provide classification including intent, urgency, topic, and summary.\n    \"\"\"\n\n    # Get structured response directly as dict\n    classification = structured_llm.invoke(classification_prompt)\n\n    # Determine next node based on classification\n    if classification['intent'] == 'billing' or classification['urgency'] == 'critical':\n        goto = \"human_review\"\n    elif classification['intent'] in ['question', 'feature']:\n        goto = \"search_documentation\"\n    elif classification['intent'] == 'bug':\n        goto = \"bug_tracking\"\n    else:\n        goto = \"draft_response\"\n\n    # Store classification as a single dict in state\n    return Command(\n        update={\"classification\": classification},\n        goto=goto\n    )\n"
    },
    {
        "title": "​Step 4: Build your nodes",
        "type": "code",
        "content": "def search_documentation(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n    \"\"\"Search knowledge base for relevant information\"\"\"\n\n    # Build search query from classification\n    classification = state.get('classification', {})\n    query = f\"{classification.get('intent', '')} {classification.get('topic', '')}\"\n\n    try:\n        # Implement your search logic here\n        # Store raw search results, not formatted text\n        search_results = [\n            \"Reset password via Settings > Security > Change Password\",\n            \"Password must be at least 12 characters\",\n            \"Include uppercase, lowercase, numbers, and symbols\"\n        ]\n    except SearchAPIError as e:\n        # For recoverable search errors, store error and continue\n        search_results = [f\"Search temporarily unavailable: {str(e)}\"]\n\n    return Command(\n        update={\"search_results\": search_results},  # Store raw results or error\n        goto=\"draft_response\"\n    )\n\ndef bug_tracking(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n    \"\"\"Create or update bug tracking ticket\"\"\"\n\n    # Create ticket in your bug tracking system\n    ticket_id = \"BUG-12345\"  # Would be created via API\n\n    return Command(\n        update={\n            \"search_results\": [f\"Bug ticket {ticket_id} created\"],\n            \"current_step\": \"bug_tracked\"\n        },\n        goto=\"draft_response\"\n    )\n"
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "text",
        "content": "Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.\n\nTo enable human-in-the-loop with interrupt() , we need to compile with a checkpointer to save state between runs:"
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import RetryPolicy\n\n# Create the graph\nworkflow = StateGraph(EmailAgentState)\n\n# Add nodes with appropriate error handling\nworkflow.add_node(\"read_email\", read_email)\nworkflow.add_node(\"classify_intent\", classify_intent)\n\n# Add retry policy for nodes that might have transient failures\nworkflow.add_node(\n    \"search_documentation\",\n    search_documentation,\n    retry_policy=RetryPolicy(max_attempts=3)\n)\nworkflow.add_node(\"bug_tracking\", bug_tracking)\nworkflow.add_node(\"draft_response\", draft_response)\nworkflow.add_node(\"human_review\", human_review)\nworkflow.add_node(\"send_reply\", send_reply)\n\n# Add only the essential edges\nworkflow.add_edge(START, \"read_email\")\nworkflow.add_edge(\"read_email\", \"classify_intent\")\nworkflow.add_edge(\"send_reply\", END)\n\n# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n"
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "text",
        "content": "The graph structure is minimal because routing happens inside nodes through Command objects. Each node declares where it can go using type hints like Command[Literal[\"node1\", \"node2\"]] , making the flow explicit and traceable."
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "code",
        "content": "Command[Literal[\"node1\", \"node2\"]]"
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "text",
        "content": "Let’s run our agent with an urgent billing issue that needs human review:"
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "code",
        "content": "# Test with an urgent billing issue\ninitial_state = {\n    \"email_content\": \"I was charged twice for my subscription! This is urgent!\",\n    \"sender_email\": \"customer@example.com\",\n    \"email_id\": \"email_123\",\n    \"messages\": []\n}\n\n# Run with a thread_id for persistence\nconfig = {\"configurable\": {\"thread_id\": \"customer_123\"}}\nresult = app.invoke(initial_state, config)\n# The graph will pause at human_review\nprint(f\"Draft ready for review: {result['draft_response'][:100]}...\")\n\n# When ready, provide human input to resume\nfrom langgraph.types import Command\n\nhuman_response = Command(\n    resume={\n        \"approved\": True,\n        \"edited_response\": \"We sincerely apologize for the double charge. I've initiated an immediate refund...\"\n    }\n)\n\n# Resume execution\nfinal_result = app.invoke(human_response, config)\nprint(f\"Email sent successfully!\")\n"
    },
    {
        "title": "​Step 5: Wire it together",
        "type": "text",
        "content": "The graph pauses when it hits interrupt() , saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The thread_id ensures all state for this conversation is preserved together."
    },
    {
        "title": "​Summary and next steps",
        "type": "text",
        "content": "Building this email agent has shown us the LangGraph way of thinking:"
    },
    {
        "title": "Break into discrete steps",
        "type": "text",
        "content": "Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps."
    },
    {
        "title": "State is shared memory",
        "type": "text",
        "content": "Store raw data, not formatted text. This lets different nodes use the same information in different ways."
    },
    {
        "title": "Nodes are functions",
        "type": "text",
        "content": "They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination."
    },
    {
        "title": "Errors are part of the flow",
        "type": "text",
        "content": "Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging."
    },
    {
        "title": "Human input is first-class",
        "type": "text",
        "content": "The interrupt() function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first."
    },
    {
        "title": "Graph structure emerges naturally",
        "type": "text",
        "content": "You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.\n\nThis section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.\n\nYou might wonder: why not combine Read Email and Classify Intent into one node?"
    },
    {
        "title": "Graph structure emerges naturally",
        "type": "text",
        "content": "Or why separate Doc Search from Draft Reply?\n\nThe answer involves trade-offs between resilience and observability.\n\nThe resilience consideration: LangGraph’s durable execution creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.\n\nWhy we chose this breakdown for the email agent:\n\nIsolation of external services: Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.\n\nIntermediate visibility: Having Classify Intent as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review."
    },
    {
        "title": "Graph structure emerges naturally",
        "type": "text",
        "content": "Different failure modes: LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.\n\nReusability and testing: Smaller nodes are easier to test in isolation and reuse in other workflows.\n\nA different valid approach: You could combine Read Email and Classify Intent into a single node. You’d lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off."
    },
    {
        "title": "Graph structure emerges naturally",
        "type": "text",
        "content": "Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn’t prescribe this.\n\nPerformance considerations: More nodes doesn’t mean slower execution. LangGraph writes checkpoints in the background by default ( async durability mode ), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use \"exit\" mode to checkpoint only at completion, or \"sync\" mode to block execution until each checkpoint is written."
    },
    {
        "title": "Graph structure emerges naturally",
        "type": "text",
        "content": "This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:"
    },
    {
        "title": "Human-in-the-loop patterns",
        "type": "text",
        "content": "Learn how to add tool approval before execution, batch approval, and other patterns"
    },
    {
        "title": "Observability",
        "type": "text",
        "content": "Add observability with LangSmith for debugging and monitoring"
    },
    {
        "title": "Tool Integration",
        "type": "text",
        "content": "Integrate more tools for web search, database queries, and API calls"
    },
    {
        "title": "Retry Logic",
        "type": "text",
        "content": "Implement retry logic with exponential backoff for failed operations\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Workflows and agents",
        "type": "text",
        "content": "This guide reviews common workflow and agent patterns.\n\nLangGraph offers several benefits when building agents and workflows, including persistence , streaming , and support for debugging as well as deployment ."
    },
    {
        "title": "​Setup",
        "type": "text",
        "content": "To build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:"
    },
    {
        "title": "​Setup",
        "type": "code",
        "content": "pip install langchain_core langchain-anthropic langgraph\n"
    },
    {
        "title": "​Setup",
        "type": "code",
        "content": "import os\nimport getpass\n\nfrom langchain_anthropic import ChatAnthropic\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n"
    },
    {
        "title": "​LLMs and augmentations",
        "type": "text",
        "content": "Workflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling , structured outputs , and short term memory are a few options for tailoring LLMs to your needs."
    },
    {
        "title": "​LLMs and augmentations",
        "type": "code",
        "content": "# Schema for structured output\nfrom pydantic import BaseModel, Field\n\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n    justification: str = Field(\n        None, description=\"Why this query is relevant to the user's request.\"\n    )\n\n\n# Augment the LLM with schema for structured output\nstructured_llm = llm.with_structured_output(SearchQuery)\n\n# Invoke the augmented LLM\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n\n# Define a tool\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\n# Augment the LLM with tools\nllm_with_tools = llm.bind_tools([multiply])\n\n# Invoke the LLM with input that triggers the tool call\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\n\n# Get the tool call\nmsg.tool_calls\n"
    },
    {
        "title": "​Prompt chaining",
        "type": "text",
        "content": "Prompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:"
    },
    {
        "title": "​Parallelization",
        "type": "text",
        "content": "With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n\nSome examples include:"
    },
    {
        "title": "​Parallelization",
        "type": "code",
        "content": "# Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    story: str\n    poem: str\n    combined_output: str\n\n\n# Nodes\ndef call_llm_1(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef call_llm_2(state: State):\n    \"\"\"Second LLM call to generate story\"\"\"\n\n    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n    return {\"story\": msg.content}\n\n\ndef call_llm_3(state: State):\n    \"\"\"Third LLM call to generate poem\"\"\"\n\n    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n    return {\"poem\": msg.content}\n\n\ndef aggregator(state: State):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n    combined += f\"POEM:\\n{state['poem']}\"\n    return {\"combined_output\": combined}\n\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\nparallel_builder.add_node(\"aggregator\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"call_llm_1\")\nparallel_builder.add_edge(START, \"call_llm_2\")\nparallel_builder.add_edge(START, \"call_llm_3\")\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\nparallel_builder.add_edge(\"aggregator\", END)\nparallel_workflow = parallel_builder.compile()\n\n# Show workflow\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\nprint(state[\"combined_output\"])\n"
    },
    {
        "title": "​Routing",
        "type": "text",
        "content": "Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc."
    },
    {
        "title": "​Orchestrator-worker",
        "type": "text",
        "content": "In an orchestrator-worker configuration, the orchestrator:\n\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization . This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern."
    },
    {
        "title": "​Orchestrator-worker",
        "type": "code",
        "content": "from typing import Annotated, List\nimport operator\n\n\n# Schema for structured output to use in planning\nclass Section(BaseModel):\n    name: str = Field(\n        description=\"Name for this section of the report.\",\n    )\n    description: str = Field(\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n    )\n\n\nclass Sections(BaseModel):\n    sections: List[Section] = Field(\n        description=\"Sections of the report.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)\n"
    },
    {
        "title": "​Orchestrator-worker",
        "type": "text",
        "content": "Orchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker."
    },
    {
        "title": "​Evaluator-optimizer",
        "type": "text",
        "content": "In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\n\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages."
    },
    {
        "title": "​Agents",
        "type": "text",
        "content": "Agents are typically implemented as an LLM performing actions using tools . They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n\nTo get started with agents, see the quickstart or read more about how they work in LangChain."
    },
    {
        "title": "​Agents",
        "type": "code",
        "content": "from langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n"
    },
    {
        "title": "​Agents",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Persistence",
        "type": "text",
        "content": "LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread , which can be accessed after graph execution. Because threads allow access to graph’s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we’ll discuss each of these concepts in more detail."
    },
    {
        "title": "Persistence",
        "type": "text",
        "content": "LangGraph API handles checkpointing automatically When using the LangGraph API, you don’t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes."
    },
    {
        "title": "​Threads",
        "type": "text",
        "content": "A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs . When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\n\nWhen invoking a graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config."
    },
    {
        "title": "​Threads",
        "type": "code",
        "content": "{\"configurable\": {\"thread_id\": \"1\"}}\n"
    },
    {
        "title": "​Threads",
        "type": "text",
        "content": "A thread’s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "Checkpoints are persisted and can be used to restore the state of a thread at a later time.\n\nLet’s see what checkpoints are saved when a simple graph is invoked as follows:"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "from langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "After we run the graph, we expect to see exactly 4 checkpoints:"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "{'foo': '', 'bar': []}"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "{'foo': 'a', 'bar': ['a']}"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "{'foo': 'b', 'bar': ['a', 'b']}"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "Note that we bar channel values contain outputs from both nodes as we have a reducer for bar channel."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "When interacting with the saved graph state, you must specify a thread identifier . You can view the latest state of the graph by calling graph.get_state(config) . This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided."
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "graph.get_state(config)"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "# get the latest state snapshot\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.get_state(config)\n\n# get a state snapshot for a specific checkpoint_id\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\ngraph.get_state(config)\n"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "In our example, the output of get_state will look like this:"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "StateSnapshot(\n    values={'foo': 'b', 'bar': ['a', 'b']},\n    next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n    created_at='2024-08-29T19:19:38.821749+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "You can get the full history of the graph execution for a given thread by calling graph.get_state_history(config) . This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list."
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "graph.get_state_history(config)"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "config = {\"configurable\": {\"thread_id\": \"1\"}}\nlist(graph.get_state_history(config))\n"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "In our example, the output of get_state_history will look like this:"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "It’s also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id , then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id , and only execute the steps after the checkpoint."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "You must pass these when invoking the graph as part of the configurable portion of the config:"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\ngraph.invoke(None, config=config)\n"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id . All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying ."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "In addition to re-playing the graph from specific checkpoints , we can also edit the graph state. We do this using update_state . This method accepts three different arguments:"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let’s walk through an example."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "Let’s assume you have defined the state of your graph with the following schema (see full example above):"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "{\"foo\": 1, \"bar\": [\"a\"]}\n"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "graph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n"
    },
    {
        "title": "​Checkpoints",
        "type": "code",
        "content": "{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n"
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "The foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar ."
    },
    {
        "title": "​Checkpoints",
        "type": "text",
        "content": "The final thing you can optionally specify when calling update_state is as_node . If you provided it, the update will be applied as if it came from node as_node . If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state ."
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\n\nBut, what if we want to retain some information across threads ? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!\n\nWith checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable."
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "LangGraph API handles stores automatically When using the LangGraph API, you don’t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\n\nFirst, let’s showcase this in isolation without using LangGraph."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "from langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "Memories are namespaced by a tuple , which in this specific example will be (<user_id>, \"memories\") . The namespace can be any length and represent anything, does not have to be user specific."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "(<user_id>, \"memories\")"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "user_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "We use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory ( memory_id ) and the value (a dictionary) is the memory itself."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "memory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "We can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "memories = in_memory_store.search(namespace_for_memory)\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "Each memory type is a Python class ( Item ) with certain attributes. We can access it as a dictionary by converting via .dict as above."
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "from langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "Now when searching, you can use natural language queries to find relevant memories:"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "# Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories = store.search(\n    namespace_for_memory,\n    query=\"What does the user like to eat?\",\n    limit=3  # Return top 3 matches\n)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "# Store with specific fields to embed\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\n        \"food_preference\": \"I love Italian cuisine\",\n        \"context\": \"Discussing dinner plans\"\n    },\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n)\n\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\"system_info\": \"Last updated: 2024-01-01\"},\n    index=False\n)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "With this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import InMemorySaver\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "We invoke the graph with a thread_id , as before, and also with a user_id , which we’ll use to namespace our memories to this particular user as we showed above."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "# Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# First let's just say hi to the AI\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here’s how we might use semantic search in a node to find relevant memories:"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "config: RunnableConfig"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # ... Analyze conversation and create a new memory\n\n    # Create a new memory ID\n    memory_id = str(uuid.uuid4())\n\n    # We create a new memory\n    store.put(namespace, memory_id, {\"memory\": memory})\n\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "As we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "memories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "We can access the memories and use them in our model call."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # Search based on the most recent message\n    memories = store.search(\n        namespace,\n        query=state[\"messages\"][-1].content,\n        limit=3\n    )\n    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n\n    # ... Use memories in the model call\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "If we create a new thread, we can still access the same memories so long as the user_id is the same."
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Let's say hi again\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "When we use the LangSmith, either locally (e.g., in Studio ) or hosted with LangSmith , the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:"
    },
    {
        "title": "​Memory Store",
        "type": "code",
        "content": "{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embeddings-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n"
    },
    {
        "title": "​Memory Store",
        "type": "text",
        "content": "See the deployment guide for more details and configuration options."
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "langgraph-checkpoint-sqlite"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "langgraph-checkpoint-postgres"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "graph.get_state_history()"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke , .astream , .abatch ), asynchronous versions of the above methods will be used ( .aput , .aput_writes , .aget_tuple , .alist )."
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "For running your graph asynchronously, you can use InMemorySaver , or async versions of Sqlite/Postgres checkpointers — AsyncSqliteSaver / AsyncPostgresSaver checkpointers."
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\n\nlanggraph_checkpoint defines protocol for implementing serializers provides a default implementation ( JsonPlusSerializer ) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more."
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "The default serializer, JsonPlusSerializer , uses ormsgpack and JSON under the hood, which is not suitable for all types of objects."
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\nyou can use the pickle_fallback argument of the JsonPlusSerializer :"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n\n# ... Define the graph ...\ngraph.compile(\n    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n)\n"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes , which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument):"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "from_pycryptodome_aes"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "import sqlite3\n\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "code",
        "content": "from langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\ncheckpointer.setup()\n"
    },
    {
        "title": "​Checkpointer libraries",
        "type": "text",
        "content": "When running on LangSmith, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer ."
    },
    {
        "title": "​Capabilities",
        "type": "text",
        "content": "First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.\n\nSecond, checkpointers allow for “memory” between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.\n\nThird, checkpointers allow for “time travel” , allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\n\nLastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.\n\nAdditionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Durable execution",
        "type": "text",
        "content": "Durable execution is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require human-in-the-loop , where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps — even after a significant delay (e.g., a week later).\n\nLangGraph’s built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted — whether by a system failure or for human-in-the-loop interactions — it can be resumed from its last recorded state.\n\nIf you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\nTo make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks . You can use tasks from both the StateGraph (Graph API) and the Functional API ."
    },
    {
        "title": "​Requirements",
        "type": "text",
        "content": "To leverage durable execution in LangGraph, you need to:\n\nEnable persistence in your workflow by specifying a checkpointer that will save workflow progress.\n\nSpecify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.\n\nWrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside @[ task ] to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay ."
    },
    {
        "title": "​Determinism and Consistent Replay",
        "type": "text",
        "content": "When you resume a workflow run, the code does NOT resume from the same line of code where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped.\n\nAs a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes .\n\nTo ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:\n\nFor some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows\nhow to structure your code using tasks to avoid these issues. The same principles apply to the StateGraph (Graph API) ."
    },
    {
        "title": "​Durability modes",
        "type": "text",
        "content": "LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application’s requirements. The durability modes, from least to most durable, are as follows:"
    },
    {
        "title": "​Durability modes",
        "type": "text",
        "content": "A higher durability mode adds more overhead to the workflow execution.\n\nAdded in v0.6.0 Use the durability parameter instead of checkpoint_during (deprecated in v0.6.0) for persistence policy management:"
    },
    {
        "title": "​Durability modes",
        "type": "code",
        "content": "checkpoint_during=True"
    },
    {
        "title": "​Durability modes",
        "type": "code",
        "content": "checkpoint_during=False"
    },
    {
        "title": "​Durability modes",
        "type": "text",
        "content": "for persistence policy management, with the following mapping:"
    },
    {
        "title": "​Durability modes",
        "type": "code",
        "content": "checkpoint_during=True"
    },
    {
        "title": "​Durability modes",
        "type": "code",
        "content": "checkpoint_during=False"
    },
    {
        "title": "​Durability modes",
        "type": "text",
        "content": "Changes are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution."
    },
    {
        "title": "​Durability modes",
        "type": "text",
        "content": "Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there’s a small risk that checkpoints might not be written if the process crashes during execution."
    },
    {
        "title": "​Durability modes",
        "type": "text",
        "content": "Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\n\nYou can specify the durability mode when calling any graph execution method:"
    },
    {
        "title": "​Durability modes",
        "type": "code",
        "content": "graph.stream(\n    {\"input\": \"test\"},\n    durability=\"sync\"\n)\n"
    },
    {
        "title": "​Using tasks in nodes",
        "type": "text",
        "content": "If a node contains multiple operations, you may find it easier to convert each operation into a task rather than refactor the operations into individual nodes."
    },
    {
        "title": "​Using tasks in nodes",
        "type": "code",
        "content": "from typing import NotRequired\nfrom typing_extensions import TypedDict\nimport uuid\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\nimport requests\n\n# Define a TypedDict to represent the state\nclass State(TypedDict):\n    url: str\n    result: NotRequired[str]\n\ndef call_api(state: State):\n    \"\"\"Example node that makes an API request.\"\"\"\n    result = requests.get(state['url']).text[:100]  # Side-effect  #\n    return {\n        \"result\": result\n    }\n\n# Create a StateGraph builder and add a node for the call_api function\nbuilder = StateGraph(State)\nbuilder.add_node(\"call_api\", call_api)\n\n# Connect the start and end nodes to the call_api node\nbuilder.add_edge(START, \"call_api\")\nbuilder.add_edge(\"call_api\", END)\n\n# Specify a checkpointer\ncheckpointer = InMemorySaver()\n\n# Compile the graph with the checkpointer\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Define a config with a thread ID.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Invoke the graph\ngraph.invoke({\"url\": \"https://www.example.com\"}, config)\n"
    },
    {
        "title": "​Resuming Workflows",
        "type": "text",
        "content": "Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:"
    },
    {
        "title": "​Starting Points for Resuming Workflows",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Streaming",
        "type": "text",
        "content": "LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\nWhat’s possible with LangGraph streaming:"
    },
    {
        "title": "​Supported stream modes",
        "type": "text",
        "content": "Pass one or more of the following stream modes as a list to the stream or astream methods:"
    },
    {
        "title": "​Basic usage example",
        "type": "text",
        "content": "LangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators."
    },
    {
        "title": "​Basic usage example",
        "type": "code",
        "content": "for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n    print(chunk)\n"
    },
    {
        "title": "​Basic usage example",
        "type": "code",
        "content": "from typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(refine_topic)\n    .add_node(generate_joke)\n    .add_edge(START, \"refine_topic\")\n    .add_edge(\"refine_topic\", \"generate_joke\")\n    .add_edge(\"generate_joke\", END)\n    .compile()\n)\n\n# The stream() method returns an iterator that yields streamed outputs\nfor chunk in graph.stream(  \n    {\"topic\": \"ice cream\"},\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n    # Other stream modes are also available. See supported stream modes for details\n    stream_mode=\"updates\",  \n):\n    print(chunk)\n"
    },
    {
        "title": "​Basic usage example",
        "type": "code",
        "content": "{'refineTopic': {'topic': 'ice cream and cats'}}\n{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}\n"
    },
    {
        "title": "​Stream multiple modes",
        "type": "text",
        "content": "You can pass a list as the stream_mode parameter to stream multiple modes at once."
    },
    {
        "title": "​Stream multiple modes",
        "type": "text",
        "content": "The streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode."
    },
    {
        "title": "​Stream multiple modes",
        "type": "code",
        "content": "for mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n    print(chunk)\n"
    },
    {
        "title": "​Stream graph state",
        "type": "text",
        "content": "Use the stream modes updates and values to stream the state of the graph as it executes."
    },
    {
        "title": "​Stream graph state",
        "type": "code",
        "content": "from typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str\n\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n  StateGraph(State)\n  .add_node(refine_topic)\n  .add_node(generate_joke)\n  .add_edge(START, \"refine_topic\")\n  .add_edge(\"refine_topic\", \"generate_joke\")\n  .add_edge(\"generate_joke\", END)\n  .compile()\n)\n"
    },
    {
        "title": "​Stream graph state",
        "type": "text",
        "content": "Use this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update."
    },
    {
        "title": "​Stream graph state",
        "type": "code",
        "content": "for chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"updates\",  \n):\n    print(chunk)\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "text",
        "content": "To include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs."
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "text",
        "content": "The outputs will be streamed as tuples (namespace, data) , where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\") ."
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "(\"parent_node:<task_id>\", \"child_node:<task_id>\")"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "for chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    # Set subgraphs=True to stream outputs from subgraphs\n    subgraphs=True,  \n    stream_mode=\"updates\",\n):\n    print(chunk)\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "from langgraph.graph import START, StateGraph\nfrom typing import TypedDict\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    # Set subgraphs=True to stream outputs from subgraphs\n    subgraphs=True,  \n):\n    print(chunk)\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "text",
        "content": "Note that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n\nUse the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state."
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "for chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"debug\",  \n):\n    print(chunk)\n"
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "Use the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks."
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "The streamed output from messages mode is a tuple (message_chunk, metadata) where:"
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "(message_chunk, metadata)"
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "If your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details."
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "Manual config required for async in Python < 3.11 When using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+."
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "from dataclasses import dataclass\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n\n\n@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nmodel = init_chat_model(model=\"gpt-4o-mini\")\n\ndef call_model(state: MyState):\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n    model_response = model.invoke(  \n        [\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n        ]\n    )\n    return {\"joke\": model_response.content}\n\ngraph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor message_chunk, metadata in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",  \n):\n    if message_chunk.content:\n        print(message_chunk.content, end=\"|\", flush=True)\n"
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "You can associate tags with LLM invocations to filter the streamed tokens by LLM invocation."
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\n# model_1 is tagged with \"joke\"\nmodel_1 = init_chat_model(model=\"gpt-4o-mini\", tags=['joke'])\n# model_2 is tagged with \"poem\"\nmodel_2 = init_chat_model(model=\"gpt-4o-mini\", tags=['poem'])\n\ngraph = ... # define a graph that uses these LLMs\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync for msg, metadata in graph.astream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  \n):\n    # Filter the streamed tokens by the tags field in the metadata to only include\n    # the tokens from the LLM invocation with the \"joke\" tag\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)\n"
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "from typing import TypedDict\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import START, StateGraph\n\n# The joke_model is tagged with \"joke\"\njoke_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"joke\"])\n# The poem_model is tagged with \"poem\"\npoem_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"poem\"])\n\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n      poem: str\n\n\nasync def call_model(state, config):\n      topic = state[\"topic\"]\n      print(\"Writing joke...\")\n      # Note: Passing the config through explicitly is required for python < 3.11\n      # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n      # The config is passed through explicitly to ensure the context vars are propagated correctly\n      # This is required for Python < 3.11 when using async code. Please see the async section for more details\n      joke_response = await joke_model.ainvoke(\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n            config,\n      )\n      print(\"\\n\\nWriting poem...\")\n      poem_response = await poem_model.ainvoke(\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n            config,\n      )\n      return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\n\ngraph = (\n      StateGraph(State)\n      .add_node(call_model)\n      .add_edge(START, \"call_model\")\n      .compile()\n)\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync for msg, metadata in graph.astream(\n      {\"topic\": \"cats\"},\n      stream_mode=\"messages\",\n):\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)\n"
    },
    {
        "title": "​LLM tokens",
        "type": "text",
        "content": "To stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:"
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "stream_mode=\"messages\""
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor msg, metadata in graph.stream(\n    inputs,\n    stream_mode=\"messages\",  \n):\n    # Filter the streamed tokens by the langgraph_node field in the metadata\n    # to only include the tokens from the specified node\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\n        ...\n"
    },
    {
        "title": "​LLM tokens",
        "type": "code",
        "content": "from typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n      poem: str\n\n\ndef write_joke(state: State):\n      topic = state[\"topic\"]\n      joke_response = model.invoke(\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n      )\n      return {\"joke\": joke_response.content}\n\n\ndef write_poem(state: State):\n      topic = state[\"topic\"]\n      poem_response = model.invoke(\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n      )\n      return {\"poem\": poem_response.content}\n\n\ngraph = (\n      StateGraph(State)\n      .add_node(write_joke)\n      .add_node(write_poem)\n      # write both the joke and the poem concurrently\n      .add_edge(START, \"write_joke\")\n      .add_edge(START, \"write_poem\")\n      .compile()\n)\n\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor msg, metadata in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  \n):\n    # Filter the streamed tokens by the langgraph_node field in the metadata\n    # to only include the tokens from the write_poem node\n    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n        print(msg.content, end=\"|\", flush=True)\n"
    },
    {
        "title": "​Stream custom data",
        "type": "text",
        "content": "To send custom user-defined data from inside a LangGraph node or tool, follow these steps:"
    },
    {
        "title": "​Stream custom data",
        "type": "code",
        "content": "[\"updates\", \"custom\"]"
    },
    {
        "title": "​Stream custom data",
        "type": "text",
        "content": "No get_stream_writer in async for Python < 3.11 In async code running on Python < 3.11, get_stream_writer will not work.\nInstead, add a writer parameter to your node or tool and pass it manually.\nSee Async with Python < 3.11 for usage examples."
    },
    {
        "title": "​Stream custom data",
        "type": "code",
        "content": "from typing import TypedDict\nfrom langgraph.config import get_stream_writer\nfrom langgraph.graph import StateGraph, START\n\nclass State(TypedDict):\n    query: str\n    answer: str\n\ndef node(state: State):\n    # Get the stream writer to send custom data\n    writer = get_stream_writer()\n    # Emit a custom key-value pair (e.g., progress update)\n    writer({\"custom_key\": \"Generating custom data inside node\"})\n    return {\"answer\": \"some data\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(node)\n    .add_edge(START, \"node\")\n    .compile()\n)\n\ninputs = {\"query\": \"example\"}\n\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):\n    print(chunk)\n"
    },
    {
        "title": "​Use with any LLM",
        "type": "text",
        "content": "You can use stream_mode=\"custom\" to stream data from any LLM API — even if that API does not implement the LangChain chat model interface."
    },
    {
        "title": "​Use with any LLM",
        "type": "text",
        "content": "This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups."
    },
    {
        "title": "​Use with any LLM",
        "type": "code",
        "content": "from langgraph.config import get_stream_writer\n\ndef call_arbitrary_model(state):\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n    # Get the stream writer to send custom data\n    writer = get_stream_writer()  \n    # Assume you have a streaming client that yields chunks\n    # Generate LLM tokens using your custom streaming client\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\n        # Use the writer to send custom data to the stream\n        writer({\"custom_llm_chunk\": chunk})  \n    return {\"result\": \"completed\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_arbitrary_model)\n    # Add other nodes and edges as needed\n    .compile()\n)\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"custom\",  \n\n):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\n"
    },
    {
        "title": "​Use with any LLM",
        "type": "text",
        "content": "Let’s invoke the graph with an AIMessage that includes a tool call:"
    },
    {
        "title": "​Use with any LLM",
        "type": "code",
        "content": "inputs = {\n    \"messages\": [\n        {\n            \"content\": None,\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"1\",\n                    \"function\": {\n                        \"arguments\": '{\"place\":\"bedroom\"}',\n                        \"name\": \"get_items\",\n                    },\n                    \"type\": \"function\",\n                }\n            ],\n        }\n    ]\n}\n\nasync for chunk in graph.astream(\n    inputs,\n    stream_mode=\"custom\",\n):\n    print(chunk[\"content\"], end=\"|\", flush=True)\n"
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "text",
        "content": "If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\n\nSet disable_streaming=True when initializing the model."
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "code",
        "content": "disable_streaming=True"
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    # Set disable_streaming=True to disable streaming for the chat model\n    disable_streaming=True\n\n)\n"
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "text",
        "content": "In Python versions < 3.11, asyncio tasks do not support the context parameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph’s streaming mechanisms in two key ways:"
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "code",
        "content": "from typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(model=\"gpt-4o-mini\")\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\n# Accept config as an argument in the async node function\nasync def call_model(state, config):\n    topic = state[\"topic\"]\n    print(\"Generating joke...\")\n    # Pass config to model.ainvoke() to ensure proper context propagation\n    joke_response = await model.ainvoke(  \n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n        config,\n    )\n    return {\"joke\": joke_response.content}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\n# Set stream_mode=\"messages\" to stream LLM tokens\nasync for chunk, metadata in graph.astream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",  \n):\n    if chunk.content:\n        print(chunk.content, end=\"|\", flush=True)\n"
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "code",
        "content": "from typing import TypedDict\nfrom langgraph.types import StreamWriter\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n\n# Add writer as an argument in the function signature of the async node or tool\n# LangGraph will automatically pass the stream writer to the function\nasync def generate_joke(state: State, writer: StreamWriter):  \n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n      return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n      StateGraph(State)\n      .add_node(generate_joke)\n      .add_edge(START, \"generate_joke\")\n      .compile()\n)\n\n# Set stream_mode=\"custom\" to receive the custom data in the stream  #\nasync for chunk in graph.astream(\n      {\"topic\": \"ice cream\"},\n      stream_mode=\"custom\",\n):\n      print(chunk)\n"
    },
    {
        "title": "​Disable streaming for specific chat models",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Interrupts",
        "type": "text",
        "content": "Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.\n\nInterrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you’re ready to continue, you resume execution by re-invoking the graph using Command , which then becomes the return value of the interrupt() call from inside the node."
    },
    {
        "title": "Interrupts",
        "type": "text",
        "content": "Unlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic —they can be placed anywhere in your code and can be conditional based on your application logic."
    },
    {
        "title": "Interrupts",
        "type": "code",
        "content": "config={\"configurable\": {\"thread_id\": ...}}"
    },
    {
        "title": "Interrupts",
        "type": "text",
        "content": "The thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state."
    },
    {
        "title": "​Pause usinginterrupt",
        "type": "text",
        "content": "The interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input."
    },
    {
        "title": "​Pause usinginterrupt",
        "type": "code",
        "content": "from langgraph.types import interrupt\n\ndef approval_node(state: State):\n    # Pause and ask for approval\n    approved = interrupt(\"Do you approve this action?\")\n\n    # When you resume, Command(resume=...) returns that value here\n    return {\"approved\": approved}\n"
    },
    {
        "title": "​Resuming interrupts",
        "type": "text",
        "content": "After an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input."
    },
    {
        "title": "​Resuming interrupts",
        "type": "code",
        "content": "from langgraph.types import Command\n\n# Initial run - hits the interrupt and pauses\n# thread_id is the persistent pointer (stores a stable ID in production)\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\nresult = graph.invoke({\"input\": \"data\"}, config=config)\n\n# Check what was interrupted\n# __interrupt__ contains the payload that was passed to interrupt()\nprint(result[\"__interrupt__\"])\n# > [Interrupt(value='Do you approve this action?')]\n\n# Resume with the human's response\n# The resume payload becomes the return value of interrupt() inside the node\ngraph.invoke(Command(resume=True), config=config)\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\n\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision."
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "from typing import Literal\nfrom langgraph.types import interrupt, Command\n\ndef approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\n    # Pause execution; payload shows up under result[\"__interrupt__\"]\n    is_approved = interrupt({\n        \"question\": \"Do you want to proceed with this action?\",\n        \"details\": state[\"action_details\"]\n    })\n\n    # Route based on the response\n    if is_approved:\n        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\n    else:\n        return Command(goto=\"cancel\")\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "When you resume the graph, pass true to approve or false to reject:"
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "# To approve\ngraph.invoke(Command(resume=True), config=config)\n\n# To reject\ngraph.invoke(Command(resume=False), config=config)\n"
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "import sqlite3\nfrom typing import Literal, Optional, TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass ApprovalState(TypedDict):\n    action_details: str\n    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\n\n\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\n    # Expose details so the caller can render them in a UI\n    decision = interrupt({\n        \"question\": \"Approve this action?\",\n        \"details\": state[\"action_details\"],\n    })\n\n    # Route to the appropriate node after resume\n    return Command(goto=\"proceed\" if decision else \"cancel\")\n\n\ndef proceed_node(state: ApprovalState):\n    return {\"status\": \"approved\"}\n\n\ndef cancel_node(state: ApprovalState):\n    return {\"status\": \"rejected\"}\n\n\nbuilder = StateGraph(ApprovalState)\nbuilder.add_node(\"approval\", approval_node)\nbuilder.add_node(\"proceed\", proceed_node)\nbuilder.add_node(\"cancel\", cancel_node)\nbuilder.add_edge(START, \"approval\")\nbuilder.add_edge(\"approval\", \"proceed\")\nbuilder.add_edge(\"approval\", \"cancel\")\nbuilder.add_edge(\"proceed\", END)\nbuilder.add_edge(\"cancel\", END)\n\n# Use a more durable checkpointer in production\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\ninitial = graph.invoke(\n    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\n    config=config,\n)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'question': ..., 'details': ...})]\n\n# Resume with the decision; True routes to proceed, False to cancel\nresumed = graph.invoke(Command(resume=True), config=config)\nprint(resumed[\"status\"])  # -> \"approved\"\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments."
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "from langgraph.types import interrupt\n\ndef review_node(state: State):\n    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\n    edited_content = interrupt({\n        \"instruction\": \"Review and edit this content\",\n        \"content\": state[\"generated_text\"]\n    })\n\n    # Update the state with the edited version\n    return {\"generated_text\": edited_content}\n"
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "graph.invoke(\n    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\n    config=config\n)\n"
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "import sqlite3\nfrom typing import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass ReviewState(TypedDict):\n    generated_text: str\n\n\ndef review_node(state: ReviewState):\n    # Ask a reviewer to edit the generated content\n    updated = interrupt({\n        \"instruction\": \"Review and edit this content\",\n        \"content\": state[\"generated_text\"],\n    })\n    return {\"generated_text\": updated}\n\n\nbuilder = StateGraph(ReviewState)\nbuilder.add_node(\"review\", review_node)\nbuilder.add_edge(START, \"review\")\nbuilder.add_edge(\"review\", END)\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"review-42\"}}\ninitial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]\n\n# Resume with the edited text from the reviewer\nfinal_state = graph.invoke(\n    Command(resume=\"Improved draft after review\"),\n    config=config,\n)\nprint(final_state[\"generated_text\"])  # -> \"Improved draft after review\"\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it’s called, and allows for human review and editing of the tool call before it is executed.\n\nFirst, define a tool that uses interrupt :"
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "from langchain.tools import tool\nfrom langgraph.types import interrupt\n\n@tool\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n    response = interrupt({\n        \"action\": \"send_email\",\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body,\n        \"message\": \"Approve sending this email?\"\n    })\n\n    if response.get(\"action\") == \"approve\":\n        # Resume value can override inputs before executing\n        final_to = response.get(\"to\", to)\n        final_subject = response.get(\"subject\", subject)\n        final_body = response.get(\"body\", body)\n        return f\"Email sent to {final_to} with subject '{final_subject}'\"\n    return \"Email cancelled by user\"\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action."
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "Sometimes you need to validate input from humans and ask again if it’s invalid. You can do this using multiple interrupt calls in a loop."
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "from langgraph.types import interrupt\n\ndef get_age_node(state: State):\n    prompt = \"What is your age?\"\n\n    while True:\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n\n        # Validate the input\n        if isinstance(answer, int) and answer > 0:\n            # Valid input - continue\n            break\n        else:\n            # Invalid input - ask again with a more specific prompt\n            prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n\n    return {\"age\": answer}\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues."
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "import sqlite3\nfrom typing import TypedDict\n\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass FormState(TypedDict):\n    age: int | None\n\n\ndef get_age_node(state: FormState):\n    prompt = \"What is your age?\"\n\n    while True:\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n\n        if isinstance(answer, int) and answer > 0:\n            return {\"age\": answer}\n\n        prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n\n\nbuilder = StateGraph(FormState)\nbuilder.add_node(\"collect_age\", get_age_node)\nbuilder.add_edge(START, \"collect_age\")\nbuilder.add_edge(\"collect_age\", END)\n\ncheckpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"form-1\"}}\nfirst = graph.invoke({\"age\": None}, config=config)\nprint(first[\"__interrupt__\"])  # -> [Interrupt(value='What is your age?', ...)]\n\n# Provide invalid data; the node re-prompts\nretry = graph.invoke(Command(resume=\"thirty\"), config=config)\nprint(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n\n# Provide valid data; loop exits and state updates\nfinal = graph.invoke(Command(resume=30), config=config)\nprint(final[\"age\"])  # -> 30\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "When you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input."
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there’s a few important rules to follow when working with interrupts to ensure they behave as expected."
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "The way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph."
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ✅ Good: interrupting first, then handling\n    # error conditions separately\n    interrupt(\"What's your name?\")\n    try:\n        fetch_data()  # This can fail\n    except Exception as e:\n        print(e)\n    return state\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ❌ Bad: wrapping interrupt in bare try/except\n    # will catch the interrupt exception\n    try:\n        interrupt(\"What's your name?\")\n    except Exception as e:\n        print(e)\n    return state\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "It’s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\n\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task’s resume list. Matching is strictly index-based , so the order of interrupt calls within the node is important."
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ✅ Good: interrupt calls happen in the same order every time\n    name = interrupt(\"What's your name?\")\n    age = interrupt(\"What's your age?\")\n    city = interrupt(\"What's your city?\")\n\n    return {\n        \"name\": name,\n        \"age\": age,\n        \"city\": city\n    }\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ❌ Bad: conditionally skipping interrupts changes the order\n    name = interrupt(\"What's your name?\")\n\n    # On first run, this might skip the interrupt\n    # On resume, it might not skip it - causing index mismatch\n    if state.get(\"needs_age\"):\n        age = interrupt(\"What's your age?\")\n\n    city = interrupt(\"What's your city?\")\n\n    return {\"name\": name, \"city\": city}\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "Depending on which checkpointer is used, complex values may not be serializable (e.g. you can’t serialize a function). To make your graphs adaptable to any deployment, it’s best practice to only use values that can be reasonably serialized."
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ✅ Good: passing simple types that are serializable\n    name = interrupt(\"What's your name?\")\n    count = interrupt(42)\n    approved = interrupt(True)\n\n    return {\"name\": name, \"count\": count, \"approved\": approved}\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def validate_input(value):\n    return len(value) > 0\n\ndef node_a(state: State):\n    # ❌ Bad: passing a function to interrupt\n    # The function cannot be serialized\n    response = interrupt({\n        \"question\": \"What's your name?\",\n        \"validator\": validate_input  # This will fail\n    })\n    return {\"name\": response}\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "Because interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution."
    },
    {
        "title": "​Rules of interrupts",
        "type": "text",
        "content": "As an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records."
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ✅ Good: using upsert operation which is idempotent\n    # Running this multiple times will have the same result\n    db.upsert_user(\n        user_id=state[\"user_id\"],\n        status=\"pending_approval\"\n    )\n\n    approved = interrupt(\"Approve this change?\")\n\n    return {\"approved\": approved}\n"
    },
    {
        "title": "​Rules of interrupts",
        "type": "code",
        "content": "def node_a(state: State):\n    # ❌ Bad: creating a new record before interrupt\n    # This will create duplicate records on each resume\n    audit_id = db.create_audit_log({\n        \"user_id\": state[\"user_id\"],\n        \"action\": \"pending_approval\",\n        \"timestamp\": datetime.now()\n    })\n\n    approved = interrupt(\"Approve this change?\")\n\n    return {\"approved\": approved, \"audit_id\": audit_id}\n"
    },
    {
        "title": "​Using with subgraphs called as functions",
        "type": "text",
        "content": "When invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and the interrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called."
    },
    {
        "title": "​Using with subgraphs called as functions",
        "type": "code",
        "content": "def node_in_parent_graph(state: State):\n    some_code()  # <-- This will re-execute when resumed\n    # Invoke a subgraph as a function.\n    # The subgraph contains an `interrupt` call.\n    subgraph_result = subgraph.invoke(some_input)\n\nasync function node_in_subgraph(state: State) {\n    someOtherCode(); # <-- This will also re-execute when resumed\n    result = interrupt(\"What's your name?\")\n    ...\n}\n"
    },
    {
        "title": "​Debugging with interrupts",
        "type": "text",
        "content": "To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph."
    },
    {
        "title": "​Debugging with interrupts",
        "type": "text",
        "content": "Static interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead."
    },
    {
        "title": "​Debugging with interrupts",
        "type": "code",
        "content": "graph = builder.compile(\n    interrupt_before=[\"node_a\"],  \n    interrupt_after=[\"node_b\", \"node_c\"],  \n    checkpointer=checkpointer,\n)\n\n# Pass a thread ID to the graph\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=config)  \n\n# Resume the graph\ngraph.invoke(None, config=config)  \n"
    },
    {
        "title": "​Debugging with interrupts",
        "type": "text",
        "content": "You can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Use time-travel",
        "type": "text",
        "content": "When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:\n\nLangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.\n\nTo use time-travel in LangGraph:"
    },
    {
        "title": "Use time-travel",
        "type": "text",
        "content": "For a conceptual overview of time-travel, see Time travel ."
    },
    {
        "title": "​In a workflow",
        "type": "text",
        "content": "This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.\n\nFirst we need to install the packages required"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "%%capture --no-stderr\npip install --quiet -U langgraph langchain_anthropic\n"
    },
    {
        "title": "​In a workflow",
        "type": "text",
        "content": "Next, we need to set API keys for Anthropic (the LLM we will use)"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n"
    },
    {
        "title": "​In a workflow",
        "type": "text",
        "content": "Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph."
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "import uuid\n\nfrom typing_extensions import TypedDict, NotRequired\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    topic: NotRequired[str]\n    joke: NotRequired[str]\n\n\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    temperature=0,\n)\n\n\ndef generate_topic(state: State):\n    \"\"\"LLM call to generate a topic for the joke\"\"\"\n    msg = model.invoke(\"Give me a funny topic for a joke\")\n    return {\"topic\": msg.content}\n\n\ndef write_joke(state: State):\n    \"\"\"LLM call to write a joke based on the topic\"\"\"\n    msg = model.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_topic\", generate_topic)\nworkflow.add_node(\"write_joke\", write_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_topic\")\nworkflow.add_edge(\"generate_topic\", \"write_joke\")\nworkflow.add_edge(\"write_joke\", END)\n\n# Compile\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\ngraph\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "config = {\n    \"configurable\": {\n        \"thread_id\": uuid.uuid4(),\n    }\n}\nstate = graph.invoke({}, config)\n\nprint(state[\"topic\"])\nprint()\nprint(state[\"joke\"])\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!\n\n# The Secret Life of Socks in the Dryer\n\nI finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.\n\nMy blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "# The states are returned in reverse chronological order.\nstates = list(graph.get_state_history(config))\n\nfor state in states:\n    print(state.next)\n    print(state.config[\"configurable\"][\"checkpoint_id\"])\n    print()\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "()\n1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e\n\n('write_joke',)\n1f02ac4a-ce2a-6494-8001-cb2e2d651227\n\n('generate_topic',)\n1f02ac4a-a4e0-630d-8000-b73c254ba748\n\n('__start__',)\n1f02ac4a-a4dd-665e-bfff-e6c8c44315d9\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "# This is the state before last (states are listed in chronological order)\nselected_state = states[1]\nprint(selected_state.next)\nprint(selected_state.values)\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "('write_joke',)\n{'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'}\n"
    },
    {
        "title": "​In a workflow",
        "type": "text",
        "content": "update_state will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID."
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "new_config = graph.update_state(selected_state.config, values={\"topic\": \"chickens\"})\nprint(new_config)\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "graph.invoke(None, new_config)\n"
    },
    {
        "title": "​In a workflow",
        "type": "code",
        "content": "{'topic': 'chickens',\n 'joke': 'Why did the chicken join a band?\\n\\nBecause it had excellent drumsticks!'}\n"
    },
    {
        "title": "​In a workflow",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Memory",
        "type": "text",
        "content": "AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "Short-term memory (thread-level persistence ) enables agents to track multi-turn conversations. To add short-term memory:"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import InMemorySaver  \nfrom langgraph.graph import StateGraph\n\ncheckpointer = InMemorySaver()  \n\nbuilder = StateGraph(...)\ngraph = builder.compile(checkpointer=checkpointer)  \n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  \n)\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "In production, use a checkpointer backed by a database:"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "from langgraph.checkpoint.postgres import PostgresSaver\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n    builder = StateGraph(...)\n    graph = builder.compile(checkpointer=checkpointer)  \n"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "You need to call checkpointer.setup() the first time you’re using Postgres checkpointer"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres import PostgresSaver  \n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "pip install -U pymongo langgraph langgraph-checkpoint-mongodb\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "Setup To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don’t already have one."
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.mongodb import MongoDBSaver  \n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"localhost:27017\"\nwith MongoDBSaver.from_conn_string(DB_URI) as checkpointer:  \n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "pip install -U langgraph langgraph-checkpoint-redis\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "You need to call checkpointer.setup() the first time you’re using Redis checkpointer"
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis import RedisSaver  \n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"redis://localhost:6379\"\nwith RedisSaver.from_conn_string(DB_URI) as checkpointer:  \n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "If your graph contains subgraphs , you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs."
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()  \n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)  \nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)  \n"
    },
    {
        "title": "​Add short-term memory",
        "type": "text",
        "content": "If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories."
    },
    {
        "title": "​Add short-term memory",
        "type": "code",
        "content": "subgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)  \n"
    },
    {
        "title": "​Add long-term memory",
        "type": "text",
        "content": "Use long-term memory to store user-specific or application-specific data across conversations."
    },
    {
        "title": "​Add long-term memory",
        "type": "code",
        "content": "from langgraph.store.memory import InMemoryStore  \nfrom langgraph.graph import StateGraph\n\nstore = InMemoryStore()  \n\nbuilder = StateGraph(...)\ngraph = builder.compile(store=store)  \n"
    },
    {
        "title": "​Add long-term memory",
        "type": "code",
        "content": "from langgraph.store.postgres import PostgresStore\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresStore.from_conn_string(DB_URI) as store:  \n    builder = StateGraph(...)\n    graph = builder.compile(store=store)  \n"
    },
    {
        "title": "​Add long-term memory",
        "type": "code",
        "content": "pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n"
    },
    {
        "title": "​Add long-term memory",
        "type": "text",
        "content": "You need to call store.setup() the first time you’re using Postgres store"
    },
    {
        "title": "​Add long-term memory",
        "type": "code",
        "content": "pip install -U langgraph langgraph-checkpoint-redis\n"
    },
    {
        "title": "​Add long-term memory",
        "type": "text",
        "content": "You need to call store.setup() the first time you’re using Redis store"
    },
    {
        "title": "​Add long-term memory",
        "type": "text",
        "content": "Enable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity."
    },
    {
        "title": "​Add long-term memory",
        "type": "code",
        "content": "from langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n"
    },
    {
        "title": "​Add long-term memory",
        "type": "code",
        "content": "\nfrom langchain.embeddings import init_embeddings\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nmodel = init_chat_model(\"gpt-4o-mini\")\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\ndef chat(state, *, store: BaseStore):\n    # Search based on user's last message\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    response = model.invoke(\n        [\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n            *state[\"messages\"],\n        ]\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(START, \"chat\")\ngraph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n    stream_mode=\"messages\",\n):\n    print(message.content, end=\"\")\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\n\nThis allows the agent to keep track of the conversation without exceeding the LLM’s context window.\n\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens ) to use for handling the boundary."
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "To trim message history, use the trim_messages function:"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from langchain_core.messages.utils import (  \n    trim_messages,  \n    count_tokens_approximately  \n)  \n\ndef call_model(state: MessagesState):\n    messages = trim_messages(  \n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from langchain_core.messages.utils import (\n    trim_messages,  \n    count_tokens_approximately  \n)\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START, MessagesState\n\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\nsummarization_model = model.bind(max_tokens=128)\n\ndef call_model(state: MessagesState):\n    messages = trim_messages(  \n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(START, \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "================================== Ai Message ==================================\n\nYour name is Bob, as you mentioned when you first introduced yourself.\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the RemoveMessage . For RemoveMessage to work, you need to use a state key with add_messages reducer , like MessagesState ."
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage  \n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from langchain.messages import RemoveMessage  \n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_sequence([call_model, delete_messages])\nbuilder.add_edge(START, \"call_model\")\n\ncheckpointer = InMemorySaver()\napp = builder.compile(checkpointer=checkpointer)\n\nfor event in app.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in app.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the MessagesState to include a summary key:"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarize_conversation node can be called after some number of messages have accumulated in the messages state key."
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "summarize_conversation"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "def summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "from typing import Any, TypedDict\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import AnyMessage\nfrom langchain_core.messages.utils import count_tokens_approximately\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langmem.short_term import SummarizationNode, RunningSummary  \n\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\nsummarization_model = model.bind(max_tokens=128)\n\nclass State(MessagesState):\n    context: dict[str, RunningSummary]  \n\nclass LLMInputState(TypedDict):  \n    summarized_messages: list[AnyMessage]\n    context: dict[str, RunningSummary]\n\nsummarization_node = SummarizationNode(  \n    token_counter=count_tokens_approximately,\n    model=summarization_model,\n    max_tokens=256,\n    max_tokens_before_summary=256,\n    max_summary_tokens=128,\n)\n\ndef call_model(state: LLMInputState):  \n    response = model.invoke(state[\"summarized_messages\"])\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(State)\nbuilder.add_node(call_model)\nbuilder.add_node(\"summarize\", summarization_node)  \nbuilder.add_edge(START, \"summarize\")\nbuilder.add_edge(\"summarize\", \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\nprint(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "================================== Ai Message ==================================\n\nFrom our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n\nSummary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "text",
        "content": "You can view and delete the information stored by the checkpointer."
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "config = {\n    \"configurable\": {\n        \"thread_id\": \"1\",  \n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #\n\n    }\n}\ngraph.get_state(config)  \n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "StateSnapshot(\n    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    created_at='2025-05-05T16:01:24.680462+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),\n    interrupts=()\n)\n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "config = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\nlist(graph.get_state_history(config))  \n"
    },
    {
        "title": "​Manage short-term memory",
        "type": "code",
        "content": "thread_id = \"1\"\ncheckpointer.delete_thread(thread_id)\n"
    },
    {
        "title": "​Prebuilt memory tools",
        "type": "text",
        "content": "LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Subgraphs",
        "type": "text",
        "content": "This guide explains the mechanics of using subgraphs. A subgraph is a graph that is used as a node in another graph.\n\nSubgraphs are useful for:\n\nWhen adding subgraphs, you need to define how the parent graph and the subgraph communicate:"
    },
    {
        "title": "​Setup",
        "type": "code",
        "content": "pip install -U langgraph\n"
    },
    {
        "title": "​Setup",
        "type": "text",
        "content": "Set up LangSmith for LangGraph development Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here ."
    },
    {
        "title": "​Invoke a graph from a node",
        "type": "text",
        "content": "A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have completely different schemas from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system.\n\nIf that’s the case for your application, you need to define a node function that invokes the subgraph . This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node."
    },
    {
        "title": "​Invoke a graph from a node",
        "type": "code",
        "content": "from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass SubgraphState(TypedDict):\n    bar: str\n\n# Subgraph\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"hi! \" + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nclass State(TypedDict):\n    foo: str\n\ndef call_subgraph(state: State):\n    # Transform the state to the subgraph state\n    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  \n    # Transform response back to the parent state\n    return {\"foo\": subgraph_output[\"bar\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", call_subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n"
    },
    {
        "title": "​Invoke a graph from a node",
        "type": "code",
        "content": "from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    # note that none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"baz\": \"baz\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\ndef node_2(state: ParentState):\n    # Transform the state to the subgraph state\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n    # Transform response back to the parent state\n    return {\"foo\": response[\"bar\"]}\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n    print(chunk)\n"
    },
    {
        "title": "​Invoke a graph from a node",
        "type": "code",
        "content": "((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n(('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_2': {'bar': 'hi! foobaz'}})\n((), {'node_2': {'foo': 'hi! foobaz'}})\n"
    },
    {
        "title": "​Invoke a graph from a node",
        "type": "text",
        "content": "This is an example with two levels of subgraphs: parent -> child -> grandchild."
    },
    {
        "title": "​Invoke a graph from a node",
        "type": "code",
        "content": "((), {'parent_1': {'my_key': 'hi Bob'}})\n(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n"
    },
    {
        "title": "​Add a graph as a node",
        "type": "text",
        "content": "When the parent graph and subgraph can communicate over a shared state key (channel) in the schema , you can add a graph as a node in another graph. For example, in multi-agent systems, the agents often communicate over a shared messages key.\n\nIf your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:"
    },
    {
        "title": "​Add a graph as a node",
        "type": "code",
        "content": "from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)  \nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n"
    },
    {
        "title": "​Add a graph as a node",
        "type": "code",
        "content": "from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # shared with parent graph state\n    bar: str  # private to SubgraphState\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}):\n    print(chunk)\n"
    },
    {
        "title": "​Add a graph as a node",
        "type": "code",
        "content": "{'node_1': {'foo': 'hi! foo'}}\n{'node_2': {'foo': 'hi! foobar'}}\n"
    },
    {
        "title": "​Add persistence",
        "type": "text",
        "content": "You only need to provide the checkpointer when compiling the parent graph . LangGraph will automatically propagate the checkpointer to the child subgraphs."
    },
    {
        "title": "​Add persistence",
        "type": "code",
        "content": "from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n"
    },
    {
        "title": "​Add persistence",
        "type": "text",
        "content": "If you want the subgraph to have its own memory , you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:"
    },
    {
        "title": "​Add persistence",
        "type": "code",
        "content": "subgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n"
    },
    {
        "title": "​View subgraph state",
        "type": "text",
        "content": "When you enable persistence , you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.\n\nYou can inspect the graph state via graph.get_state(config) . To view the subgraph state, you can use graph.get_state(config, subgraphs=True) ."
    },
    {
        "title": "​View subgraph state",
        "type": "code",
        "content": "graph.get_state(config)"
    },
    {
        "title": "​View subgraph state",
        "type": "code",
        "content": "graph.get_state(config, subgraphs=True)"
    },
    {
        "title": "​View subgraph state",
        "type": "text",
        "content": "Available only when interrupted Subgraph state can only be viewed when the subgraph is interrupted . Once you resume the graph, you won’t be able to access the subgraph state."
    },
    {
        "title": "​View subgraph state",
        "type": "code",
        "content": "from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import interrupt, Command\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    value = interrupt(\"Provide value:\")\n    return {\"foo\": state[\"foo\"] + value}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\n\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\ngraph.invoke({\"foo\": \"\"}, config)\nparent_state = graph.get_state(config)\n\n# This will be available only when the subgraph is interrupted.\n# Once you resume the graph, you won't be able to access the subgraph state.\nsubgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state\n\n# resume the subgraph\ngraph.invoke(Command(resume=\"bar\"), config)\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "text",
        "content": "To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs."
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "for chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    subgraphs=True, \n    stream_mode=\"updates\",\n):\n    print(chunk)\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    subgraphs=True, \n):\n    print(chunk)\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "code",
        "content": "((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\n"
    },
    {
        "title": "​Stream subgraph outputs",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "A LangGraph application consists of one or more graphs, a configuration file ( langgraph.json ), a file that specifies dependencies, and an optional .env file that specifies environment variables."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "This guide shows a typical structure of an application and shows how the required information to deploy an application using the LangSmith is specified."
    },
    {
        "title": "​Key Concepts",
        "type": "text",
        "content": "To deploy using the LangSmith, the following information should be provided:"
    },
    {
        "title": "​File structure",
        "type": "text",
        "content": "Below are examples of directory structures for applications:"
    },
    {
        "title": "​File structure",
        "type": "code",
        "content": "my-app/\n├── my_agent # all project code lies within here\n│   ├── utils # utilities for your graph\n│   │   ├── __init__.py\n│   │   ├── tools.py # tools for your graph\n│   │   ├── nodes.py # node functions for your graph\n│   │   └── state.py # state definition of your graph\n│   ├── __init__.py\n│   └── agent.py # code for constructing your graph\n├── .env # environment variables\n├── requirements.txt # package dependencies\n└── langgraph.json # configuration file for LangGraph\n"
    },
    {
        "title": "​File structure",
        "type": "text",
        "content": "The directory structure of a LangGraph application can vary depending on the programming language and the package manager used."
    },
    {
        "title": "​Configuration file",
        "type": "text",
        "content": "The langgraph.json file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application."
    },
    {
        "title": "​Configuration file",
        "type": "text",
        "content": "See the LangGraph configuration file reference for details on all supported keys in the JSON file.\n\nThe LangGraph CLI defaults to using the configuration file langgraph.json in the current directory."
    },
    {
        "title": "​Configuration file",
        "type": "code",
        "content": "./your_package/your_file.py"
    },
    {
        "title": "​Configuration file",
        "type": "code",
        "content": "{\n  \"dependencies\": [\"langchain_openai\", \"./your_package\"],\n  \"graphs\": {\n    \"my_agent\": \"./your_package/your_file.py:agent\"\n  },\n  \"env\": \"./.env\"\n}\n"
    },
    {
        "title": "​Dependencies",
        "type": "text",
        "content": "A LangGraph application may depend on other Python packages.\n\nYou will generally need to specify the following information for dependencies to be set up correctly:\n\nA file in the directory that specifies the dependencies (e.g. requirements.txt , pyproject.toml , or package.json )."
    },
    {
        "title": "​Dependencies",
        "type": "text",
        "content": "A dependencies key in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application."
    },
    {
        "title": "​Dependencies",
        "type": "text",
        "content": "Any additional binaries or system libraries can be specified using dockerfile_lines key in the LangGraph configuration file ."
    },
    {
        "title": "​Graphs",
        "type": "text",
        "content": "Use the graphs key in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application."
    },
    {
        "title": "​Graphs",
        "type": "text",
        "content": "You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined."
    },
    {
        "title": "​Environment variables",
        "type": "text",
        "content": "If you’re working with a deployed LangGraph application locally, you can configure environment variables in the env key of the LangGraph configuration file ."
    },
    {
        "title": "​Environment variables",
        "type": "text",
        "content": "For a production deployment, you will typically want to configure the environment variables in the deployment environment.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Studio",
        "type": "text",
        "content": "This guide will walk you through how to use Studio to visualize, interact, and debug your agent locally.\n\nStudio is our free-to-use, powerful agent IDE that integrates with LangSmith to enable tracing, evaluation, and prompt engineering. See exactly how your agent thinks, trace every decision, and ship smarter, more reliable agents."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "# Python >= 3.11 is required.\npip install --upgrade \"langgraph-cli[inmem]\"\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "We’ll use the following simple agent as an example:"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "from langchain.agents import create_agent\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email\"\"\"\n    email = {\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body\n    }\n    # ... email sending logic\n\n    return f\"Email sent to {to}\"\n\nagent = create_agent(\n    \"gpt-4o\",\n    tools=[send_email],\n    system_prompt=\"You are an email assistant. Always use the send_email tool.\",\n)\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Create a .env file in the root of your project and fill in the necessary API keys. We’ll need to set the LANGSMITH_API_KEY environment variable to the API key you get from LangSmith ."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Be sure not to commit your .env to version control systems such as Git!"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "LANGSMITH_API_KEY=lsv2...\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Inside your app’s directory, create a configuration file langgraph.json :"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.py:agent\"\n  },\n  \"env\": \".env\"\n}\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "create_agent automatically returns a compiled LangGraph graph that we can pass to the graphs key in our configuration file."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "See the LangGraph configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\n\nSo far, our project structure looks like this:"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "my-app/\n├── src\n│   └── agent.py\n├── .env\n└── langgraph.json\n"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "In the root of your new LangGraph app, install the dependencies:"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Safari blocks localhost connections to Studio. To work around this, run the above command with --tunnel to access Studio via a secure tunnel."
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Your agent will be accessible via API ( http://127.0.0.1:2024 ) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 :"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "http://127.0.0.1:2024"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "code",
        "content": "https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024"
    },
    {
        "title": "​Setup local LangGraph server",
        "type": "text",
        "content": "Studio makes each step of your agent easily observable. Replay any input and inspect the exact prompt, tool arguments, return values, and token/latency metrics. If a tool throws an exception, Studio records it with surrounding state so you can spend less time debugging.\n\nKeep your dev server running, edit prompts or tool signatures, and watch Studio hot-reload. Re-run the conversation thread from any step to verify behavior changes. See Manage threads for more details.\n\nAs your agent grows, the same view scales from a single-tool demo to multi-node graphs, keeping decisions legible and reproducible.\n\nFor an in-depth look at Studio, check out the overview page .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Test",
        "type": "text",
        "content": "After you’ve prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.\n\nNote that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out this section that uses LangChain’s built-in create_agent instead."
    },
    {
        "title": "​Prerequisites",
        "type": "code",
        "content": "$ pip install -U pytest\n"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.\n\nThe below example shows how this works with a simple, linear graph that progresses through node1 and node2 . Each node updates the single state key my_key :"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "import pytest\n\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\n\ndef create_graph() -> StateGraph:\n    class MyState(TypedDict):\n        my_key: str\n\n    graph = StateGraph(MyState)\n    graph.add_node(\"node1\", lambda state: {\"my_key\": \"hello from node1\"})\n    graph.add_node(\"node2\", lambda state: {\"my_key\": \"hello from node2\"})\n    graph.add_edge(START, \"node1\")\n    graph.add_edge(\"node1\", \"node2\")\n    graph.add_edge(\"node2\", END)\n    return graph\n\ndef test_basic_agent_execution() -> None:\n    checkpointer = MemorySaver()\n    graph = create_graph()\n    compiled_graph = graph.compile(checkpointer=checkpointer)\n    result = compiled_graph.invoke(\n        {\"my_key\": \"initial_value\"},\n        config={\"configurable\": {\"thread_id\": \"1\"}}\n    )\n    assert result[\"my_key\"] == \"hello from node2\"\n"
    },
    {
        "title": "​Testing individual nodes and edges",
        "type": "text",
        "content": "Compiled LangGraph agents expose references to each individual node as graph.nodes . You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:"
    },
    {
        "title": "​Testing individual nodes and edges",
        "type": "code",
        "content": "import pytest\n\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\n\ndef create_graph() -> StateGraph:\n    class MyState(TypedDict):\n        my_key: str\n\n    graph = StateGraph(MyState)\n    graph.add_node(\"node1\", lambda state: {\"my_key\": \"hello from node1\"})\n    graph.add_node(\"node2\", lambda state: {\"my_key\": \"hello from node2\"})\n    graph.add_edge(START, \"node1\")\n    graph.add_edge(\"node1\", \"node2\")\n    graph.add_edge(\"node2\", END)\n    return graph\n\ndef test_individual_node_execution() -> None:\n    # Will be ignored in this example\n    checkpointer = MemorySaver()\n    graph = create_graph()\n    compiled_graph = graph.compile(checkpointer=checkpointer)\n    # Only invoke node 1\n    result = compiled_graph.nodes[\"node1\"].invoke(\n        {\"my_key\": \"initial_value\"},\n    )\n    assert result[\"my_key\"] == \"hello from node1\"\n"
    },
    {
        "title": "​Partial execution",
        "type": "text",
        "content": "For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to restructure these sections as subgraphs , which you can invoke in isolation as normal.\n\nHowever, if you do not wish to make changes to your agent graph’s overall structure, you can use LangGraph’s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:"
    },
    {
        "title": "​Partial execution",
        "type": "text",
        "content": "Here’s an example that executes only the second and third nodes in a linear graph:"
    },
    {
        "title": "​Partial execution",
        "type": "code",
        "content": "import pytest\n\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\n\ndef create_graph() -> StateGraph:\n    class MyState(TypedDict):\n        my_key: str\n\n    graph = StateGraph(MyState)\n    graph.add_node(\"node1\", lambda state: {\"my_key\": \"hello from node1\"})\n    graph.add_node(\"node2\", lambda state: {\"my_key\": \"hello from node2\"})\n    graph.add_node(\"node3\", lambda state: {\"my_key\": \"hello from node3\"})\n    graph.add_node(\"node4\", lambda state: {\"my_key\": \"hello from node4\"})\n    graph.add_edge(START, \"node1\")\n    graph.add_edge(\"node1\", \"node2\")\n    graph.add_edge(\"node2\", \"node3\")\n    graph.add_edge(\"node3\", \"node4\")\n    graph.add_edge(\"node4\", END)\n    return graph\n\ndef test_partial_execution_from_node2_to_node3() -> None:\n    checkpointer = MemorySaver()\n    graph = create_graph()\n    compiled_graph = graph.compile(checkpointer=checkpointer)\n    compiled_graph.update_state(\n        config={\n          \"configurable\": {\n            \"thread_id\": \"1\"\n          }\n        },\n        # The state passed into node 2 - simulating the state at\n        # the end of node 1\n        values={\"my_key\": \"initial_value\"},\n        # Update saved state as if it came from node 1\n        # Execution will resume at node 2\n        as_node=\"node1\",\n    )\n    result = compiled_graph.invoke(\n        # Resume execution by passing None\n        None,\n        config={\"configurable\": {\"thread_id\": \"1\"}},\n        # Stop after node 3 so that node 4 doesn't run\n        interrupt_after=\"node3\",\n    )\n    assert result[\"my_key\"] == \"hello from node3\"\n"
    },
    {
        "title": "​Partial execution",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Deploy",
        "type": "text",
        "content": "LangSmith is the fastest way to turn agents into production systems. Traditional hosting platforms are built for stateless, short-lived web apps, while LangGraph is purpose-built for stateful, long-running agents , so you can go from repo to reliable cloud deployment in minutes."
    },
    {
        "title": "​Deploy your agent",
        "type": "text",
        "content": "Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide . Then, push your code to the repository.\n\nLog in to LangSmith . In the left sidebar, select Deployments .\n\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\n\nIf you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n\nSelect your application’s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\n\nOnce your application is deployed:"
    },
    {
        "title": "​Deploy your agent",
        "type": "code",
        "content": "pip install langgraph-sdk\n"
    },
    {
        "title": "​Deploy your agent",
        "type": "code",
        "content": "from langgraph_sdk import get_sync_client # or get_client for async\n\nclient = get_sync_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")\n\nfor chunk in client.runs.stream(\n    None,    # Threadless run\n    \"agent\", # Name of agent. Defined in langgraph.json.\n    input={\n        \"messages\": [{\n            \"role\": \"human\",\n            \"content\": \"What is LangGraph?\",\n        }],\n    },\n    stream_mode=\"updates\",\n):\n    print(f\"Receiving new event of type: {chunk.event}...\")\n    print(chunk.data)\n    print(\"\\n\\n\")\n"
    },
    {
        "title": "​Deploy your agent",
        "type": "text",
        "content": "LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the Platform setup overview .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Agent Chat UI",
        "type": "text",
        "content": "LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using create_agent . This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith )."
    },
    {
        "title": "​Agent Chat UI",
        "type": "text",
        "content": "Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking.\n\nAgent Chat UI is open source and can be adapted to your application needs.\n\nStudio automatically renders tool calls and results in an intuitive interface.\n\nNavigate through conversation history and fork from any point\n\nView and modify agent state at any point during execution\n\nBuilt-in support for reviewing and responding to agent requests\n\nYou can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph .\n\nThe fastest way to get started is using the hosted version:\n\nFor customization or local development, you can run Agent Chat UI locally:"
    },
    {
        "title": "​Agent Chat UI",
        "type": "code",
        "content": "# Create a new Agent Chat UI project\nnpx create-agent-chat-app --project-name my-chat-ui\ncd my-chat-ui\n\n# Install dependencies and start\npnpm install\npnpm dev\n"
    },
    {
        "title": "​Agent Chat UI",
        "type": "text",
        "content": "Agent Chat UI can connect to both local and deployed agents .\n\nAfter starting Agent Chat UI, you’ll need to configure it to connect to your agent:"
    },
    {
        "title": "​Agent Chat UI",
        "type": "code",
        "content": "http://localhost:2024"
    },
    {
        "title": "​Agent Chat UI",
        "type": "text",
        "content": "Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n\nAgent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Observability",
        "type": "text",
        "content": "Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use LangSmith to visualize these execution steps. To use it, enable tracing for your application . This enables you to do the following:"
    },
    {
        "title": "​Enable tracing",
        "type": "text",
        "content": "To enable tracing for your application, set the following environment variables:"
    },
    {
        "title": "​Enable tracing",
        "type": "code",
        "content": "export LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n"
    },
    {
        "title": "​Enable tracing",
        "type": "text",
        "content": "By default, the trace will be logged to the project with the name default . To configure a custom project name, see Log to a project ."
    },
    {
        "title": "​Trace selectively",
        "type": "text",
        "content": "You may opt to trace specific invocations or parts of your application using LangSmith’s tracing_context context manager:"
    },
    {
        "title": "​Trace selectively",
        "type": "code",
        "content": "import langsmith as ls\n\n# This WILL be traced\nwith ls.tracing_context(enabled=True):\n    agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]})\n\n# This will NOT be traced (if LANGSMITH_TRACING is not set)\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]})\n"
    },
    {
        "title": "​Log to a project",
        "type": "text",
        "content": "You can set a custom project name for your entire application by setting the LANGSMITH_PROJECT environment variable:"
    },
    {
        "title": "​Log to a project",
        "type": "code",
        "content": "export LANGSMITH_PROJECT=my-agent-project\n"
    },
    {
        "title": "​Log to a project",
        "type": "text",
        "content": "You can set the project name programmatically for specific operations:"
    },
    {
        "title": "​Log to a project",
        "type": "code",
        "content": "import langsmith as ls\n\nwith ls.tracing_context(project_name=\"email-agent-test\", enabled=True):\n    response = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]\n    })\n"
    },
    {
        "title": "​Add metadata to traces",
        "type": "text",
        "content": "You can annotate your traces with custom metadata and tags:"
    },
    {
        "title": "​Add metadata to traces",
        "type": "code",
        "content": "response = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]},\n    config={\n        \"tags\": [\"production\", \"email-assistant\", \"v1.0\"],\n        \"metadata\": {\n            \"user_id\": \"user_123\",\n            \"session_id\": \"session_456\",\n            \"environment\": \"production\"\n        }\n    }\n)\n"
    },
    {
        "title": "​Add metadata to traces",
        "type": "text",
        "content": "tracing_context also accepts tags and metadata for fine-grained control:"
    },
    {
        "title": "​Add metadata to traces",
        "type": "code",
        "content": "with ls.tracing_context(\n    project_name=\"email-agent-test\",\n    enabled=True,\n    tags=[\"production\", \"email-assistant\", \"v1.0\"],\n    metadata={\"user_id\": \"user_123\", \"session_id\": \"session_456\", \"environment\": \"production\"}):\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}\n    )\n"
    },
    {
        "title": "​Add metadata to traces",
        "type": "text",
        "content": "This custom metadata and tags will be attached to the trace in LangSmith.\n\nTo learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation ."
    },
    {
        "title": "​Use anonymizers to prevent logging of sensitive data in traces",
        "type": "text",
        "content": "You may want to mask sensitive data to prevent it from being logged to LangSmith. You can create anonymizers and apply them to\nyour graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith."
    },
    {
        "title": "​Use anonymizers to prevent logging of sensitive data in traces",
        "type": "code",
        "content": "from langchain_core.tracers.langchain import LangChainTracer\nfrom langgraph.graph import StateGraph, MessagesState\nfrom langsmith import Client\nfrom langsmith.anonymizer import create_anonymizer\n\nanonymizer = create_anonymizer([\n    # Matches SSNs\n    { \"pattern\": r\"\\b\\d{3}-?\\d{2}-?\\d{4}\\b\", \"replace\": \"<ssn>\" }\n])\n\ntracer_client = Client(anonymizer=anonymizer)\ntracer = LangChainTracer(client=tracer_client)\n# Define the graph\ngraph = (\n    StateGraph(MessagesState)\n    ...\n    .compile()\n    .with_config({'callbacks': [tracer]})\n)\n"
    },
    {
        "title": "​Use anonymizers to prevent logging of sensitive data in traces",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Deep Agents overview",
        "type": "text",
        "content": "deepagents is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents."
    },
    {
        "title": "​When to use deep agents",
        "type": "text",
        "content": "Use deep agents when you need agents that can:\n\nFor simpler use cases, consider using LangChain’s create_agent or building a custom LangGraph workflow."
    },
    {
        "title": "Planning and task decomposition",
        "type": "text",
        "content": "Deep agents include a built-in write_todos tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges."
    },
    {
        "title": "Context management",
        "type": "text",
        "content": "File system tools ( ls , read_file , write_file , edit_file ) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results."
    },
    {
        "title": "Subagent spawning",
        "type": "text",
        "content": "A built-in task tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent’s context clean while still going deep on specific subtasks."
    },
    {
        "title": "Long-term memory",
        "type": "text",
        "content": "Extend agents with persistent memory across threads using LangGraph’s Store. Agents can save and retrieve information from previous conversations."
    },
    {
        "title": "​Relationship to the LangChain ecosystem",
        "type": "text",
        "content": "Deep agents is built on top of:\n\nDeep agents applications can be deployed via LangSmith Deployment and monitored with LangSmith Observability ."
    },
    {
        "title": "Reference",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Deep Agents overview",
        "type": "text",
        "content": "deepagents is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents."
    },
    {
        "title": "​When to use deep agents",
        "type": "text",
        "content": "Use deep agents when you need agents that can:\n\nFor simpler use cases, consider using LangChain’s create_agent or building a custom LangGraph workflow."
    },
    {
        "title": "Planning and task decomposition",
        "type": "text",
        "content": "Deep agents include a built-in write_todos tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges."
    },
    {
        "title": "Context management",
        "type": "text",
        "content": "File system tools ( ls , read_file , write_file , edit_file ) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results."
    },
    {
        "title": "Subagent spawning",
        "type": "text",
        "content": "A built-in task tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent’s context clean while still going deep on specific subtasks."
    },
    {
        "title": "Long-term memory",
        "type": "text",
        "content": "Extend agents with persistent memory across threads using LangGraph’s Store. Agents can save and retrieve information from previous conversations."
    },
    {
        "title": "​Relationship to the LangChain ecosystem",
        "type": "text",
        "content": "Deep agents is built on top of:\n\nDeep agents applications can be deployed via LangSmith Deployment and monitored with LangSmith Observability ."
    },
    {
        "title": "Reference",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Quickstart",
        "type": "text",
        "content": "This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You’ll build a research agent that can conduct research and write reports."
    },
    {
        "title": "​Prerequisites",
        "type": "text",
        "content": "Before you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI)."
    },
    {
        "title": "​Prerequisites",
        "type": "code",
        "content": "pip install deepagents tavily-python\n"
    },
    {
        "title": "​Prerequisites",
        "type": "code",
        "content": "export ANTHROPIC_API_KEY=\"your-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"\n"
    },
    {
        "title": "​Prerequisites",
        "type": "code",
        "content": "import os\nfrom typing import Literal\nfrom tavily import TavilyClient\nfrom deepagents import create_deep_agent\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n"
    },
    {
        "title": "​Prerequisites",
        "type": "code",
        "content": "# System prompt to steer the agent to be an expert researcher\nresearch_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\n## `internet_search`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n\"\"\"\n\nagent = create_deep_agent(\n    tools=[internet_search],\n    system_prompt=research_instructions\n)\n"
    },
    {
        "title": "​Prerequisites",
        "type": "code",
        "content": "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n\n# Print the agent's response\nprint(result[\"messages\"][-1].content)\n"
    },
    {
        "title": "​Next steps",
        "type": "text",
        "content": "Now that you’ve built your first deep agent:\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Model",
        "type": "text",
        "content": "By default, deepagents uses \"claude-sonnet-4-5-20250929\" . You can customize this by passing any LangChain model object ."
    },
    {
        "title": "​Model",
        "type": "code",
        "content": "\"claude-sonnet-4-5-20250929\""
    },
    {
        "title": "​Model",
        "type": "code",
        "content": "from langchain.chat_models import init_chat_model\nfrom deepagents import create_deep_agent\n\nmodel = init_chat_model(\n    model=\"gpt-5\",\n)\nagent = create_deep_agent(\n    model=model,\n)\n"
    },
    {
        "title": "​System prompt",
        "type": "text",
        "content": "Deep agents come with a built-in system prompt inspired by Claude Code’s system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.\n\nEach deep agent tailored to a use case should include a custom system prompt specific to that use case."
    },
    {
        "title": "​System prompt",
        "type": "code",
        "content": "from deepagents import create_deep_agent\n\nresearch_instructions = \"\"\"\\\nYou are an expert researcher. Your job is to conduct \\\nthorough research, and then write a polished report. \\\n\"\"\"\n\nagent = create_deep_agent(\n    system_prompt=research_instructions,\n)\n"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to."
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "import os\nfrom typing import Literal\nfrom tavily import TavilyClient\nfrom deepagents import create_deep_agent\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n\nagent = create_deep_agent(\n    tools=[internet_search]\n)\n"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "In addition to any tools that you provide, deep agents also get access to a number of default tools:"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Agent harness capabilities",
        "type": "text",
        "content": "We think of deepagents as an “agent harness”. It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities."
    },
    {
        "title": "Agent harness capabilities",
        "type": "text",
        "content": "This page lists out the components that make up the agent harness."
    },
    {
        "title": "​File system access",
        "type": "text",
        "content": "The harness provides six tools for file system operations, making files first-class citizens in the agent’s environment:"
    },
    {
        "title": "​Large tool result eviction",
        "type": "text",
        "content": "The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.\n\nHow it works:"
    },
    {
        "title": "​Pluggable storage backends",
        "type": "text",
        "content": "The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.\n\nAvailable backends:\n\nStateBackend - Ephemeral in-memory storage\n\nFilesystemBackend - Real filesystem access\n\nStoreBackend - Persistent cross-conversation storage\n\nCompositeBackend - Route different paths to different backends"
    },
    {
        "title": "​Task delegation (subagents)",
        "type": "text",
        "content": "The harness allows the main agent to create ephemeral “subagents” for isolated multi-step tasks.\n\nWhy it’s useful:\n\nHow it works:"
    },
    {
        "title": "​Conversation history summarization",
        "type": "text",
        "content": "The harness automatically compresses old conversation history when token usage becomes excessive.\n\nConfiguration:\n\nWhy it’s useful:"
    },
    {
        "title": "​Dangling tool call repair",
        "type": "text",
        "content": "The harness fixes message history when tool calls are interrupted or cancelled before receiving results.\n\nThe problem:\n\nThe solution:\n\nWhy it’s useful:"
    },
    {
        "title": "​To-do list tracking",
        "type": "text",
        "content": "The harness provides a write_todos tool that agents can use to maintain a structured task list."
    },
    {
        "title": "​Human-in-the-Loop",
        "type": "text",
        "content": "The harness pauses agent execution at specified tool calls to allow human approval/modification.\n\nConfiguration:"
    },
    {
        "title": "​Prompt caching (Anthropic)",
        "type": "text",
        "content": "The harness enables Anthropic’s prompt caching feature to reduce redundant token processing.\n\nHow it works:\n\nWhy it’s useful:\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Backends",
        "type": "text",
        "content": "Deep agents expose a filesystem surface to the agent via tools like ls , read_file , write_file , edit_file , glob , and grep . These tools operate through a pluggable backend."
    },
    {
        "title": "Backends",
        "type": "text",
        "content": "This page explains how to choose a backend , route different paths to different backends , implement your own virtual filesystem (e.g., S3 or Postgres), add policy hooks , and comply with the backend protocol ."
    },
    {
        "title": "​Quickstart",
        "type": "text",
        "content": "Here are a few pre-built filesystem backends that you can quickly use with your deep agent:"
    },
    {
        "title": "​Quickstart",
        "type": "code",
        "content": "agent = create_deep_agent()"
    },
    {
        "title": "​Quickstart",
        "type": "code",
        "content": "agent = create_deep_agent(backend=FilesystemBackend(root_dir=\"/Users/nh/Desktop/\"))"
    },
    {
        "title": "​Quickstart",
        "type": "code",
        "content": "agent = create_deep_agent(backend=lambda rt: StoreBackend(rt))"
    },
    {
        "title": "​Built-in backends",
        "type": "code",
        "content": "# By default we provide a StateBackend\nagent = create_deep_agent()\n\n# Under the hood, it looks like\nfrom deepagents.backends import StateBackend\n\nagent = create_deep_agent(\n    backend=(lambda rt: StateBackend(rt))   # Note that the tools access State through the runtime.state\n)\n"
    },
    {
        "title": "​Built-in backends",
        "type": "code",
        "content": "from deepagents.backends import FilesystemBackend\n\nagent = create_deep_agent(\n    backend=FilesystemBackend(root_dir=\"/Users/nh/Desktop/\")\n)\n"
    },
    {
        "title": "​Built-in backends",
        "type": "code",
        "content": "from deepagents.backends import StoreBackend\n\nagent = create_deep_agent(\n    backend=(lambda rt: StoreBackend(rt))   # Note that the tools access Store through the runtime.store\n)\n"
    },
    {
        "title": "​Built-in backends",
        "type": "code",
        "content": "from deepagents import create_deep_agent\nfrom deepagents.backends import FilesystemBackend\nfrom deepagents.backends.composite import build_composite_state_backend\n\ncomposite_backend = lambda rt: CompositeBackend(\n    default=StateBackend(rt)\n    routes={\n        \"/memories/\": StoreBackend(rt),\n        \"/docs/\": CustomBackend()\n    }\n)\n\nagent = create_deep_agent(backend=composite_backend)\n"
    },
    {
        "title": "​Specify a backend",
        "type": "code",
        "content": "create_deep_agent(backend=...)"
    },
    {
        "title": "​Specify a backend",
        "type": "code",
        "content": "FilesystemBackend(root_dir=\".\")"
    },
    {
        "title": "​Specify a backend",
        "type": "code",
        "content": "BackendFactory = Callable[[ToolRuntime], BackendProtocol]"
    },
    {
        "title": "​Specify a backend",
        "type": "code",
        "content": "lambda rt: StateBackend(rt)"
    },
    {
        "title": "​Route to different backends",
        "type": "text",
        "content": "Route parts of the namespace to different backends. Commonly used to persist /memories/* and keep everything else ephemeral."
    },
    {
        "title": "​Route to different backends",
        "type": "code",
        "content": "from deepagents import create_deep_agent\nfrom deepagents.backends import FilesystemBackend\nfrom deepagents.backends.composite import build_composite_state_backend\n\ncomposite_backend = lambda rt: CompositeBackend(\n    routes={\n        \"/memories/\": FilesystemBackend(root_dir=\"/deepagents/myagent\"),\n    },\n)\n\nagent = create_deep_agent(backend=composite_backend)\n"
    },
    {
        "title": "​Route to different backends",
        "type": "code",
        "content": "\"/memories/projects/\""
    },
    {
        "title": "​Use a virtual filesystem",
        "type": "text",
        "content": "Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.\n\nDesign guidelines:"
    },
    {
        "title": "​Use a virtual filesystem",
        "type": "code",
        "content": "from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult\nfrom deepagents.backends.utils import FileInfo, GrepMatch\n\nclass S3Backend(BackendProtocol):\n    def __init__(self, bucket: str, prefix: str = \"\"):\n        self.bucket = bucket\n        self.prefix = prefix.rstrip(\"/\")\n\n    def _key(self, path: str) -> str:\n        return f\"{self.prefix}{path}\"\n\n    def ls_info(self, path: str) -> list[FileInfo]:\n        # List objects under _key(path); build FileInfo entries (path, size, modified_at)\n        ...\n\n    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:\n        # Fetch object; return numbered content or an error string\n        ...\n\n    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:\n        # Optionally filter server‑side; else list and scan content\n        ...\n\n    def glob_info(self, pattern: str, path: str = \"/\") -> list[FileInfo]:\n        # Apply glob relative to path across keys\n        ...\n\n    def write(self, file_path: str, content: str) -> WriteResult:\n        # Enforce create‑only semantics; return WriteResult(path=file_path, files_update=None)\n        ...\n\n    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:\n        # Read → replace (respect uniqueness vs replace_all) → write → return occurrences\n        ...\n"
    },
    {
        "title": "​Use a virtual filesystem",
        "type": "code",
        "content": "files(path text primary key, content text, created_at timestamptz, modified_at timestamptz)"
    },
    {
        "title": "​Use a virtual filesystem",
        "type": "code",
        "content": "WHERE path LIKE $1 || '%'"
    },
    {
        "title": "​Add policy hooks",
        "type": "text",
        "content": "Enforce enterprise rules by subclassing or wrapping a backend.\n\nBlock writes/edits under selected prefixes (subclass):"
    },
    {
        "title": "​Add policy hooks",
        "type": "code",
        "content": "from deepagents.backends.filesystem import FilesystemBackend\nfrom deepagents.backends.protocol import WriteResult, EditResult\n\nclass GuardedBackend(FilesystemBackend):\n    def __init__(self, *, deny_prefixes: list[str], **kwargs):\n        super().__init__(**kwargs)\n        self.deny_prefixes = [p if p.endswith(\"/\") else p + \"/\" for p in deny_prefixes]\n\n    def write(self, file_path: str, content: str) -> WriteResult:\n        if any(file_path.startswith(p) for p in self.deny_prefixes):\n            return WriteResult(error=f\"Writes are not allowed under {file_path}\")\n        return super().write(file_path, content)\n\n    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:\n        if any(file_path.startswith(p) for p in self.deny_prefixes):\n            return EditResult(error=f\"Edits are not allowed under {file_path}\")\n        return super().edit(file_path, old_string, new_string, replace_all)\n"
    },
    {
        "title": "​Add policy hooks",
        "type": "code",
        "content": "from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult\nfrom deepagents.backends.utils import FileInfo, GrepMatch\n\nclass PolicyWrapper(BackendProtocol):\n    def __init__(self, inner: BackendProtocol, deny_prefixes: list[str] | None = None):\n        self.inner = inner\n        self.deny_prefixes = [p if p.endswith(\"/\") else p + \"/\" for p in (deny_prefixes or [])]\n\n    def _deny(self, path: str) -> bool:\n        return any(path.startswith(p) for p in self.deny_prefixes)\n\n    def ls_info(self, path: str) -> list[FileInfo]:\n        return self.inner.ls_info(path)\n    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:\n        return self.inner.read(file_path, offset=offset, limit=limit)\n    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:\n        return self.inner.grep_raw(pattern, path, glob)\n    def glob_info(self, pattern: str, path: str = \"/\") -> list[FileInfo]:\n        return self.inner.glob_info(pattern, path)\n    def write(self, file_path: str, content: str) -> WriteResult:\n        if self._deny(file_path):\n            return WriteResult(error=f\"Writes are not allowed under {file_path}\")\n        return self.inner.write(file_path, content)\n    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:\n        if self._deny(file_path):\n            return EditResult(error=f\"Edits are not allowed under {file_path}\")\n        return self.inner.edit(file_path, old_string, new_string, replace_all)\n"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "ls_info(path: str) -> list[FileInfo]"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "read(file_path: str, offset: int = 0, limit: int = 2000) -> str"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "\"Error: File '/x' not found\""
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "grep_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "\"Invalid regex pattern: ...\""
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "glob_info(pattern: str, path: str = \"/\") -> list[FileInfo]"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "write(file_path: str, content: str) -> WriteResult"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "WriteResult(error=...)"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "edit(file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "WriteResult(error, path, files_update)"
    },
    {
        "title": "​Protocol reference",
        "type": "code",
        "content": "EditResult(error, path, files_update, occurrences)"
    },
    {
        "title": "​Protocol reference",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Subagents",
        "type": "text",
        "content": "Deep agents can create subagents to delegate work. You can specify custom subagents in the subagents parameter. Subagents are useful for context quarantine (keeping the main agent’s context clean) and for providing specialized instructions."
    },
    {
        "title": "​Why use subagents?",
        "type": "text",
        "content": "Subagents solve the context bloat problem . When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work—the main agent receives only the final result, not the dozens of tool calls that produced it.\n\nWhen to use subagents:\n\nWhen NOT to use subagents:"
    },
    {
        "title": "​Configuration",
        "type": "text",
        "content": "subagents should be a list of dictionaries or CompiledSubAgent objects. There are two types:"
    },
    {
        "title": "​Configuration",
        "type": "text",
        "content": "For most use cases, define subagents as dictionaries:\n\nRequired fields:"
    },
    {
        "title": "​Configuration",
        "type": "code",
        "content": "\"provider:model-name\""
    },
    {
        "title": "​Configuration",
        "type": "text",
        "content": "For complex workflows, use a pre-built LangGraph graph:\n\nFields:"
    },
    {
        "title": "​Using SubAgent",
        "type": "code",
        "content": "import os\nfrom typing import Literal\nfrom tavily import TavilyClient\nfrom deepagents import create_deep_agent\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n\nresearch_subagent = {\n    \"name\": \"research-agent\",\n    \"description\": \"Used to research more in depth questions\",\n    \"system_prompt\": \"You are a great researcher\",\n    \"tools\": [internet_search],\n    \"model\": \"openai:gpt-4o\",  # Optional override, defaults to main agent model\n}\nsubagents = [research_subagent]\n\nagent = create_deep_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    subagents=subagents\n)\n"
    },
    {
        "title": "​Using CompiledSubAgent",
        "type": "text",
        "content": "For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:"
    },
    {
        "title": "​Using CompiledSubAgent",
        "type": "code",
        "content": "from deepagents import create_deep_agent, CompiledSubAgent\nfrom langchain.agents import create_agent\n\n# Create a custom agent graph\ncustom_graph = create_agent(\n    model=your_model,\n    tools=specialized_tools,\n    prompt=\"You are a specialized agent for data analysis...\"\n)\n\n# Use it as a custom subagent\ncustom_subagent = CompiledSubAgent(\n    name=\"data-analyzer\",\n    description=\"Specialized agent for complex data analysis tasks\",\n    runnable=custom_graph\n)\n\nsubagents = [custom_subagent]\n\nagent = create_deep_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[internet_search],\n    system_prompt=research_instructions,\n    subagents=subagents\n)\n"
    },
    {
        "title": "​The general-purpose subagent",
        "type": "text",
        "content": "In addition to any user-defined subagents, deep agents have access to a general-purpose subagent at all times. This subagent:"
    },
    {
        "title": "​The general-purpose subagent",
        "type": "text",
        "content": "The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls."
    },
    {
        "title": "Example",
        "type": "text",
        "content": "Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: task(name=\"general-purpose\", task=\"Research quantum computing trends\") . The subagent performs all the searches internally and returns only a summary."
    },
    {
        "title": "Example",
        "type": "code",
        "content": "task(name=\"general-purpose\", task=\"Research quantum computing trends\")"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "The main agent uses descriptions to decide which subagent to call. Be specific:\n\n✅ Good: \"Analyzes financial data and generates investment insights with confidence scores\""
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "\"Analyzes financial data and generates investment insights with confidence scores\""
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "Include specific guidance on how to use tools and format outputs:"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "research_subagent = {\n    \"name\": \"research-agent\",\n    \"description\": \"Conducts in-depth research using web search and synthesizes findings\",\n    \"system_prompt\": \"\"\"You are a thorough researcher. Your job is to:\n\n    1. Break down the research question into searchable queries\n    2. Use internet_search to find relevant information\n    3. Synthesize findings into a comprehensive but concise summary\n    4. Cite sources when making claims\n\n    Output format:\n    - Summary (2-3 paragraphs)\n    - Key findings (bullet points)\n    - Sources (with URLs)\n\n    Keep your response under 500 words to maintain clean context.\"\"\",\n    \"tools\": [internet_search],\n}\n"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "Only give subagents the tools they need. This improves focus and security:"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "# ✅ Good: Focused tool set\nemail_agent = {\n    \"name\": \"email-sender\",\n    \"tools\": [send_email, validate_email],  # Only email-related\n}\n\n# ❌ Bad: Too many tools\nemail_agent = {\n    \"name\": \"email-sender\",\n    \"tools\": [send_email, web_search, database_query, file_upload],  # Unfocused\n}\n"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "subagents = [\n    {\n        \"name\": \"contract-reviewer\",\n        \"description\": \"Reviews legal documents and contracts\",\n        \"system_prompt\": \"You are an expert legal reviewer...\",\n        \"tools\": [read_document, analyze_contract],\n        \"model\": \"claude-sonnet-4-5-20250929\",  # Large context for long documents\n    },\n    {\n        \"name\": \"financial-analyst\",\n        \"description\": \"Analyzes financial data and market trends\",\n        \"system_prompt\": \"You are an expert financial analyst...\",\n        \"tools\": [get_stock_price, analyze_fundamentals],\n        \"model\": \"openai:gpt-4o\",  # Better for numerical analysis\n    },\n]\n"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "Instruct subagents to return summaries, not raw data:"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "data_analyst = {\n    \"system_prompt\": \"\"\"Analyze the data and return:\n    1. Key insights (3-5 bullet points)\n    2. Overall confidence score\n    3. Recommended next actions\n\n    Do NOT include:\n    - Raw data\n    - Intermediate calculations\n    - Detailed tool outputs\n\n    Keep response under 300 words.\"\"\"\n}\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "Create specialized subagents for different domains:"
    },
    {
        "title": "​Common patterns",
        "type": "code",
        "content": "from deepagents import create_deep_agent\n\nsubagents = [\n    {\n        \"name\": \"data-collector\",\n        \"description\": \"Gathers raw data from various sources\",\n        \"system_prompt\": \"Collect comprehensive data on the topic\",\n        \"tools\": [web_search, api_call, database_query],\n    },\n    {\n        \"name\": \"data-analyzer\",\n        \"description\": \"Analyzes collected data for insights\",\n        \"system_prompt\": \"Analyze data and extract key insights\",\n        \"tools\": [statistical_analysis],\n    },\n    {\n        \"name\": \"report-writer\",\n        \"description\": \"Writes polished reports from analysis\",\n        \"system_prompt\": \"Create professional reports from insights\",\n        \"tools\": [format_document],\n    },\n]\n\nagent = create_deep_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    system_prompt=\"You coordinate data analysis and reporting. Use subagents for specialized tasks.\",\n    subagents=subagents\n)\n"
    },
    {
        "title": "​Common patterns",
        "type": "text",
        "content": "Workflow:\n\nEach subagent works with clean context focused only on its task."
    },
    {
        "title": "​Troubleshooting",
        "type": "text",
        "content": "Problem : Main agent tries to do work itself instead of delegating.\n\nSolutions :\n\nMake descriptions more specific:"
    },
    {
        "title": "​Troubleshooting",
        "type": "code",
        "content": "# ✅ Good\n{\"name\": \"research-specialist\", \"description\": \"Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches.\"}\n\n# ❌ Bad\n{\"name\": \"helper\", \"description\": \"helps with stuff\"}\n"
    },
    {
        "title": "​Troubleshooting",
        "type": "code",
        "content": "agent = create_deep_agent(\n    system_prompt=\"\"\"...your instructions...\n\n    IMPORTANT: For complex tasks, delegate to your subagents using the task() tool.\n    This keeps your context clean and improves results.\"\"\",\n    subagents=[...]\n)\n"
    },
    {
        "title": "​Troubleshooting",
        "type": "text",
        "content": "Problem : Context fills up despite using subagents.\n\nSolutions :\n\nInstruct subagent to return concise results:"
    },
    {
        "title": "​Troubleshooting",
        "type": "code",
        "content": "system_prompt=\"\"\"...\n\nIMPORTANT: Return only the essential summary.\nDo NOT include raw data, intermediate search results, or detailed tool outputs.\nYour response should be under 500 words.\"\"\"\n"
    },
    {
        "title": "​Troubleshooting",
        "type": "code",
        "content": "system_prompt=\"\"\"When you gather large amounts of data:\n1. Save raw data to /data/raw_results.txt\n2. Process and analyze the data\n3. Return only the analysis summary\n\nThis keeps context clean.\"\"\"\n"
    },
    {
        "title": "​Troubleshooting",
        "type": "text",
        "content": "Problem : Main agent calls inappropriate subagent for the task.\n\nSolution : Differentiate subagents clearly in descriptions:"
    },
    {
        "title": "​Troubleshooting",
        "type": "code",
        "content": "subagents = [\n    {\n        \"name\": \"quick-researcher\",\n        \"description\": \"For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.\",\n    },\n    {\n        \"name\": \"deep-researcher\",\n        \"description\": \"For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.\",\n    }\n]\n"
    },
    {
        "title": "​Troubleshooting",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Human-in-the-loop",
        "type": "text",
        "content": "Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the interrupt_on parameter."
    },
    {
        "title": "​Basic configuration",
        "type": "text",
        "content": "The interrupt_on parameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with:"
    },
    {
        "title": "​Basic configuration",
        "type": "code",
        "content": "{\"allowed_decisions\": [...]}"
    },
    {
        "title": "​Basic configuration",
        "type": "code",
        "content": "from langchain_core.tools import tool\nfrom deepagents import create_deep_agent\nfrom langgraph.checkpoint.memory import MemorySaver\n\n@tool\ndef delete_file(path: str) -> str:\n    \"\"\"Delete a file from the filesystem.\"\"\"\n    return f\"Deleted {path}\"\n\n@tool\ndef read_file(path: str) -> str:\n    \"\"\"Read a file from the filesystem.\"\"\"\n    return f\"Contents of {path}\"\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> str:\n    \"\"\"Send an email.\"\"\"\n    return f\"Sent email to {to}\"\n\n# Checkpointer is REQUIRED for human-in-the-loop\ncheckpointer = MemorySaver()\n\nagent = create_deep_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[delete_file, read_file, send_email],\n    interrupt_on={\n        \"delete_file\": True,  # Default: approve, edit, reject\n        \"read_file\": False,   # No interrupts needed\n        \"send_email\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing\n    },\n    checkpointer=checkpointer  # Required!\n)\n"
    },
    {
        "title": "​Decision types",
        "type": "text",
        "content": "The allowed_decisions list controls what actions a human can take when reviewing a tool call:"
    },
    {
        "title": "​Decision types",
        "type": "text",
        "content": "You can customize which decisions are available for each tool:"
    },
    {
        "title": "​Decision types",
        "type": "code",
        "content": "interrupt_on = {\n    # Sensitive operations: allow all options\n    \"delete_file\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n\n    # Moderate risk: approval or rejection only\n    \"write_file\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n\n    # Must approve (no rejection allowed)\n    \"critical_operation\": {\"allowed_decisions\": [\"approve\"]},\n}\n"
    },
    {
        "title": "​Handle interrupts",
        "type": "text",
        "content": "When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly."
    },
    {
        "title": "​Handle interrupts",
        "type": "code",
        "content": "import uuid\nfrom langgraph.types import Command\n\n# Create config with thread_id for state persistence\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n\n# Invoke the agent\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Delete the file temp.txt\"}]\n}, config=config)\n\n# Check if execution was interrupted\nif result.get(\"__interrupt__\"):\n    # Extract interrupt information\n    interrupts = result[\"__interrupt__\"][0].value\n    action_requests = interrupts[\"action_requests\"]\n    review_configs = interrupts[\"review_configs\"]\n\n    # Create a lookup map from tool name to review config\n    config_map = {cfg[\"action_name\"]: cfg for cfg in review_configs}\n\n    # Display the pending actions to the user\n    for action in action_requests:\n        review_config = config_map[action[\"name\"]]\n        print(f\"Tool: {action['name']}\")\n        print(f\"Arguments: {action['args']}\")\n        print(f\"Allowed decisions: {review_config['allowed_decisions']}\")\n\n    # Get user decisions (one per action_request, in order)\n    decisions = [\n        {\"type\": \"approve\"}  # User approved the deletion\n    ]\n\n    # Resume execution with decisions\n    result = agent.invoke(\n        Command(resume={\"decisions\": decisions}),\n        config=config  # Must use the same config!\n    )\n\n# Process final result\nprint(result[\"messages\"][-1][\"content\"])\n"
    },
    {
        "title": "​Multiple tool calls",
        "type": "text",
        "content": "When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order."
    },
    {
        "title": "​Multiple tool calls",
        "type": "code",
        "content": "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n\nresult = agent.invoke({\n    \"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"Delete temp.txt and send an email to admin@example.com\"\n    }]\n}, config=config)\n\nif result.get(\"__interrupt__\"):\n    interrupts = result[\"__interrupt__\"][0].value\n    action_requests = interrupts[\"action_requests\"]\n\n    # Two tools need approval\n    assert len(action_requests) == 2\n\n    # Provide decisions in the same order as action_requests\n    decisions = [\n        {\"type\": \"approve\"},  # First tool: delete_file\n        {\"type\": \"reject\"}    # Second tool: send_email\n    ]\n\n    result = agent.invoke(\n        Command(resume={\"decisions\": decisions}),\n        config=config\n    )\n"
    },
    {
        "title": "​Edit tool arguments",
        "type": "text",
        "content": "When \"edit\" is in the allowed decisions, you can modify the tool arguments before execution:"
    },
    {
        "title": "​Edit tool arguments",
        "type": "code",
        "content": "if result.get(\"__interrupt__\"):\n    interrupts = result[\"__interrupt__\"][0].value\n    action_request = interrupts[\"action_requests\"][0]\n\n    # Original args from the agent\n    print(action_request[\"args\"])  # {\"to\": \"everyone@company.com\", ...}\n\n    # User decides to edit the recipient\n    decisions = [{\n        \"type\": \"edit\",\n        \"edited_action\": {\n            \"name\": action_request[\"name\"],  # Must include the tool name\n            \"args\": {\"to\": \"team@company.com\", \"subject\": \"...\", \"body\": \"...\"}\n        }\n    }]\n\n    result = agent.invoke(\n        Command(resume={\"decisions\": decisions}),\n        config=config\n    )\n"
    },
    {
        "title": "​Subagent interrupts",
        "type": "text",
        "content": "Each subagent can have its own interrupt_on configuration that overrides the main agent’s settings:"
    },
    {
        "title": "​Subagent interrupts",
        "type": "code",
        "content": "agent = create_deep_agent(\n    tools=[delete_file, read_file],\n    interrupt_on={\n        \"delete_file\": True,\n        \"read_file\": False,\n    },\n    subagents=[{\n        \"name\": \"file-manager\",\n        \"description\": \"Manages file operations\",\n        \"system_prompt\": \"You are a file management assistant.\",\n        \"tools\": [delete_file, read_file],\n        \"interrupt_on\": {\n            # Override: require approval for reads in this subagent\n            \"delete_file\": True,\n            \"read_file\": True,  # Different from main agent!\n        }\n    }],\n    checkpointer=checkpointer\n)\n"
    },
    {
        "title": "​Subagent interrupts",
        "type": "text",
        "content": "When a subagent triggers an interrupt, the handling is the same – check for __interrupt__ and resume with Command ."
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "from langgraph.checkpoint.memory import MemorySaver\n\ncheckpointer = MemorySaver()\nagent = create_deep_agent(\n    tools=[...],\n    interrupt_on={...},\n    checkpointer=checkpointer  # Required for HITL\n)\n"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "When resuming, you must use the same config with the same thread_id :"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "# First call\nconfig = {\"configurable\": {\"thread_id\": \"my-thread\"}}\nresult = agent.invoke(input, config=config)\n\n# Resume (use same config)\nresult = agent.invoke(Command(resume={...}), config=config)\n"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "The decisions list must match the order of action_requests :"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "if result.get(\"__interrupt__\"):\n    interrupts = result[\"__interrupt__\"][0].value\n    action_requests = interrupts[\"action_requests\"]\n\n    # Create one decision per action, in order\n    decisions = []\n    for action in action_requests:\n        decision = get_user_decision(action)  # Your logic\n        decisions.append(decision)\n\n    result = agent.invoke(\n        Command(resume={\"decisions\": decisions}),\n        config=config\n    )\n"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "Configure different tools based on their risk level:"
    },
    {
        "title": "​Best practices",
        "type": "code",
        "content": "interrupt_on = {\n    # High risk: full control (approve, edit, reject)\n    \"delete_file\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n    \"send_email\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n\n    # Medium risk: no editing allowed\n    \"write_file\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n\n    # Low risk: no interrupts\n    \"read_file\": False,\n    \"list_files\": False,\n}\n"
    },
    {
        "title": "​Best practices",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Integration packages",
        "type": "text",
        "content": "LangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more."
    },
    {
        "title": "Tools and toolkits",
        "type": "text",
        "content": "To see a full list of integrations by component type, refer to the categories in the sidebar."
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-huggingface"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-nvidia-ai-endpoints"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-elasticsearch"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-unstructured"
    },
    {
        "title": "​All providers",
        "type": "text",
        "content": "See all providers or search for a provider using the search field.\n\nIf you’d like to contribute an integration, see our contributing guide .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Integration packages",
        "type": "text",
        "content": "LangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more."
    },
    {
        "title": "Tools and toolkits",
        "type": "text",
        "content": "To see a full list of integrations by component type, refer to the categories in the sidebar."
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-huggingface"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-nvidia-ai-endpoints"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-elasticsearch"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Popular providers",
        "type": "code",
        "content": "langchain-unstructured"
    },
    {
        "title": "​All providers",
        "type": "text",
        "content": "See all providers or search for a provider using the search field.\n\nIf you’d like to contribute an integration, see our contributing guide .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "All integration providers",
        "type": "text",
        "content": "Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more."
    },
    {
        "title": "Abso",
        "type": "text",
        "content": "Custom AI integration platform for enterprise workflows."
    },
    {
        "title": "Acreom",
        "type": "text",
        "content": "Knowledge management platform with AI-powered organization."
    },
    {
        "title": "ActiveLoop DeepLake",
        "type": "text",
        "content": "Vector database for AI applications with deep learning focus."
    },
    {
        "title": "Ads4GPTs",
        "type": "text",
        "content": "Advertising platform for GPT applications and AI services."
    },
    {
        "title": "Airbyte",
        "type": "text",
        "content": "Data integration platform for ETL and ELT pipelines."
    },
    {
        "title": "Aleph Alpha",
        "type": "text",
        "content": "European AI company’s multilingual language models."
    },
    {
        "title": "Anthropic",
        "type": "text",
        "content": "Claude models for advanced reasoning and conversation."
    },
    {
        "title": "AWS",
        "type": "text",
        "content": "Amazon Web Services cloud platform and AI services."
    },
    {
        "title": "Datadog",
        "type": "text",
        "content": "Monitoring and analytics platform for applications."
    },
    {
        "title": "Vectara",
        "type": "text",
        "content": "Neural search platform with built-in understanding."
    },
    {
        "title": "Zotero",
        "type": "text",
        "content": "Reference management and research tool.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "AzureOpenAI",
        "type": "text",
        "content": "Wrapper for (legacy) OpenAI text completion models hosted on Azure."
    },
    {
        "title": "AzureOpenAIEmbeddings",
        "type": "text",
        "content": "Wrapper for OpenAI embedding models hosted on Azure."
    },
    {
        "title": "Dall-E Image Generator",
        "type": "text",
        "content": "Text-to-image generation using OpenAI’s Dall-E models."
    },
    {
        "title": "ChatGPTPluginRetriever",
        "type": "text",
        "content": "Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc."
    },
    {
        "title": "ChatGPTLoader",
        "type": "text",
        "content": "Load conversations.json from your ChatGPT data export folder."
    },
    {
        "title": "OpenAIModerationChain",
        "type": "text",
        "content": "Detect text that could be hateful, violent, etc.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "AnthropicLLM",
        "type": "text",
        "content": "(Legacy) Anthropic text completion models.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Google",
        "type": "text",
        "content": "All LangChain integrations with Google Cloud , Google Gemini and other Google products."
    },
    {
        "title": "Google",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "Google",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "Google",
        "type": "code",
        "content": "langchain-google-cloud-sql-pg"
    },
    {
        "title": "Google",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "Google",
        "type": "text",
        "content": "See Google’s guide on migrating from the Gemini API to Vertex AI for more details on the differences.\n\nIntegration packages for Gemini models and the Vertex AI platform are maintained in the langchain-google repository. You can find a host of LangChain integrations with other Google APIs and services in the googleapis Github organization and the langchain-google-community package."
    },
    {
        "title": "Google",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "text",
        "content": "Access Google Gemini models directly using the Gemini API, best suited for rapid development and experimentation. Gemini models are available in Google AI Studio ."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "pip install -U langchain-google-genai\n"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "text",
        "content": "Start for free and get your API key from Google AI Studio ."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "export GOOGLE_API_KEY=\"YOUR_API_KEY\"\n"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "text",
        "content": "Use the ChatGoogleGenerativeAI class to interact with Gemini models. See\ndetails in this guide ."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "ChatGoogleGenerativeAI"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "from langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.messages import HumanMessage\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n\n# Simple text invocation\nresult = llm.invoke(\"Sing a ballad of LangChain.\")\nprint(result.content)\n\n# Multimodal invocation with gemini-pro-vision\nmessage = HumanMessage(\n    content=[\n        {\n            \"type\": \"text\",\n            \"text\": \"What's in this image?\",\n        },\n        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n    ]\n)\nresult = llm.invoke([message])\nprint(result.content)\n"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "text",
        "content": "The image_url can be a public URL, a GCS URI ( gs://... ), a local file path, a base64 encoded image string ( data:image/png;base64,... ), or a PIL Image object."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "data:image/png;base64,..."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "text",
        "content": "Generate text embeddings using models like gemini-embedding-001 with the GoogleGenerativeAIEmbeddings class."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "GoogleGenerativeAIEmbeddings"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\nvector = embeddings.embed_query(\"What are embeddings?\")\nprint(vector[:5])\n"
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "text",
        "content": "Access the same Gemini models using the (legacy) LLM\ninterface with the GoogleGenerativeAI class."
    },
    {
        "title": "​Google Generative AI (Gemini API & AI Studio)",
        "type": "code",
        "content": "from langchain_google_genai import GoogleGenerativeAI\n\nllm = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\nresult = llm.invoke(\"Sing a ballad of LangChain.\")\nprint(result)\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Access Gemini models, Vertex AI Model Garden and other Google Cloud services via Vertex AI and specific cloud integrations.\n\nVertex AI models require the langchain-google-vertexai package. Other services might require additional packages like langchain-google-community , langchain-google-cloud-sql-pg , etc."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-cloud-sql-pg"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-vertexai\n# pip install langchain-google-community[...] # For other services\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud integrations typically use Application Default Credentials (ADC). Refer to the Google Cloud authentication documentation for setup instructions (e.g., using gcloud auth application-default login )."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "gcloud auth application-default login"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Access chat models like Gemini via the Vertex AI platform.\n\nSee a usage example ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai import ChatVertexAI\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.model_garden_maas.mistral import VertexModelGardenMistral\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Local Gemma model loaded from HuggingFace. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.gemma import GemmaChatLocalHF\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Local Gemma model loaded from Kaggle. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.gemma import GemmaChatLocalKaggle\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.gemma import GemmaChatVertexAIModelGarden\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Implementation of the Image Captioning model as a chat. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.vision_models import VertexAIImageCaptioningChat\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Given an image and a prompt, edit the image. Currently only supports mask-free editing. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.vision_models import VertexAIImageEditorChat\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Generates an image from a prompt. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Chat implementation of a visual QnA model. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.vision_models import VertexAIVisualQnAChat\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "You can also use the (legacy) string-in, string-out LLM\ninterface.\n\nAccess Gemini, and hundreds of OSS models via Vertex AI Model Garden service. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai import VertexAIModelGarden\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Local Gemma model loaded from HuggingFace. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.gemma import GemmaLocalHF\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Local Gemma model loaded from Kaggle. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.gemma import GemmaLocalKaggle\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Implementation of the Image Captioning model as an LLM. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.vision_models import VertexAIImageCaptioning\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Generate embeddings using models deployed on Vertex AI. Requires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai import VertexAIEmbeddings\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Load documents from various Google Cloud sources.\n\nGoogle Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-alloydb-pg\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_alloydb_pg import AlloyDBLoader # AlloyDBEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud BigQuery is a serverless data warehouse.\n\nInstall with BigQuery dependencies:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[bigquery]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import BigQueryLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Bigtable is a fully managed NoSQL Big Data database service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-bigtable\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_bigtable import BigtableLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud SQL for MySQL is a fully-managed MySQL database service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-cloud-sql-mysql\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_cloud_sql_mysql import MySQLLoader # MySQLEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-cloud-sql-mssql\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_cloud_sql_mssql import MSSQLLoader # MSSQLEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-cloud-sql-pg\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_cloud_sql_pg import PostgresLoader # PostgresEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Cloud Storage is a managed service for storing unstructured data.\n\nInstall with GCS dependencies:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[gcs]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Load from a directory or a specific file:\n\nSee directory usage example ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import GCSDirectoryLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import GCSFileLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Load data using Google Cloud Vision API.\n\nInstall with Vision dependencies:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[vision]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community.vision import CloudVisionLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google El Carro Oracle Operator runs Oracle databases in Kubernetes.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-el-carro\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_el_carro import ElCarroLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Firestore is a NoSQL document database.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-firestore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_firestore import FirestoreLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Firestore in Datastore mode .\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-datastore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_datastore import DatastoreLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Memorystore for Redis is a fully managed Redis service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-memorystore-redis\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_memorystore_redis import MemorystoreDocumentLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Spanner is a fully managed, globally distributed relational database service.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-spanner\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_spanner import SpannerLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Speech-to-Text transcribes audio files.\n\nInstall with Speech-to-Text dependencies:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[speech]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import SpeechToTextLoader\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Transform documents using Google Cloud services.\n\nGoogle Cloud Document AI is a Google Cloud\nservice that transforms unstructured data from documents into structured data, making it easier\nto understand, analyze, and consume.\n\nWe need to set up a GCS bucket and create your own OCR processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs:// )\nand a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID .\nWe can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[docai]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_core.document_loaders.blob_loaders import Blob\nfrom langchain_google_community import DocAIParser\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Translate is a multilingual neural machine\ntranslation service developed by Google to translate text, documents and websites\nfrom one language into another.\n\nThe GoogleTranslateTransformer allows you to translate text and HTML with the Google Cloud Translation API ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "GoogleTranslateTransformer"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "First, we need to install the langchain-google-community with translate dependencies."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[translate]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import GoogleTranslateTransformer\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Store and search vectors using Google Cloud databases and Vertex AI Vector Search.\n\nGoogle Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-alloydb-pg\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_alloydb_pg import AlloyDBVectorStore # AlloyDBEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud BigQuery ,\nBigQuery is a serverless and cost-effective enterprise data warehouse in Google Cloud.\n\nGoogle Cloud BigQuery Vector Search BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.\n\nIt can calculate Euclidean or Cosine distance. With LangChain, we default to use Euclidean distance.\n\nWe need to install several python packages."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-cloud-bigquery\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "# Note: BigQueryVectorSearch might be in langchain or langchain_community depending on version\n# Check imports in the usage example.\nfrom langchain.vectorstores import BigQueryVectorSearch # Or langchain_community.vectorstores\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Vector store using Memorystore for Redis .\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-memorystore-redis\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_memorystore_redis import RedisVectorStore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Vector store using Cloud Spanner .\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-spanner\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_spanner import SpannerVectorStore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Vector store using Firestore .\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-firestore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_firestore import FirestoreVectorStore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Vector store using Cloud SQL for MySQL .\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-cloud-sql-mysql\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_cloud_sql_mysql import MySQLVectorStore # MySQLEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Vector store using Cloud SQL for PostgreSQL .\n\nInstall the python package:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-cloud-sql-pg\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_cloud_sql_pg import PostgresVectorStore # PostgresEngine also available\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Google Cloud Vertex AI Vector Search from Google Cloud,\nformerly known as Vertex AI Matching Engine , provides the industry’s leading high-scale\nlow latency vector database. These vector databases are commonly\nreferred to as vector similarity-matching or an approximate nearest neighbor (ANN) service."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "Vertex AI Matching Engine"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-vertexai\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai import VectorSearchVectorStore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Vector search using Datastore for document storage.\n\nSee usage example ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai import VectorSearchVectorStoreDatastore\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Alias for VectorSearchVectorStore storing documents/index in GCS."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "VectorSearchVectorStore"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai import VectorSearchVectorStoreGCS\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Retrieve information using Google Cloud services.\n\nBuild generative AI powered search engines using Vertex AI Search .\nfrom Google Cloud allows developers to quickly build generative AI powered search engines for customers and employees.\n\nSee a usage example .\n\nNote: GoogleVertexAISearchRetriever is deprecated. Use the components below from langchain-google-community ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "GoogleVertexAISearchRetriever"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Install the google-cloud-discoveryengine package for underlying access."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "google-cloud-discoveryengine"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-cloud-discoveryengine langchain-google-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import VertexAIMultiTurnSearchRetriever\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "# Note: The example code shows VertexAIMultiTurnSearchRetriever, confirm if VertexAISearchRetriever is separate or related.\n# Assuming it might be related or a typo in the original doc:\nfrom langchain_google_community import VertexAISearchRetriever # Verify class name if needed\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import VertexAISearchSummaryTool\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Search, store, and manage documents using Document AI Warehouse .\n\nNote: GoogleDocumentAIWarehouseRetriever (from langchain ) is deprecated. Use DocumentAIWarehouseRetriever from langchain-google-community ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "GoogleDocumentAIWarehouseRetriever"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "DocumentAIWarehouseRetriever"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Requires installation of relevant Document AI packages (check specific docs)."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community # Add specific docai dependencies if needed\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community.documentai_warehouse import DocumentAIWarehouseRetriever\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Integrate agents with various Google services.\n\nGoogle Cloud Text-to-Speech is a Google Cloud service that enables developers to\nsynthesize natural-sounding speech with 100+ voices, available in multiple languages and variants.\nIt applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks\nto deliver the highest fidelity possible.\n\nInstall required packages:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-cloud-text-to-speech langchain-google-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import TextToSpeechTool\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Tools for interacting with Google Drive.\n\nInstall required packages:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper\nfrom langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Query financial data. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.tools.google_finance import GoogleFinanceQueryRun\nfrom langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Query job listings. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.tools.google_jobs import GoogleJobsQueryRun\n# Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility\n# from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Perform visual searches. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.tools.google_lens import GoogleLensQueryRun\nfrom langchain_community.utilities.google_lens import GoogleLensAPIWrapper\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Search for places information. Requires googlemaps package and a Google Maps API key."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install googlemaps langchain # Requires base langchain\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "# Note: GooglePlacesTool might be in langchain or langchain_community depending on version\nfrom langchain.tools import GooglePlacesTool # Or langchain_community.tools\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Search academic papers. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\nfrom langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Perform web searches using Google Custom Search Engine (CSE). Requires GOOGLE_API_KEY and GOOGLE_CSE_ID ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_community import GoogleSearchAPIWrapper\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.tools import GoogleSearchRun, GoogleSearchResults\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.agent_toolkits.load_tools import load_tools\ntools = load_tools([\"google-search\"])\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "See detailed notebook .\n\nQuery Google Trends data. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_community.tools.google_trends import GoogleTrendsQueryRun\nfrom langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Collections of tools for specific Google services.\n\nGoogle Gmail is a free email service provided by Google.\nThis toolkit works with emails through the Gmail API ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install langchain-google-community[gmail]\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "# Load the whole toolkit\nfrom langchain_google_community import GmailToolkit\n\n# Or use individual tools\nfrom langchain_google_community.gmail.create_draft import GmailCreateDraft\nfrom langchain_google_community.gmail.get_message import GmailGetMessage\nfrom langchain_google_community.gmail.get_thread import GmailGetThread\nfrom langchain_google_community.gmail.search import GmailSearch\nfrom langchain_google_community.gmail.send_message import GmailSendMessage\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "MCP Toolbox provides a simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB . With MCP Toolbox, you can seamlessly integrate your database with LangChain to build powerful, data-driven applications.\n\nTo get started, install the Toolbox server and client .\n\nConfigure a tools.yaml to define your tools, and then execute toolbox to start the server:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "toolbox --tools-file \"tools.yaml\"\n"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "pip install toolbox-langchain\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Here is a quick example of how to use MCP Toolbox to connect to your database:"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from toolbox_langchain import ToolboxClient\n\nasync with ToolboxClient(\"http://127.0.0.1:5000\") as client:\n\n    tools = client.load_toolset()\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "See usage example and setup instructions .\n\nTrack LLM/Chat model usage.\n\nCallback Handler that tracks VertexAI usage info."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.callbacks import VertexAICallbackHandler\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Evaluate model outputs using Vertex AI.\n\nRequires langchain-google-vertexai ."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "from langchain_google_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator\n"
    },
    {
        "title": "​Google Cloud",
        "type": "text",
        "content": "Evaluate a single prediction string using Vertex AI models."
    },
    {
        "title": "​Google Cloud",
        "type": "code",
        "content": "# Note: Original doc listed VertexPairWiseStringEvaluator twice. Assuming this class exists.\nfrom langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator # Verify class name if needed\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Integrations with various Google services beyond the core Cloud Platform.\n\nGoogle Drive file storage. Currently supports Google Docs.\n\nInstall with Drive dependencies:"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install langchain-google-community[drive]\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_google_community import GoogleDriveLoader\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Google ScaNN (Scalable Nearest Neighbors) is a python package.\n\nScaNN is a method for efficient vector similarity search at scale."
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "ScaNN includes search space pruning and quantization for Maximum Inner\nProduct Search and also supports other distance functions such as\nEuclidean distance. The implementation is optimized for x86 processors\nwith AVX2 support. See its Google Research github for more details."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install scann langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.vectorstores import ScaNN\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Retrieve documents from Google Drive.\n\nInstall required packages:"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_googledrive.retrievers import GoogleDriveRetriever\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Tools for interacting with Google Drive.\n\nInstall required packages:"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper\nfrom langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Query financial data. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.tools.google_finance import GoogleFinanceQueryRun\nfrom langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Query job listings. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.tools.google_jobs import GoogleJobsQueryRun\n# Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility\n# from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Perform visual searches. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.tools.google_lens import GoogleLensQueryRun\nfrom langchain_community.utilities.google_lens import GoogleLensAPIWrapper\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Search for places information. Requires googlemaps package and a Google Maps API key."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install googlemaps langchain # Requires base langchain\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "# Note: GooglePlacesTool might be in langchain or langchain_community depending on version\nfrom langchain.tools import GooglePlacesTool # Or langchain_community.tools\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Search academic papers. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\nfrom langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Perform web searches using Google Custom Search Engine (CSE). Requires GOOGLE_API_KEY and GOOGLE_CSE_ID ."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install langchain-google-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_google_community import GoogleSearchAPIWrapper\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.tools import GoogleSearchRun, GoogleSearchResults\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.agent_toolkits.load_tools import load_tools\ntools = load_tools([\"google-search\"])\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "See detailed notebook .\n\nQuery Google Trends data. Requires google-search-results package and SerpApi key."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "google-search-results"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install google-search-results langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_community.tools.google_trends import GoogleTrendsQueryRun\nfrom langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Google Gmail is a free email service provided by Google.\nThis toolkit works with emails through the Gmail API ."
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install langchain-google-community[gmail]\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "# Load the whole toolkit\nfrom langchain_google_community import GmailToolkit\n\n# Or use individual tools\nfrom langchain_google_community.gmail.create_draft import GmailCreateDraft\nfrom langchain_google_community.gmail.get_message import GmailGetMessage\nfrom langchain_google_community.gmail.get_thread import GmailGetThread\nfrom langchain_google_community.gmail.search import GmailSearch\nfrom langchain_google_community.gmail.send_message import GmailSendMessage\n"
    },
    {
        "title": "​Other Google Products",
        "type": "text",
        "content": "Load chat history from Gmail threads.\n\nInstall with Gmail dependencies:"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "pip install langchain-google-community[gmail]\n"
    },
    {
        "title": "​Other Google Products",
        "type": "code",
        "content": "from langchain_google_community import GMailLoader\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "Access Google services via third-party APIs.\n\nSearchApi provides API access to Google search, YouTube, etc. Requires langchain-community ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "See usage examples and authorization instructions ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "from langchain_community.utilities import SearchApiAPIWrapper\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "SerpApi provides API access to Google search results. Requires langchain-community ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "See a usage example and authorization instructions ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "from langchain_community.utilities import SerpAPIWrapper\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "Google Serper provides API access to Google search results. Requires langchain-community ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "See a usage example and authorization instructions ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "from langchain_community.utilities import GoogleSerperAPIWrapper\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "Search YouTube videos without the official API. Requires youtube_search package."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "pip install youtube_search langchain # Requires base langchain\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "# Note: YouTubeSearchTool might be in langchain or langchain_community\nfrom langchain.tools import YouTubeSearchTool # Or langchain_community.tools\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "Download audio from YouTube videos. Requires yt_dlp , pydub , librosa ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "pip install yt_dlp pydub librosa langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n# Often used with whisper parsers:\n# from langchain_community.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "Load video transcripts. Requires youtube-transcript-api ."
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "youtube-transcript-api"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "pip install youtube-transcript-api langchain-community # Requires langchain-community\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "code",
        "content": "from langchain_community.document_loaders import YoutubeLoader\n"
    },
    {
        "title": "​3rd Party Integrations",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "AWS (Amazon)",
        "type": "text",
        "content": "All LangChain integrations with the Amazon AWS platform."
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "Amazon Bedrock is a fully managed service that offers a choice of\nhigh-performing foundation models (FMs) from leading AI companies like AI21 Labs , Anthropic , Cohere , Meta , Stability AI , and Amazon via a single API, along with a broad set of capabilities you need to\nbuild generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock ,\nyou can easily experiment with and evaluate top FMs for your use case, privately customize them with\nyour data using techniques such as fine-tuning and Retrieval Augmented Generation ( RAG ), and build\nagents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is\nserverless, you don’t have to manage any infrastructure, and you can securely integrate and deploy\ngenerative AI capabilities into your applications using the AWS services you are already familiar with."
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "Retrieval Augmented Generation"
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "from langchain_aws import ChatBedrock\n"
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "AWS Bedrock maintains a Converse API that provides a unified conversational interface for Bedrock models. This API does not\nyet support custom models. You can see a list of all models that are supported here .\n\nWe recommend the Converse API for users who do not need to use custom models. It can be accessed using ChatBedrockConverse .\n\nSee a usage example ."
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "from langchain_aws import ChatBedrockConverse\n"
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_aws import BedrockLLM\n"
    },
    {
        "title": "​LLMs",
        "type": "text",
        "content": "Amazon API Gateway is a fully managed service that makes it easy for\ndevelopers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the “front door”\nfor applications to access data, business logic, or functionality from your backend services. Using API Gateway , you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication\napplications. API Gateway supports containerized and serverless workloads, as well as web applications."
    },
    {
        "title": "​LLMs",
        "type": "text",
        "content": "API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of\nconcurrent API calls, including traffic management, CORS support, authorization and access control,\nthrottling, monitoring, and API version management. API Gateway has no minimum fees or startup costs.\nYou pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales."
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_community.llms import AmazonAPIGateway\n"
    },
    {
        "title": "​LLMs",
        "type": "text",
        "content": "Amazon SageMaker is a system that can build, train, and deploy\nmachine learning (ML) models with fully managed infrastructure, tools, and workflows.\n\nWe use SageMaker to host our model and expose it as the SageMaker Endpoint ."
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_aws import SagemakerEndpoint\n"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_aws import BedrockEmbeddings\n"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_community.embeddings import SagemakerEndpointEmbeddings\nfrom langchain_community.llms.sagemaker_endpoint import ContentHandlerBase\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Amazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory AWS S3 Buckets\n\nSee a usage example for S3DirectoryLoader .\n\nSee a usage example for S3FileLoader ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Amazon Textract is a machine\nlearning (ML) service that automatically extracts text, handwriting, and data from scanned documents.\n\nSee a usage example ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import AmazonTextractPDFLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Amazon Athena is a serverless, interactive analytics service built\non open-source frameworks, supporting open-table and file formats.\n\nSee a usage example ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders.athena import AthenaLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "The AWS Glue Data Catalog is a centralized metadata\nrepository that allows you to manage, access, and share metadata about\nyour data stored in AWS. It acts as a metadata store for your data assets,\nenabling various AWS services and your applications to query and connect\nto the data they need efficiently.\n\nSee a usage example ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders.glue_catalog import GlueCatalogLoader\n"
    },
    {
        "title": "​Vector stores",
        "type": "text",
        "content": "Amazon OpenSearch Service performs\ninteractive log analytics, real-time application monitoring, website search, and more. OpenSearch is\nan open source,\ndistributed search and analytics suite derived from Elasticsearch . Amazon OpenSearch Service offers the\nlatest versions of OpenSearch , support for many versions of Elasticsearch , as well as\nvisualization capabilities powered by OpenSearch Dashboards and Kibana ."
    },
    {
        "title": "​Vector stores",
        "type": "code",
        "content": "Amazon OpenSearch Service"
    },
    {
        "title": "​Vector stores",
        "type": "code",
        "content": "OpenSearch Dashboards"
    },
    {
        "title": "​Vector stores",
        "type": "code",
        "content": "pip install boto3 requests requests-aws4auth\n"
    },
    {
        "title": "​Vector stores",
        "type": "code",
        "content": "from langchain_community.vectorstores import OpenSearchVectorSearch\n"
    },
    {
        "title": "​Vector stores",
        "type": "text",
        "content": "Amazon DocumentDB (with MongoDB Compatibility) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\nWith Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.\nVector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.\n\nSee detail configuration instructions .\n\nWe need to install the pymongo python package."
    },
    {
        "title": "​Vector stores",
        "type": "text",
        "content": "Amazon DocumentDB (with MongoDB Compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\n\nAWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see Cloud Computing with Amazon Web Services .\n\nSee a usage example ."
    },
    {
        "title": "​Vector stores",
        "type": "code",
        "content": "from langchain_community.vectorstores import DocumentDBVectorSearch\n"
    },
    {
        "title": "​Vector stores",
        "type": "text",
        "content": "Amazon MemoryDB is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,\nenabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.\n\nInMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB."
    },
    {
        "title": "​Vector stores",
        "type": "code",
        "content": "from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore\n\nvds = InMemoryVectorStore.from_documents(\n            chunks,\n            embeddings,\n            redis_url=\"rediss://cluster_endpoint:6379/ssl=True ssl_cert_reqs=none\",\n            vector_schema=vector_schema,\n            index_name=INDEX_NAME,\n        )\n"
    },
    {
        "title": "​Retrievers",
        "type": "text",
        "content": "Amazon Kendra is an intelligent search service\nprovided by Amazon Web Services ( AWS ). It utilizes advanced natural language processing (NLP) and machine\nlearning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately,\nimproving productivity and decision-making."
    },
    {
        "title": "​Retrievers",
        "type": "text",
        "content": "With Kendra , we can search across a wide range of content types, including documents, FAQs, knowledge bases,\nmanuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and\ncontextual meanings to provide highly relevant search results."
    },
    {
        "title": "​Retrievers",
        "type": "code",
        "content": "pip install langchain-aws\n"
    },
    {
        "title": "​Retrievers",
        "type": "code",
        "content": "from langchain_aws import AmazonKendraRetriever\n"
    },
    {
        "title": "​Retrievers",
        "type": "text",
        "content": "Knowledge bases for Amazon Bedrock is an Amazon Web Services ( AWS ) offering which lets you quickly build RAG applications by using your\nprivate data to customize foundation model response."
    },
    {
        "title": "​Retrievers",
        "type": "code",
        "content": "pip install langchain-aws\n"
    },
    {
        "title": "​Retrievers",
        "type": "code",
        "content": "from langchain_aws import AmazonKnowledgeBasesRetriever\n"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "Amazon AWS Lambda is a serverless computing service provided by Amazon Web Services ( AWS ). It helps developers to build and run applications and services without\nprovisioning or managing servers. This serverless architecture enables you to focus on writing and\ndeploying code, while AWS automatically takes care of scaling, patching, and managing the\ninfrastructure required to run your applications."
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n"
    },
    {
        "title": "​Graphs",
        "type": "text",
        "content": "Amazon Neptune is a high-performance graph analytics and serverless database for superior scalability and availability.\n\nFor the Cypher and SPARQL integrations below, we need to install the langchain-aws library."
    },
    {
        "title": "​Graphs",
        "type": "code",
        "content": "pip install langchain-aws\n"
    },
    {
        "title": "​Graphs",
        "type": "code",
        "content": "from langchain_aws.graphs import NeptuneGraph\nfrom langchain_aws.graphs import NeptuneAnalyticsGraph\nfrom langchain_aws.chains import create_neptune_opencypher_qa_chain\n"
    },
    {
        "title": "​Graphs",
        "type": "code",
        "content": "from langchain_aws.graphs import NeptuneRdfGraph\nfrom langchain_aws.chains import create_neptune_sparql_qa_chain\n"
    },
    {
        "title": "​Callbacks",
        "type": "code",
        "content": "from langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler\n"
    },
    {
        "title": "​Callbacks",
        "type": "text",
        "content": "Amazon SageMaker is a fully managed service that is used to quickly\nand easily build, train and deploy machine learning (ML) models.\n\nAmazon SageMaker Experiments is a capability\nof Amazon SageMaker that lets you organize, track,\ncompare and evaluate ML experiments and model versions."
    },
    {
        "title": "​Callbacks",
        "type": "code",
        "content": "pip install google-search-results sagemaker\n"
    },
    {
        "title": "​Callbacks",
        "type": "code",
        "content": "from langchain_community.callbacks import SageMakerCallbackHandler\n"
    },
    {
        "title": "​Chains",
        "type": "text",
        "content": "Amazon Comprehend is a natural-language processing (NLP) service that\nuses machine learning to uncover valuable insights and connections in text.\n\nWe need to install the boto3 and nltk libraries."
    },
    {
        "title": "​Chains",
        "type": "code",
        "content": "pip install boto3 nltk\n"
    },
    {
        "title": "​Chains",
        "type": "code",
        "content": "from langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n"
    },
    {
        "title": "​Chains",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Hugging Face",
        "type": "text",
        "content": "All LangChain integrations with Hugging Face Hub and libraries like transformers , sentence transformers , and datasets ."
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "We can use the Hugging Face LLM classes or directly use the ChatHuggingFace class."
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "from langchain_huggingface import ChatHuggingFace\n"
    },
    {
        "title": "​LLMs",
        "type": "text",
        "content": "We can use the HuggingFaceEndpoint class to run open source models via serverless Inference Providers or via dedicated Inference Endpoints ."
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_huggingface import HuggingFaceEndpoint\n"
    },
    {
        "title": "​LLMs",
        "type": "text",
        "content": "We can use the HuggingFacePipeline class to run open source models locally."
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_huggingface import HuggingFacePipeline\n"
    },
    {
        "title": "​Embedding Models",
        "type": "text",
        "content": "We can use the HuggingFaceEmbeddings class to run open source embedding models locally."
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "HuggingFaceEmbeddings"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_huggingface import HuggingFaceEmbeddings\n"
    },
    {
        "title": "​Embedding Models",
        "type": "text",
        "content": "We can use the HuggingFaceEndpointEmbeddings class to run open source embedding models via a dedicated Inference Endpoint ."
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "HuggingFaceEndpointEmbeddings"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n"
    },
    {
        "title": "​Embedding Models",
        "type": "text",
        "content": "We can use the HuggingFaceInferenceAPIEmbeddings class to run open source embedding models via Inference Providers ."
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "HuggingFaceInferenceAPIEmbeddings"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n"
    },
    {
        "title": "​Embedding Models",
        "type": "text",
        "content": "We can use the HuggingFaceInstructEmbeddings class to run open source embedding models locally."
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "HuggingFaceInstructEmbeddings"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n"
    },
    {
        "title": "​Embedding Models",
        "type": "text",
        "content": "BGE models on the HuggingFace are one of the best open-source embedding models .\nBGE model is created by the Beijing Academy of Artificial Intelligence (BAAI) . BAAI is a private non-profit organization engaged in AI research and development."
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Hugging Face Hub is home to over 75,000 datasets in more than 100 languages\nthat can be used for a broad range of tasks across NLP, Computer Vision, and Audio.\nThey used for a diverse range of tasks such as translation, automatic speech\nrecognition, and image classification.\n\nWe need to install datasets python package."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install datasets\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Load model information from Hugging Face Hub , including README content."
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "This loader interfaces with the Hugging Face Models API to fetch\nand load model metadata and README files.\nThe API allows you to search and filter models based on\nspecific criteria such as model tags, authors, and more."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "Hugging Face Models API"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import HuggingFaceModelLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "It uses the Hugging Face models to generate image captions.\n\nWe need to install several python packages."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install transformers pillow\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import ImageCaptionLoader\n"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "Hugging Face Tools support text I/O and are loaded using the load_huggingface_tool function."
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "load_huggingface_tool"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "pip install transformers huggingface_hub\n"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "from langchain_community.agent_toolkits.load_tools import load_huggingface_tool\n"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "OpenAI Text-to-Speech API"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "from langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference\n"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Microsoft",
        "type": "text",
        "content": "All LangChain integrations with Microsoft Azure and other Microsoft products."
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "Microsoft offers three main options for accessing chat models through Azure:\n\nMicrosoft Azure , often referred to as Azure is a cloud computing platform run by Microsoft , which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems."
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "Azure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3 , Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation."
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "pip install langchain-openai\n"
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "Set the environment variables to get access to the Azure OpenAI service."
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "import os\n\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://<your-endpoint.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\n"
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "from langchain_openai import AzureChatOpenAI\n"
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "Azure AI Foundry provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the AzureAIChatCompletionsModel class."
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "AzureAIChatCompletionsModel"
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "pip install -U langchain-azure-ai\n"
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "export AZURE_AI_CREDENTIAL=your-api-key\nexport AZURE_AI_ENDPOINT=your-endpoint\n"
    },
    {
        "title": "​Chat models",
        "type": "code",
        "content": "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n\nllm = AzureAIChatCompletionsModel(\n    model_name=\"gpt-4o\",\n    api_version=\"2024-05-01-preview\",\n)\n"
    },
    {
        "title": "​Chat models",
        "type": "text",
        "content": "See a usage example\n\nSee the documentation here for accessing chat\nmodels hosted with Azure Machine Learning ."
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint\n"
    },
    {
        "title": "​LLMs",
        "type": "code",
        "content": "from langchain_openai import AzureOpenAI\n"
    },
    {
        "title": "​Embedding Models",
        "type": "text",
        "content": "Microsoft offers two main options for accessing embedding models through Azure:\n\nSee a usage example"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_openai import AzureOpenAIEmbeddings\n"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "pip install -U langchain-azure-ai\n"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "export AZURE_AI_CREDENTIAL=your-api-key\nexport AZURE_AI_ENDPOINT=your-endpoint\n"
    },
    {
        "title": "​Embedding Models",
        "type": "code",
        "content": "from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n\nembed_model = AzureAIEmbeddingsModel(\n    model_name=\"text-embedding-ada-002\"\n)\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Azure AI Foundry (formerly Azure AI Studio provides the capability to upload data assets\nto cloud storage and register existing data assets from the following sources:"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "Azure Data Lake gen 2"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "First, you need to install several python packages."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install azureml-fsspec, azure-ai-generative\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain.document_loaders import AzureAIDataLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Azure AI Document Intelligence (formerly known\nas Azure Form Recognizer ) is machine-learning\nbased service that extracts texts (including handwriting), tables, document structures,\nand key-value-pairs\nfrom digital or scanned PDFs, images, Office and HTML files."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "Azure Form Recognizer"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Document Intelligence supports PDF , JPEG/JPG , PNG , BMP , TIFF , HEIF , DOCX , XLSX , PPTX and HTML ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install azure-ai-documentintelligence\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain.document_loaders import AzureAIDocumentIntelligenceLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Azure Blob Storage is Microsoft’s object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn’t adhere to a particular data model or definition, such as text or binary data.\n\nAzure Blob Storage is designed for:"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install langchain-azure-storage\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "See usage examples for the Azure Blob Storage Loader ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_azure_storage.document_loaders import AzureBlobStorageLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Microsoft OneDrive (formerly SkyDrive ) is a file-hosting service operated by Microsoft."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import OneDriveLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Microsoft OneDrive (formerly SkyDrive ) is a file-hosting service operated by Microsoft."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import OneDriveFileLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Microsoft Word is a word processor developed by Microsoft.\n\nSee a usage example ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Microsoft Excel is a spreadsheet editor developed by\nMicrosoft for Windows, macOS, Android, iOS and iPadOS.\nIt features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming\nlanguage called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.\n\nThe UnstructuredExcelLoader is used to load Microsoft Excel files. The loader works with both .xlsx and .xls files.\nThe page content will be the raw text of the Excel file. If you use the loader in \"elements\" mode, an HTML\nrepresentation of the Excel file will be available in the document metadata under the text_as_html key."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "UnstructuredExcelLoader"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import UnstructuredExcelLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Microsoft SharePoint is a website-based collaboration system\nthat uses workflow applications, “list” databases, and other web parts and security features to\nempower business teams to work together developed by Microsoft.\n\nSee a usage example ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders.sharepoint import SharePointLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Microsoft PowerPoint is a presentation program by Microsoft.\n\nSee a usage example ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install bs4 msal\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders.onenote import OneNoteLoader\n"
    },
    {
        "title": "​Document loaders",
        "type": "text",
        "content": "Playwright is an open-source automation tool\ndeveloped by Microsoft that allows you to programmatically control and automate\nweb browsers. It is designed for end-to-end testing, scraping, and automating\ntasks across various web browsers such as Chromium , Firefox , and WebKit ."
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "pip install playwright unstructured\n"
    },
    {
        "title": "​Document loaders",
        "type": "code",
        "content": "from langchain_community.document_loaders.onenote import OneNoteLoader\n"
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "AI agents can rely on Azure Cosmos DB as a unified memory system solution, enjoying speed, scale, and simplicity. This service successfully enabled OpenAI’s ChatGPT service to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world’s first globally distributed NoSQL , relational , and vector database service that offers a serverless mode.\n\nBelow are two available Azure Cosmos DB APIs that can provide vector store functionalities.\n\nAzure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support.\nYou can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account’s connection string.\nUse vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that’s stored in Azure Cosmos DB.\n\nSee detailed configuration instructions .\n\nWe need to install pymongo python package."
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.\n\nWith Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.\n\nSign Up for free to get started today.\n\nSee a usage example ."
    },
    {
        "title": "​Vector Stores",
        "type": "code",
        "content": "from langchain_community.vectorstores import AzureCosmosDBVectorSearch\n"
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "Azure Cosmos DB for NoSQL now offers vector indexing and search in preview.\nThis feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors\ndirectly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,\nbut also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,\nas the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the\nefficiency of vector-based operations.\n\nSee detail configuration instructions .\n\nWe need to install azure-cosmos python package."
    },
    {
        "title": "​Vector Stores",
        "type": "code",
        "content": "pip install azure-cosmos\n"
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available\nin every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.\n\nSign Up for free to get started today.\n\nSee a usage example ."
    },
    {
        "title": "​Vector Stores",
        "type": "code",
        "content": "from langchain_community.vectorstores import AzureCosmosDBNoSQLVectorSearch\n"
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It’s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n\nSee set up instructions for Azure Database for PostgreSQL.\n\nSimply use the connection string from your Azure Portal.\n\nSince Azure Database for PostgreSQL is open-source Postgres, you can use the LangChain’s Postgres support to connect to Azure Database for PostgreSQL.\n\nAzure SQL Database is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.\n\nBy leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.\n\nSee detail configuration instructions .\n\nWe need to install the langchain-sqlserver python package."
    },
    {
        "title": "​Vector Stores",
        "type": "code",
        "content": "!pip install langchain-sqlserver==0.1.1\n"
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "Sign Up for free to get started today.\n\nSee a usage example ."
    },
    {
        "title": "​Vector Stores",
        "type": "code",
        "content": "from langchain_sqlserver import SQLServer_VectorStore\n"
    },
    {
        "title": "​Vector Stores",
        "type": "text",
        "content": "Azure AI Search is a cloud search service\nthat gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid\nqueries at scale. See here for usage examples."
    },
    {
        "title": "​Vector Stores",
        "type": "code",
        "content": "from langchain_community.vectorstores.azuresearch import AzureSearch\n"
    },
    {
        "title": "​Retrievers",
        "type": "text",
        "content": "Azure AI Search (formerly known as Azure Search or Azure Cognitive Search ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications."
    },
    {
        "title": "​Retrievers",
        "type": "code",
        "content": "Azure Cognitive Search"
    },
    {
        "title": "​Retrievers",
        "type": "text",
        "content": "Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you’ll work with the following capabilities:\n\nSee set up instructions .\n\nSee a usage example ."
    },
    {
        "title": "​Retrievers",
        "type": "code",
        "content": "from langchain_community.retrievers import AzureAISearchRetriever\n"
    },
    {
        "title": "​Vector Store",
        "type": "text",
        "content": "Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It’s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\n\nSee set up instructions for Azure Database for PostgreSQL.\n\nYou need to enable pgvector extension in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the PGVector in LangChain to connect to Azure Database for PostgreSQL.\n\nSee a usage example . Simply use the connection string from your Azure Portal."
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "We need to get the POOL_MANAGEMENT_ENDPOINT environment variable from the Azure Container Apps service.\nSee the instructions here ."
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "POOL_MANAGEMENT_ENDPOINT"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "pip install langchain-azure-dynamic-sessions\n"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "from langchain_azure_dynamic_sessions import SessionsPythonREPLTool\n"
    },
    {
        "title": "​Tools",
        "type": "text",
        "content": "Follow the documentation here to get a detail explanations and instructions of this tool.\n\nThe environment variable BING_SUBSCRIPTION_KEY and BING_SEARCH_URL are required from Bing Search resource."
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "BING_SUBSCRIPTION_KEY"
    },
    {
        "title": "​Tools",
        "type": "code",
        "content": "from langchain_community.tools.bing_search import BingSearchResults\nfrom langchain_community.utilities import BingSearchAPIWrapper\n\napi_wrapper = BingSearchAPIWrapper()\ntool = BingSearchResults(api_wrapper=api_wrapper)\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.agent_toolkits import azure_ai_services\n"
    },
    {
        "title": "​Toolkits",
        "type": "text",
        "content": "The azure_ai_services toolkit includes the following tools:"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.agent_toolkits import AzureCognitiveServicesToolkit\n"
    },
    {
        "title": "​Toolkits",
        "type": "text",
        "content": "The azure_ai_services toolkit includes the tools that queries the Azure Cognitive Services :"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "Azure Cognitive Services"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "AzureCogsFormRecognizerTool"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "AzureCogsImageAnalysisTool"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "AzureCogsSpeech2TextTool"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "AzureCogsText2SpeechTool"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "AzureCogsTextAnalyticsHealthTool"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.tools.azure_cognitive_services import (\n    AzureCogsFormRecognizerTool,\n    AzureCogsImageAnalysisTool,\n    AzureCogsSpeech2TextTool,\n    AzureCogsText2SpeechTool,\n    AzureCogsTextAnalyticsHealthTool,\n)\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.agent_toolkits import O365Toolkit\n"
    },
    {
        "title": "​Toolkits",
        "type": "text",
        "content": "You can use individual tools from the Office 365 Toolkit:"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "O365CreateDraftMessage"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.tools.office365 import O365CreateDraftMessage\nfrom langchain_community.tools.office365 import O365SearchEmails\nfrom langchain_community.tools.office365 import O365SearchEvents\nfrom langchain_community.tools.office365 import O365SendEvent\nfrom langchain_community.tools.office365 import O365SendMessage\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "pip install azure-identity\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.agent_toolkits import PowerBIToolkit\nfrom langchain_community.utilities.powerbi import PowerBIDataset\n"
    },
    {
        "title": "​Toolkits",
        "type": "text",
        "content": "You can use individual tools from the Azure PowerBI Toolkit:"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.tools.powerbi.tool import InfoPowerBITool\nfrom langchain_community.tools.powerbi.tool import ListPowerBITool\nfrom langchain_community.tools.powerbi.tool import QueryPowerBITool\n"
    },
    {
        "title": "​Toolkits",
        "type": "text",
        "content": "Playwright is an open-source automation tool\ndeveloped by Microsoft that allows you to programmatically control and automate\nweb browsers. It is designed for end-to-end testing, scraping, and automating\ntasks across various web browsers such as Chromium , Firefox , and WebKit ."
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "pip install playwright lxml\n"
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n"
    },
    {
        "title": "​Toolkits",
        "type": "text",
        "content": "You can use individual tools from the PlayWright Browser Toolkit."
    },
    {
        "title": "​Toolkits",
        "type": "code",
        "content": "from langchain_community.tools.playwright import ClickTool\nfrom langchain_community.tools.playwright import CurrentWebPageTool\nfrom langchain_community.tools.playwright import ExtractHyperlinksTool\nfrom langchain_community.tools.playwright import ExtractTextTool\nfrom langchain_community.tools.playwright import GetElementsTool\nfrom langchain_community.tools.playwright import NavigateTool\nfrom langchain_community.tools.playwright import NavigateBackTool\n"
    },
    {
        "title": "​Graphs",
        "type": "code",
        "content": "pip install gremlinpython\n"
    },
    {
        "title": "​Graphs",
        "type": "code",
        "content": "from langchain_community.graphs import GremlinGraph\nfrom langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n"
    },
    {
        "title": "​Utilities",
        "type": "text",
        "content": "Microsoft Bing , commonly referred to as Bing or Bing Search ,\nis a web search engine owned and operated by Microsoft ."
    },
    {
        "title": "​Utilities",
        "type": "code",
        "content": "from langchain_community.utilities import BingSearchAPIWrapper\n"
    },
    {
        "title": "​More",
        "type": "text",
        "content": "Presidio (Origin from Latin praesidium ‘protection, garrison’)\nhelps to ensure sensitive data is properly managed and governed. It provides fast identification and\nanonymization modules for private entities in text and images such as credit card numbers, names,\nlocations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\n\nFirst, you need to install several python packages and download a SpaCy model."
    },
    {
        "title": "​More",
        "type": "code",
        "content": "pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\npython -m spacy download en_core_web_lg\n"
    },
    {
        "title": "​More",
        "type": "code",
        "content": "from langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer\n"
    },
    {
        "title": "​More",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Ollama",
        "type": "text",
        "content": "All LangChain integrations with Ollama .\n\nOllama allows you to run open-source models (like gpt-oss ) locally."
    },
    {
        "title": "Ollama",
        "type": "text",
        "content": "For a complete list of supported models and variants, see the Ollama model library ."
    },
    {
        "title": "OllamaEmbeddings",
        "type": "text",
        "content": "Ollama embedding models.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Chat models",
        "type": "text",
        "content": "Chat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to traditional, plaintext LLMs) ."
    },
    {
        "title": "​Featured providers",
        "type": "text",
        "content": "While all these LangChain classes support the indicated advanced feature , you may have to open the provider-specific documentation to learn which hosted models or backends support the feature."
    },
    {
        "title": "​Featured providers",
        "type": "code",
        "content": "langchain-google-vertexai"
    },
    {
        "title": "​Featured providers",
        "type": "code",
        "content": "ChatGoogleGenerativeAI"
    },
    {
        "title": "​Featured providers",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "​Featured providers",
        "type": "code",
        "content": "langchain-huggingface"
    },
    {
        "title": "​Featured providers",
        "type": "code",
        "content": "langchain-nvidia-ai-endpoints"
    },
    {
        "title": "​Chat Completions API",
        "type": "text",
        "content": "Certain model providers offer endpoints that are compatible with OpenAI’s Chat Completions API . In such case, you can use ChatOpenAI with a custom base_url to connect to these endpoints."
    },
    {
        "title": "​Chat Completions API",
        "type": "text",
        "content": "To use OpenRouter, you will need to sign up for an account and obtain an API key ."
    },
    {
        "title": "​Chat Completions API",
        "type": "code",
        "content": "from langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"...\",  # Specify a model available on OpenRouter\n    api_key=\"OPENROUTER_API_KEY\",\n    base_url=\"https://openrouter.ai/api/v1\",\n)\n"
    },
    {
        "title": "​Chat Completions API",
        "type": "text",
        "content": "Refer to the OpenRouter documentation for more details.\n\nTo capture reasoning tokens ,"
    },
    {
        "title": "​Chat Completions API",
        "type": "code",
        "content": "model = ChatDeepSeek(\n    model=\"...\",\n    api_key=\"...\",\n    api_base=\"https://openrouter.ai/api/v1\",\n    extra_body={\"reasoning\": {\"enabled\": True}},\n)\n"
    },
    {
        "title": "​Chat Completions API",
        "type": "text",
        "content": "This is a known limitation with ChatOpenAI and will be addressed in a future release."
    },
    {
        "title": "ZHIPU AI",
        "type": "text",
        "content": "If you’d like to contribute an integration, see Contributing integrations .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Tools and toolkits",
        "type": "text",
        "content": "Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n\nA toolkit is a collection of tools meant to be used together."
    },
    {
        "title": "​Search",
        "type": "text",
        "content": "The following table shows tools that execute online searches in some shape or form:"
    },
    {
        "title": "​Code Interpreter",
        "type": "text",
        "content": "The following table shows tools that can be used as code interpreters:"
    },
    {
        "title": "​Productivity",
        "type": "text",
        "content": "The following table shows tools that can be used to automate tasks in productivity tools:"
    },
    {
        "title": "​Web Browsing",
        "type": "text",
        "content": "The following table shows tools that can be used to automate tasks in web browsers:"
    },
    {
        "title": "​Database",
        "type": "text",
        "content": "The following table shows tools that can be used to automate tasks in databases:"
    },
    {
        "title": "​Finance",
        "type": "text",
        "content": "The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:"
    },
    {
        "title": "​Integration Platforms",
        "type": "text",
        "content": "The following platforms provide access to multiple tools and services through a unified interface:"
    },
    {
        "title": "ZenGuard AI",
        "type": "text",
        "content": "If you’d like to contribute an integration, see Contributing integrations .\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Retrievers",
        "type": "text",
        "content": "A retriever is an interface that returns documents given an unstructured query.\nIt is more general than a vector store.\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\nRetrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra .\n\nRetrievers accept a string query as input and return a list of Documents as output.\n\nNote that all vector stores can be cast to retrievers. Refer to the vector store integration docs for available vector stores.\nThis page lists custom retrievers, implemented via subclassing BaseRetriever."
    },
    {
        "title": "​Bring-your-own documents",
        "type": "text",
        "content": "The below retrievers allow you to index and search a custom corpus of documents."
    },
    {
        "title": "​Bring-your-own documents",
        "type": "code",
        "content": "AmazonKnowledgeBasesRetriever"
    },
    {
        "title": "​Bring-your-own documents",
        "type": "code",
        "content": "AzureAISearchRetriever"
    },
    {
        "title": "​Bring-your-own documents",
        "type": "code",
        "content": "ElasticsearchRetriever"
    },
    {
        "title": "​Bring-your-own documents",
        "type": "code",
        "content": "langchain-elasticsearch"
    },
    {
        "title": "​Bring-your-own documents",
        "type": "code",
        "content": "VertexAISearchRetriever"
    },
    {
        "title": "​Bring-your-own documents",
        "type": "code",
        "content": "langchain-google-community"
    },
    {
        "title": "​External index",
        "type": "text",
        "content": "The below retrievers will search over an external index (e.g., constructed from Internet data or similar)."
    },
    {
        "title": "​External index",
        "type": "code",
        "content": "TavilySearchAPIRetriever"
    },
    {
        "title": "​All retrievers",
        "type": "text",
        "content": "Note: The descriptions in the table below are truncated for readability."
    },
    {
        "title": "Zotero",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Text splitters",
        "type": "text",
        "content": "Text splitters break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.\n\nThere are several strategies for splitting documents, each with its own advantages.\n\nFor most use cases, start with the RecursiveCharacterTextSplitter . It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application."
    },
    {
        "title": "​Text structure-based",
        "type": "text",
        "content": "Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain’s RecursiveCharacterTextSplitter implements this concept:"
    },
    {
        "title": "​Text structure-based",
        "type": "code",
        "content": "RecursiveCharacterTextSplitter"
    },
    {
        "title": "​Text structure-based",
        "type": "code",
        "content": "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_text(document)\n"
    },
    {
        "title": "​Length-based",
        "type": "text",
        "content": "An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn’t exceed a specified size limit. Key benefits of length-based splitting:\n\nTypes of length-based splitting:\n\nExample implementation using LangChain’s CharacterTextSplitter with token-based splitting:"
    },
    {
        "title": "​Length-based",
        "type": "code",
        "content": "from langchain_text_splitters import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n)\ntexts = text_splitter.split_text(document)\n"
    },
    {
        "title": "​Document structure-based",
        "type": "text",
        "content": "Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it’s beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:\n\nExamples of structure-based splitting:\n\nAvailable text splitters :"
    },
    {
        "title": "WRITER",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "This overview covers text-based embedding models . LangChain does not currently support multimodal embeddings.\n\nEmbedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its semantic meaning . These vectors allow machines to compare and search text based on meaning rather than exact words.\n\nIn practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase “machine learning” , embeddings can surface documents that discuss related concepts even when different wording is used.\n\nSeveral metrics are commonly used to compare embeddings:\n\nHere’s an example of computing cosine similarity between two vectors:"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "import numpy as np\n\ndef cosine_similarity(vec1, vec2):\n    dot = np.dot(vec1, vec2)\n    return dot / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\nsimilarity = cosine_similarity(query_embedding, document_embedding)\nprint(\"Cosine Similarity:\", similarity)\n"
    },
    {
        "title": "​Interface",
        "type": "text",
        "content": "LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the Embeddings interface.\n\nTwo main methods are available:"
    },
    {
        "title": "​Interface",
        "type": "code",
        "content": "embed_documents(texts: List[str]) → List[List[float]]"
    },
    {
        "title": "​Interface",
        "type": "code",
        "content": "embed_query(text: str) → List[float]"
    },
    {
        "title": "​Interface",
        "type": "text",
        "content": "The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice."
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "​Caching",
        "type": "text",
        "content": "Embeddings can be stored or temporarily cached to avoid needing to recompute them.\n\nCaching embeddings can be done using a CacheBackedEmbeddings . This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache."
    },
    {
        "title": "​Caching",
        "type": "code",
        "content": "CacheBackedEmbeddings"
    },
    {
        "title": "​Caching",
        "type": "text",
        "content": "The main supported way to initialize a CacheBackedEmbeddings is from_bytes_store . It takes the following parameters:"
    },
    {
        "title": "​Caching",
        "type": "code",
        "content": "CacheBackedEmbeddings"
    },
    {
        "title": "​Caching",
        "type": "code",
        "content": "document_embedding_cache"
    },
    {
        "title": "​Caching",
        "type": "code",
        "content": "query_embedding_cache"
    },
    {
        "title": "​Caching",
        "type": "code",
        "content": "document_embedding_cache"
    },
    {
        "title": "​Caching",
        "type": "code",
        "content": "import time\nfrom langchain_classic.embeddings import CacheBackedEmbeddings  \nfrom langchain_classic.storage import LocalFileStore \nfrom langchain_core.vectorstores import InMemoryVectorStore\n\n# Create your underlying embeddings model\nunderlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.\n\n# Store persists embeddings to the local filesystem\n# This isn't for production use, but is useful for local\nstore = LocalFileStore(\"./cache/\") \n\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings,\n    store,\n    namespace=underlying_embeddings.model\n)\n\n# Example: caching a query embedding\ntic = time.time()\nprint(cached_embedder.embed_query(\"Hello, world!\"))\nprint(f\"First call took: {time.time() - tic:.2f} seconds\")\n\n# Subsequent calls use the cache\ntic = time.time()\nprint(cached_embedder.embed_query(\"Hello, world!\"))\nprint(f\"Second call took: {time.time() - tic:.2f} seconds\")\n"
    },
    {
        "title": "​Caching",
        "type": "text",
        "content": "In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see stores integrations for options."
    },
    {
        "title": "ZhipuAI",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "A vector store stores embedded data and performs similarity search.\n\nLangChain provides a unified interface for vector stores, allowing you to:"
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "This abstraction lets you switch between different implementations without altering your application logic.\n\nTo initialize a vector store, provide it with an embedding model:"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "from langchain_core.vectorstores import InMemoryVectorStore\nvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\n"
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "Add Document objects (holding page_content and optional metadata) like so:"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "vector_store.add_documents(documents=[doc1, doc2], ids=[\"id1\", \"id2\"])\n"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "vector_store.delete(ids=[\"id1\"])\n"
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "Issue a semantic query using similarity_search , which returns the closest embedded documents:"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "similar_docs = vector_store.similarity_search(\"your query here\")\n"
    },
    {
        "title": "​Overview",
        "type": "text",
        "content": "Embedding similarity may be computed using:\n\nEfficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.\n\nFiltering by metadata (e.g., source, date) can refine search results:"
    },
    {
        "title": "​Overview",
        "type": "code",
        "content": "vector_store.similarity_search(\n  \"query\",\n  k=3,\n  filter={\"source\": \"tweets\"}\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-openai\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU \"langchain[azure]\"\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n  os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-google-genai\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"GOOGLE_API_KEY\"):\n  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-google-vertexai\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_google_vertexai import VertexAIEmbeddings\n\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-aws\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_aws import BedrockEmbeddings\n\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-huggingface\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_huggingface import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-ollama\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_ollama import OllamaEmbeddings\n\nembeddings = OllamaEmbeddings(model=\"llama3\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-cohere\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"COHERE_API_KEY\"):\n  os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n\nfrom langchain_cohere import CohereEmbeddings\n\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-mistralai\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\n  os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n\nfrom langchain_mistralai import MistralAIEmbeddings\n\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-nomic\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"NOMIC_API_KEY\"):\n  os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n\nfrom langchain_nomic import NomicEmbeddings\n\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-nvidia-ai-endpoints\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"NVIDIA_API_KEY\"):\n  os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-voyageai\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"VOYAGE_API_KEY\"):\n  os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n\nfrom langchain-voyageai import VoyageAIEmbeddings\n\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-ibm\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"WATSONX_APIKEY\"):\n  os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n\nfrom langchain_ibm import WatsonxEmbeddings\n\nembeddings = WatsonxEmbeddings(\n    model_id=\"ibm/slate-125m-english-rtrvr\",\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    project_id=\"<WATSONX PROJECT_ID>\",\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-core\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU \"langchain[langchain-xai]\"\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"XAI_API_KEY\"):\n  os.environ[\"XAI_API_KEY\"] = getpass.getpass(\"Enter API key for xAI: \")\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"grok-2\", model_provider=\"xai\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU \"langchain[langchain-perplexity]\"\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"PPLX_API_KEY\"):\n  os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\"Enter API key for Perplexity: \")\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"llama-3.1-sonar-small-128k-online\", model_provider=\"perplexity\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU \"langchain[langchain-deepseek]\"\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import getpass\nimport os\n\nif not os.environ.get(\"DEEPSEEK_API_KEY\"):\n  os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter API key for DeepSeek: \")\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-core\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-astradb\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_astradb import AstraDBVectorStore\n\nvector_store = AstraDBVectorStore(\n    embedding=embeddings,\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\n    collection_name=\"astra_vector_langchain\",\n    token=ASTRA_DB_APPLICATION_TOKEN,\n    namespace=ASTRA_DB_NAMESPACE,\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-azure-ai azure-cosmos\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_azure_ai.vectorstores.azure_cosmos_db_no_sql import (\n    AzureCosmosDBNoSqlVectorSearch,\n)\nvector_search = AzureCosmosDBNoSqlVectorSearch.from_documents(\n    documents=docs,\n    embedding=openai_embeddings,\n    cosmos_client=cosmos_client,\n    database_name=database_name,\n    container_name=container_name,\n    vector_embedding_policy=vector_embedding_policy,\n    full_text_policy=full_text_policy,\n    indexing_policy=indexing_policy,\n    cosmos_container_properties=cosmos_container_properties,\n    cosmos_database_properties={},\n    full_text_search_enabled=True,\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-azure-ai pymongo\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_azure_ai.vectorstores.azure_cosmos_db_mongo_vcore import (\n    AzureCosmosDBMongoVCoreVectorSearch,\n)\n\nvectorstore = AzureCosmosDBMongoVCoreVectorSearch.from_documents(\n    docs,\n    openai_embeddings,\n    collection=collection,\n    index_name=INDEX_NAME,\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-chroma\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-community\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "import faiss\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain_community.vectorstores import FAISS\n\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\nindex = faiss.IndexFlatL2(embedding_dim)\n\nvector_store = FAISS(\n    embedding_function=embeddings,\n    index=index,\n    docstore=InMemoryDocstore(),\n    index_to_docstore_id={},\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-milvus\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_milvus import Milvus\n\nURI = \"./milvus_example.db\"\n\nvector_store = Milvus(\n    embedding_function=embeddings,\n    connection_args={\"uri\": URI},\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-mongodb\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_mongodb import MongoDBAtlasVectorSearch\n\nvector_store = MongoDBAtlasVectorSearch(\n    embedding=embeddings,\n    collection=MONGODB_COLLECTION,\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n    relevance_score_fn=\"cosine\",\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-postgres\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_postgres import PGVector\n\nvector_store = PGVector(\n    embeddings=embeddings,\n    collection_name=\"my_docs\",\n    connection=\"postgresql+psycopg://...\"\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-postgres\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_postgres import PGEngine, PGVectorStore\n\n$engine = PGEngine.from_connection_string(\n    url=\"postgresql+psycopg://...\"\n)\n\nvector_store = PGVectorStore.create_sync(\n    engine=pg_engine,\n    table_name='test_table',\n    embedding_service=embedding\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-pinecone\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from langchain_pinecone import PineconeVectorStore\nfrom pinecone import Pinecone\n\npc = Pinecone(api_key=...)\nindex = pc.Index(index_name)\n\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "pip install -qU langchain-qdrant\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "from qdrant_client.models import Distance, VectorParams\nfrom langchain_qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\":memory:\")\n\nvector_size = len(embeddings.embed_query(\"sample text\"))\n\nif not client.collection_exists(\"test\"):\n    client.create_collection(\n        collection_name=\"test\",\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n    )\nvector_store = QdrantVectorStore(\n    client=client,\n    collection_name=\"test\",\n    embedding=embeddings,\n)\n"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "AzureCosmosDBNoSqlVectorStore"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "AzureCosmosDBMongoVCoreVectorStore"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "CouchbaseSearchVectorStore"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "DatabricksVectorSearch"
    },
    {
        "title": "​Top integrations",
        "type": "code",
        "content": "MongoDBAtlasVectorSearch"
    },
    {
        "title": "Zilliz",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Document loaders",
        "type": "text",
        "content": "Document loaders provide a standard interface for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s Document format.\nThis ensures that data can be handled consistently regardless of the source.\n\nAll document loaders implement the BaseLoader interface."
    },
    {
        "title": "​Interface",
        "type": "text",
        "content": "Each document loader may define its own parameters, but they share a common API:"
    },
    {
        "title": "​Interface",
        "type": "code",
        "content": "from langchain_community.document_loaders.csv_loader import CSVLoader\n\nloader = CSVLoader(\n    ...  # Integration-specific parameters here\n)\n\n# Load all documents\ndocuments = loader.load()\n\n# For large datasets, lazily load documents\nfor document in loader.lazy_load():\n    print(document)\n"
    },
    {
        "title": "​By category",
        "type": "text",
        "content": "The below document loaders allow you to load webpages.\n\nThe below document loaders allow you to load PDF documents."
    },
    {
        "title": "​By category",
        "type": "text",
        "content": "The below document loaders allow you to load documents from your favorite cloud providers."
    },
    {
        "title": "​By category",
        "type": "code",
        "content": "AzureBlobStorageLoader"
    },
    {
        "title": "​By category",
        "type": "code",
        "content": "TencentCOSDirectoryLoader"
    },
    {
        "title": "​By category",
        "type": "text",
        "content": "The below document loaders allow you to load documents from different social media platforms."
    },
    {
        "title": "​By category",
        "type": "text",
        "content": "The below document loaders allow you to load data from different messaging platforms."
    },
    {
        "title": "​By category",
        "type": "code",
        "content": "TelegramChatFileLoader"
    },
    {
        "title": "​By category",
        "type": "text",
        "content": "The below document loaders allow you to load data from commonly used productivity tools."
    },
    {
        "title": "​By category",
        "type": "code",
        "content": "NotionDirectoryLoader"
    },
    {
        "title": "​By category",
        "type": "text",
        "content": "The below document loaders allow you to load data from common data formats."
    },
    {
        "title": "ZeroxPDFLoader",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Learn",
        "type": "text",
        "content": "In the Learn section of the documentation, you’ll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph."
    },
    {
        "title": "​Use Cases",
        "type": "text",
        "content": "Below are tutorials for common use cases, organized by framework.\n\nLangChain agent implementations make it easy to get started for most use cases."
    },
    {
        "title": "Semantic Search",
        "type": "text",
        "content": "Build a semantic search engine over a PDF with LangChain components."
    },
    {
        "title": "RAG Agent",
        "type": "text",
        "content": "Create a Retrieval Augmented Generation (RAG) agent."
    },
    {
        "title": "SQL Agent",
        "type": "text",
        "content": "Build a SQL agent to interact with databases with human-in-the-loop review."
    },
    {
        "title": "Supervisor Agent",
        "type": "text",
        "content": "Build a personal assistant that delegates to sub-agents.\n\nLangChain’s agent implementations use LangGraph primitives.\nIf deeper customization is required, agents can be implemented directly in LangGraph."
    },
    {
        "title": "Custom RAG Agent",
        "type": "text",
        "content": "Build a RAG agent using LangGraph primitives for fine-grained control."
    },
    {
        "title": "Custom SQL Agent",
        "type": "text",
        "content": "Implement a SQL agent directly in LangGraph for maximum flexibility."
    },
    {
        "title": "​Conceptual Overviews",
        "type": "text",
        "content": "These guides explain the core concepts and APIs underlying LangChain and LangGraph."
    },
    {
        "title": "Memory",
        "type": "text",
        "content": "Understand persistence of interactions within and across threads."
    },
    {
        "title": "Context engineering",
        "type": "text",
        "content": "Learn methods for providing AI applications the right information and tools to accomplish a task."
    },
    {
        "title": "Graph API",
        "type": "text",
        "content": "Explore LangGraph’s declarative graph-building API."
    },
    {
        "title": "LangChain Academy",
        "type": "text",
        "content": "Courses and exercises to level up your LangChain skills."
    },
    {
        "title": "Case Studies",
        "type": "text",
        "content": "See how teams are using LangChain and LangGraph in production.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Learn",
        "type": "text",
        "content": "In the Learn section of the documentation, you’ll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph."
    },
    {
        "title": "​Use Cases",
        "type": "text",
        "content": "Below are tutorials for common use cases, organized by framework.\n\nLangChain agent implementations make it easy to get started for most use cases."
    },
    {
        "title": "Semantic Search",
        "type": "text",
        "content": "Build a semantic search engine over a PDF with LangChain components."
    },
    {
        "title": "RAG Agent",
        "type": "text",
        "content": "Create a Retrieval Augmented Generation (RAG) agent."
    },
    {
        "title": "SQL Agent",
        "type": "text",
        "content": "Build a SQL agent to interact with databases with human-in-the-loop review."
    },
    {
        "title": "Supervisor Agent",
        "type": "text",
        "content": "Build a personal assistant that delegates to sub-agents.\n\nLangChain’s agent implementations use LangGraph primitives.\nIf deeper customization is required, agents can be implemented directly in LangGraph."
    },
    {
        "title": "Custom RAG Agent",
        "type": "text",
        "content": "Build a RAG agent using LangGraph primitives for fine-grained control."
    },
    {
        "title": "Custom SQL Agent",
        "type": "text",
        "content": "Implement a SQL agent directly in LangGraph for maximum flexibility."
    },
    {
        "title": "​Conceptual Overviews",
        "type": "text",
        "content": "These guides explain the core concepts and APIs underlying LangChain and LangGraph."
    },
    {
        "title": "Memory",
        "type": "text",
        "content": "Understand persistence of interactions within and across threads."
    },
    {
        "title": "Context engineering",
        "type": "text",
        "content": "Learn methods for providing AI applications the right information and tools to accomplish a task."
    },
    {
        "title": "Graph API",
        "type": "text",
        "content": "Explore LangGraph’s declarative graph-building API."
    },
    {
        "title": "LangChain Academy",
        "type": "text",
        "content": "Courses and exercises to level up your LangChain skills."
    },
    {
        "title": "Case Studies",
        "type": "text",
        "content": "See how teams are using LangChain and LangGraph in production.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Memory overview",
        "type": "text",
        "content": "Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nThis conceptual guide covers two types of memory, based on their recall scope:"
    },
    {
        "title": "​Short-term memory",
        "type": "text",
        "content": "Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n\nLangGraph manages short-term memory as part of the agent’s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph’s state, the bot can access the full context for a given conversation while maintaining separation between different threads.\n\nConversation history is the most common form of short-term memory, and long conversations pose a challenge to today’s LLMs. A full history may not fit inside an LLM’s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.\n\nChat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.\n\nFor more information on common techniques for managing messages, see the Add and manage memory guide."
    },
    {
        "title": "​Long-term memory",
        "type": "code",
        "content": "# Node that *uses* the instructions\ndef call_model(state: State, store: BaseStore):\n    namespace = (\"agent_instructions\", )\n    instructions = store.get(namespace, key=\"agent_a\")[0]\n    # Application logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n    ...\n\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n"
    },
    {
        "title": "​Long-term memory",
        "type": "code",
        "content": "from langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context)\nstore.put(\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\")\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search(\n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n"
    },
    {
        "title": "​Long-term memory",
        "type": "text",
        "content": "For more information about the memory store, see the Persistence guide.\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Context overview",
        "type": "text",
        "content": "Context engineering is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n\nRuntime context refers to local context: data and dependencies your code needs to run. It does not refer to:\n\nRuntime context can be used to optimize the LLM context. For example, you can use user metadata\nin the runtime context to fetch user preferences and feed them into the context window.\n\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:"
    },
    {
        "title": "​Static runtime context",
        "type": "text",
        "content": "Static runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the context argument to invoke / stream . This data does not change during execution."
    },
    {
        "title": "​Static runtime context",
        "type": "code",
        "content": "@dataclass\nclass ContextSchema:\n    user_name: str\n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]},\n    context={\"user_name\": \"John Smith\"}  \n)\n"
    },
    {
        "title": "​Static runtime context",
        "type": "code",
        "content": "from dataclasses import dataclass\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\n@dataclass\nclass ContextSchema:\n    user_name: str\n\n@dynamic_prompt\ndef personalized_prompt(request: ModelRequest) -> str:  \n    user_name = request.runtime.context.user_name\n    return f\"You are a helpful assistant. Address the user as {user_name}.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    middleware=[personalized_prompt],\n    context_schema=ContextSchema\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    context=ContextSchema(user_name=\"John Smith\")  \n)\n"
    },
    {
        "title": "​Static runtime context",
        "type": "text",
        "content": "See Agents for details.\n\nThe Runtime object can be used to access static context and other utilities like the active store and stream writer.\nSee the @[ Runtime ][langgraph.runtime.Runtime] documentation for details."
    },
    {
        "title": "​Dynamic runtime context",
        "type": "text",
        "content": "Dynamic runtime context represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.\n\nExample shows how to incorporate state into an agent prompt .\n\nState can also be accessed by the agent’s tools , which can read or update the state as needed. See tool calling guide for details."
    },
    {
        "title": "​Dynamic runtime context",
        "type": "code",
        "content": "from langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):  \n    user_name: str\n\n@dynamic_prompt\ndef personalized_prompt(request: ModelRequest) -> str:  \n    user_name = request.state.get(\"user_name\", \"User\")\n    return f\"You are a helpful assistant. User's name is {user_name}\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[...],\n    state_schema=CustomState,  \n    middleware=[personalized_prompt],  \n)\n\nagent.invoke({\n    \"messages\": \"hi!\",\n    \"user_name\": \"John Smith\"\n})\n"
    },
    {
        "title": "​Dynamic runtime context",
        "type": "text",
        "content": "Turning on memory Please see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agent’s state across multiple invocations. Otherwise, the state is scoped only to a single run."
    },
    {
        "title": "​Dynamic cross-conversation context",
        "type": "text",
        "content": "Dynamic cross-conversation context represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions)."
    },
    {
        "title": "​See also",
        "type": "text",
        "content": "Edit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Contributing",
        "type": "text",
        "content": "Welcome! Thank you for your interest in contributing.\n\nLangChain has helped form the largest developer community in generative AI, and we’re always open to new contributors. Whether you’re fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone."
    },
    {
        "title": "​Ways to Contribute",
        "type": "text",
        "content": "Found a bug? Please help us fix it by following these steps:\n\nCheck if the issue already exists in our GitHub Issues for the respective repo:"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example . Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.\n\nA project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.\n\nIf you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,"
    },
    {
        "title": "LangGraph",
        "type": "code",
        "content": "This issue is blocked by #123 and related to #456.\n"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "Have an idea for a new feature or enhancement?\n\nSearch the issues for the respective repository for existing feature requests:"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "If no requests exist, start a new discussion under the relevant category so that project maintainers and the community can provide feedback.\n\nBe sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.\n\nDocumentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference."
    },
    {
        "title": "How to propose changes to the documentation",
        "type": "text",
        "content": "With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!"
    },
    {
        "title": "How to make your first Pull Request",
        "type": "text",
        "content": "If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.\n\nIf you are looking for something to work on, check out the issues labeled “good first issue” or “help wanted” in our repos:"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "Thank you for helping make LangChain better! 🦜❤️\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Contributing",
        "type": "text",
        "content": "Welcome! Thank you for your interest in contributing.\n\nLangChain has helped form the largest developer community in generative AI, and we’re always open to new contributors. Whether you’re fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone."
    },
    {
        "title": "​Ways to Contribute",
        "type": "text",
        "content": "Found a bug? Please help us fix it by following these steps:\n\nCheck if the issue already exists in our GitHub Issues for the respective repo:"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example . Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.\n\nA project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.\n\nIf you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,"
    },
    {
        "title": "LangGraph",
        "type": "code",
        "content": "This issue is blocked by #123 and related to #456.\n"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "Have an idea for a new feature or enhancement?\n\nSearch the issues for the respective repository for existing feature requests:"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "If no requests exist, start a new discussion under the relevant category so that project maintainers and the community can provide feedback.\n\nBe sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.\n\nDocumentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference."
    },
    {
        "title": "How to propose changes to the documentation",
        "type": "text",
        "content": "With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!"
    },
    {
        "title": "How to make your first Pull Request",
        "type": "text",
        "content": "If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.\n\nIf you are looking for something to work on, check out the issues labeled “good first issue” or “help wanted” in our repos:"
    },
    {
        "title": "LangGraph",
        "type": "text",
        "content": "Thank you for helping make LangChain better! 🦜❤️\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Contributing to documentation",
        "type": "text",
        "content": "Accessible documentation is a vital part of LangChain. We welcome both documentation for new features/ integrations , as well as community improvements to existing docs.\n\nWe generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please open a new issue .\n\nAll documentation falls under one of four categories:"
    },
    {
        "title": "Conceptual guides",
        "type": "text",
        "content": "Explanations that provide deeper understanding and insights"
    },
    {
        "title": "References",
        "type": "text",
        "content": "Technical descriptions of APIs and implementation details"
    },
    {
        "title": "Tutorials (Learn)",
        "type": "text",
        "content": "Lessons that guide users through practical activities to build understanding"
    },
    {
        "title": "How-to guides",
        "type": "text",
        "content": "Task-oriented instructions for users who know what they want to accomplish"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "For simple changes like fixing typos, you can edit directly on GitHub without setting up a local development environment:\n\nPrerequisites:\n\nNavigate to any documentation page, scroll to the bottom of the page, and click the link “Edit the source of this page on GitHub”\n\nGitHub will prompt you to fork the repository to your account. Make sure to fork into your personal account\n\nCorrect the typo directly in GitHub’s web editor\n\nClick Commit changes... and give your commit a descriptive title like fix(docs): summary of change . If applicable, add an extended description"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "fix(docs): summary of change"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist, if present\n\nDocs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers. Do not bump the PR unless you have new information to provide - maintainers will address it as their availability permits.\n\nFor larger changes or ongoing contributions, it’s important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting.\n\nPlease review the steps to set up your environment outlined in the docs repo README.md ."
    },
    {
        "title": "​Documentation types",
        "type": "text",
        "content": "Where applicable, all documentation must have translations in both Python and JavaScript/TypeScript. See our localization guide for details.\n\nConceptual guide cover core concepts abstractly, providing deep understanding."
    },
    {
        "title": "Context",
        "type": "text",
        "content": "Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it."
    },
    {
        "title": "Python reference",
        "type": "text",
        "content": "A good reference should:\n\nSee the contributing guide for Python reference docs ."
    },
    {
        "title": "​Writing standard",
        "type": "text",
        "content": "Reference documentation has different standards - see the reference docs contributing guide for details.\n\nUse appropriate Mintlify components to enhance readability:"
    },
    {
        "title": "​Writing standard",
        "type": "text",
        "content": "Every documentation page must begin with YAML frontmatter:"
    },
    {
        "title": "​Writing standard",
        "type": "code",
        "content": "---\ntitle: \"Clear, specific title\"\n---\n"
    },
    {
        "title": "​Writing standard",
        "type": "text",
        "content": "All documentation must be localized in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:"
    },
    {
        "title": "​Writing standard",
        "type": "code",
        "content": ":::python\nPython-specific content. In real docs, the preceding backslash (before `python`) is omitted.\n:::\n\n:::js\nJavaScript/TypeScript-specific content. In real docs, the preceding backslash (before `js`) is omitted.\n:::\n\nContent for both languages (not wrapped)\n"
    },
    {
        "title": "​Writing standard",
        "type": "text",
        "content": "We don’t want a lack of parity to block contributions. If a feature is only available in one language, it’s okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.\n\nIf you need help translating content between Python and JavaScript/TypeScript, please ask in the community slack or tag a maintainer in your PR."
    },
    {
        "title": "​Quality standards",
        "type": "text",
        "content": "Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.\n\nDocumentation sections don’t exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.\n\nTake a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.\n\nEnsure documentation is accessible to all users:\n\nBefore submitting documentation:\n\nRun all code examples to ensure they work correctly"
    },
    {
        "title": "​Quality standards",
        "type": "code",
        "content": "make lint\nmake format\n"
    },
    {
        "title": "​Quality standards",
        "type": "text",
        "content": "Verify the build completes without errors\n\nCheck that all internal links work correctly"
    },
    {
        "title": "​In-code documentation",
        "type": "text",
        "content": "Use Google-style docstrings with complete type hints for all public functions.\n\nFollow these standards for all documentation:\n\nAlways test code examples before publishing. Never include real API keys or secrets.\n\nRequirements for code examples:\n\nInclude complete, runnable examples that users can copy and execute without errors\n\nUse realistic data instead of placeholder values like “foo” or “example”\n\nShow proper error handling and edge case management\n\nAdd explanatory comments for complex logic\n\nExample of a well-documented function:"
    },
    {
        "title": "​In-code documentation",
        "type": "code",
        "content": "def filter_unknown_users(users: list[str], known_users: set[str]) -> list[str]:\n    \"\"\"Filter out users that are not in the known users set.\n\n    Args:\n        users: List of user identifiers to filter.\n        known_users: Set of known/valid user identifiers.\n\n    Returns:\n        List of users that are not in the known_users set.\n\n    Raises:\n        ValueError: If users list contains invalid identifiers.\n    \"\"\"\n    return [user for user in users if user not in known_users]\n"
    },
    {
        "title": "​Getting help",
        "type": "text",
        "content": "Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post .\n\nYou now have everything you need to contribute high-quality documentation to LangChain! 🎤🦜\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    },
    {
        "title": "Contributing to code",
        "type": "text",
        "content": "Code contributions are always welcome! Whether you’re fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.\n\nBefore submitting large new features or refactors , please first discuss your ideas in the forum . This ensures alignment with project goals and prevents duplicate work.\n\nThis does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. Be sure to link any relevant issues in your PR description. Use closing keywords to automatically close issues when the PR is merged.\n\nNew integrations should follow the integration guidelines ."
    },
    {
        "title": "​Philosophy",
        "type": "text",
        "content": "Aim to follow these core principles for all code contributions:"
    },
    {
        "title": "Backwards compatibility",
        "type": "text",
        "content": "Maintain stable public interfaces and avoid breaking changes"
    },
    {
        "title": "Testing first",
        "type": "text",
        "content": "Every change must include comprehensive tests to verify correctness and prevent regressions"
    },
    {
        "title": "Code quality",
        "type": "text",
        "content": "Follow consistent style, documentation, and architecture patterns"
    },
    {
        "title": "Security focused",
        "type": "text",
        "content": "Prioritize secure coding practices and vulnerability prevention"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "For simple bugfixes, you can get started immediately:\n\nFork the LangChain or LangGraph repo to your personal GitHub account"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "git clone https://github.com/your-username/name-of-forked-repo.git\n\n# For instance, for LangChain:\ngit clone https://github.com/parrot123/langchain.git\n\n# For LangGraph:\ngit clone https://github.com/parrot123/langgraph.git\n"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "# Inside your repo, install dependencies\nuv sync --all-groups\n"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "You will need to install uv if you haven’t previously."
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "git checkout -b your-username/short-bugfix-name\n"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "Fix the bug while following our code quality standards\n\nInclude unit tests that fail without your fix. This allows us to verify the bug is resolved and prevents regressions\n\nEnsure all tests pass locally before submitting your PR"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "make lint\nmake test\n\n# For bugfixes involving integrations, also run:\nmake integration_tests\n"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "Follow the PR template provided. If applicable, reference the issue you’re fixing using a closing keyword (e.g. Fixes #123 )."
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "For ongoing development or larger contributions:\n\nSet up your environment following our setup guide below\n\nUnderstand the repository structure and package organization\n\nLearn our development workflow including testing and linting\n\nReview our contribution guidelines for features, bugfixes, and integrations\n\nOur Python projects use uv for dependency management. Make sure you have the latest version installed."
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "Set up a development environment for the package(s) you’re working on.\n\nFor changes to langchain-core :"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "cd libs/core\nuv sync --all-groups\nmake test  # Ensure tests pass before starting development\n"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "cd libs/langchain\nuv sync --all-groups\nmake test  # Ensure tests pass before starting development\n"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "cd libs/partners/langchain-{partner}\nuv sync --all-groups\nmake test  # Ensure tests pass before starting development\n"
    },
    {
        "title": "​Getting started",
        "type": "text",
        "content": "For changes to community integrations (located in a separate repo ):"
    },
    {
        "title": "​Getting started",
        "type": "code",
        "content": "cd libs/community/langchain_community/path/to/integration\nuv sync --all-groups\nmake test  # Ensure tests pass before starting development\n"
    },
    {
        "title": "​Repository structure",
        "type": "text",
        "content": "LangChain is organized as a monorepo with multiple packages:"
    },
    {
        "title": "​Repository structure",
        "type": "text",
        "content": "Located in libs/partners/ , these are independently versioned packages for specific integrations. For example:"
    },
    {
        "title": "​Repository structure",
        "type": "code",
        "content": "langchain-google-genai"
    },
    {
        "title": "​Repository structure",
        "type": "text",
        "content": "Many partner packages are in external repositories. Please check the list of integrations for details."
    },
    {
        "title": "​Repository structure",
        "type": "code",
        "content": "langchain-text-splitters"
    },
    {
        "title": "​Repository structure",
        "type": "code",
        "content": "langchain-standard-tests"
    },
    {
        "title": "​Development workflow",
        "type": "text",
        "content": "Directories are relative to the package you’re working in.\n\nEvery code change must include comprehensive tests.\n\nLocation : tests/unit_tests/"
    },
    {
        "title": "​Development workflow",
        "type": "code",
        "content": "make test\n# Or directly:\nuv run --group test pytest tests/unit_tests\n"
    },
    {
        "title": "​Development workflow",
        "type": "text",
        "content": "Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.\n\nNot every code change will require an integration test, but keep in mind that we’ll require/ run integration tests separately as apart of our review process.\n\nLocation : tests/integration_tests/"
    },
    {
        "title": "​Development workflow",
        "type": "code",
        "content": "tests/integration_tests/"
    },
    {
        "title": "​Development workflow",
        "type": "code",
        "content": "make integration_tests\n"
    },
    {
        "title": "​Development workflow",
        "type": "text",
        "content": "Quality requirements:\n\nRequired : Complete type annotations for all functions"
    },
    {
        "title": "​Development workflow",
        "type": "code",
        "content": "def process_documents(\n    docs: list[Document],\n    processor: DocumentProcessor,\n    *,\n    batch_size: int = 100\n) -> ProcessingResult:\n    \"\"\"Process documents in batches.\n\n    Args:\n        docs: List of documents to process.\n        processor: Document processing instance.\n        batch_size: Number of documents per batch.\n\n    Returns:\n        Processing results with success/failure counts.\n    \"\"\"\n"
    },
    {
        "title": "​Development workflow",
        "type": "text",
        "content": "Code formatting and linting are enforced via CI/CD. Run these commands before committing to ensure your code passes checks.\n\nRun formatting and linting:"
    },
    {
        "title": "​Development workflow",
        "type": "text",
        "content": "Both commands will show you any formatting or linting issues that need to be addressed before committing."
    },
    {
        "title": "​Contribution guidelines",
        "type": "text",
        "content": "Breaking changes to public APIs are not allowed except for critical security fixes.\n\nSee our versioning policy for details on major version releases.\n\nMaintain compatibility:\n\nAlways preserve :\n\nAcceptable modifications :\n\nAdding new optional parameters\n\nAdding new methods to classes\n\nImproving performance without changing behavior\n\nAdding new modules or functions\n\nWould this break existing user code?\n\nCheck if your target is public\n\nIf needed, is it exported in __init__.py ?"
    },
    {
        "title": "​Contribution guidelines",
        "type": "text",
        "content": "Are there existing usage patterns in tests?\n\nFor bugfix contributions:\n\nCreate a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification\n\nAdd unit tests that would fail without your fix\n\nMake the minimal change necessary to resolve the issue\n\nEnsure that tests pass and no regressions are introduced\n\nUpdate docstrings if behavior changes, add comments for complex logic\n\nWe aim to keep the bar high for new features. We generally don’t accept new core abstractions, changes to infra, changes to dependencies, or new agents/chains from outside contributors without an existing issue that demonstrates an acute need for them.\n\nIn general, feature contribution requirements include:\n\nOpen an issue describing:\n\nWe will reject features that are likely to lead to security vulnerabilities or reports.\n\nSecurity is paramount. Never introduce vulnerabilities or unsafe patterns.\n\nSecurity checklist:"
    },
    {
        "title": "​Testing and validation",
        "type": "text",
        "content": "Before submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph."
    },
    {
        "title": "​Testing and validation",
        "type": "code",
        "content": "make integration_tests\n"
    },
    {
        "title": "​Testing and validation",
        "type": "code",
        "content": "make format\nmake lint\n"
    },
    {
        "title": "​Testing and validation",
        "type": "text",
        "content": "All type hints must be valid\n\nPush your branch and open a pull request. Follow the provided form template. Note related issues using a closing keyword . After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.\n\nIn order to write effective tests, there’s a few good practices to follow:"
    },
    {
        "title": "​Testing and validation",
        "type": "code",
        "content": "def test_document_processor_handles_empty_input():\n    \"\"\"Test processor gracefully handles empty document list.\"\"\"\n    processor = DocumentProcessor()\n\n    result = processor.process([])\n\n    assert result.success\n    assert result.processed_count == 0\n    assert len(result.errors) == 0\n"
    },
    {
        "title": "​Getting help",
        "type": "text",
        "content": "Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post .\n\nYou’re now ready to contribute high-quality code to LangChain!\n\nEdit the source of this page on GitHub.\n\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers."
    }
]