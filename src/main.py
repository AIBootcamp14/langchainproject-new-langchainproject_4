# src/main.py

from fastapi import FastAPI
from dotenv import load_dotenv
from .schemas.rag_schema import QuestionRequest, AnswerResponse
from typing import List

# .env íŒŒì¼ ë¡œë“œ (SOLAR_API_KEY ë“±ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ê°€ì ¸ì˜´)
load_dotenv() 

# ğŸ’¡ ì°¸ê³ : LLM ë° RAG ëª¨ë“ˆì€ ë‚˜ì¤‘ì— í†µí•©í•  ê±°ì•¼
# from .modules.retriever import get_rag_response 

app = FastAPI(
    title="LangChain RAG Chatbot API",
    description="ê¸°ìˆ  ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ End-to-End RAG API ì„œë¹„ìŠ¤"
)

@app.get("/")
def health_check():
    """
    API ì„œë²„ ìƒíƒœ í™•ì¸ìš© ì—”ë“œí¬ì¸íŠ¸
    """
    return {"status": "ok", "message": "RAG API is running."}


@app.post("/ask", response_model=AnswerResponse)
async def ask_chatbot(request: QuestionRequest):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ RAG ì²´ì¸ì„ í†µí•´ ë‹µë³€ê³¼ ì¶œì²˜ë¥¼ ë°˜í™˜
    """
    question = request.question
    
    # ğŸ“Œ TODO: íŒ€ì› 2ì˜ RAG ì½”ì–´ ë¡œì§ í†µí•© ìœ„ì¹˜
    # answer, sources = get_rag_response(question)
    
    # ì„ì‹œ ì‘ë‹µ (íŒ€ì› 2 ì‘ì—…ì´ ì™„ë£Œë˜ê¸° ì „ê¹Œì§€ ì‚¬ìš©)
    # íŒ€ì› 2, 3ì´ API í…ŒìŠ¤íŠ¸ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ëª…ì„¸ì— ë§ëŠ” ë”ë¯¸ ì‘ë‹µì„ ë°˜í™˜
    dummy_answer = f"FastAPI ì„œë²„ ì‘ë™ í™•ì¸: ì§ˆë¬¸ '{question}'ì— ëŒ€í•œ ë‹µë³€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤."
    dummy_sources: List[str] = [
        "https://docs.langchain.com/oss/python/integrations/splitter-example1", 
        "https://docs.langchain.com/oss/python/integrations/example-source2"
    ]

    return AnswerResponse(
        answer=dummy_answer,
        sources=dummy_sources
    )

# ğŸ’¡ ì°¸ê³ : uvicorn ëª…ë ¹ì–´ë¡œ ì‹¤í–‰ë¨: uvicorn main:app --host 0.0.0.0 --port 8000