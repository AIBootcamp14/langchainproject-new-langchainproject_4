from typing import List, Dict, Any, Optional

from langchain_core.runnables import Runnable, RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.vectorstores import VectorStore
from langchain_core.documents import Document
from langchain_core.language_models import BaseChatModel

from .llm import get_solar_llm
from .prompts import RAG_PROMPT

# PEP 8: ëª¨ë“ˆ ìˆ˜ì¤€ ìƒìˆ˜ëŠ” ëŒ€ë¬¸ìë¡œ
DEFAULT_K = 3 # ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜ ê¸°ë³¸ê°’

# íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•œ ë³„ì¹­ (PEP 484 ì¤€ìˆ˜)
RAGChain = Runnable[Dict[str, Any], Dict[str, Any]]


def format_docs(docs: List[Document]) -> str:
    """
    ê²€ìƒ‰ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ LLMì˜ Context ì…ë ¥ì— ì í•©í•œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
    """
    return "\n\n".join(doc.page_content for doc in docs)


def get_source_aware_rag_chain(vectorstore: VectorStore, k: int = DEFAULT_K) -> RAGChain:
    """
    LCELì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ì¶œì²˜ ì •ë³´ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    
    Args:
        vectorstore: ë°ì´í„°ê°€ ì ì¬ëœ ë²¡í„° ì €ì¥ì†Œ ê°ì²´
        k: ê²€ìƒ‰í•  ë¬¸ì„œì˜ ê°œìˆ˜

    Returns:
        RAGChain: ì‚¬ìš©ì ì§ˆë¬¸(str)ì„ ë°›ì•„ {'answer': str, 'source_documents': List[Document]}ë¥¼ ë°˜í™˜í•˜ëŠ” ì²´ì¸
    """
    llm: BaseChatModel = get_solar_llm()
    
    # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •]: vectorstore.as_retriever() ì‚¬ìš© ëŒ€ì‹ , 
    #    Langchain-upstage ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì ì¬ì  ë²„ê·¸ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ 
    #    similarity_searchë¥¼ RunnableLambdaë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
    retriever = RunnableLambda(
        # xëŠ” {'question': str, 'session_id': str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
        lambda x: vectorstore.similarity_search(x["question"], k=k) 
    ).with_config(run_name="CustomRetrieval")

    # 1. ê²€ìƒ‰ ë‹¨ê³„ (Retrieval): ì§ˆë¬¸ì„ ë°›ì•„ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³ , Contextë¡œ í¬ë§·íŒ…
    # RunnablePassthrough.assignì„ ì‚¬ìš©í•´ ìµœì¢… ì¶œë ¥ì— 'docs'ì™€ 'context'ë¥¼ í¬í•¨
    setup_and_retrieval = RunnablePassthrough.assign(
        docs=retriever, # ê²€ìƒ‰ëœ Document ë¦¬ìŠ¤íŠ¸
    ).assign(
        context=lambda x: format_docs(x["docs"]), # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ Context ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
        question=lambda x: x["question"], # ì§ˆë¬¸ í‚¤ë¥¼ ìœ ì§€
    )

    # 2. ìƒì„± ë‹¨ê³„ (Generation): ê²€ìƒ‰ ê²°ê³¼ì™€ ì§ˆë¬¸ì„ í”„ë¡¬í”„íŠ¸ì— ë„£ì–´ LLMì—ê²Œ ë‹µë³€ì„ ìš”ì²­
    answer_generation = (
        RAG_PROMPT
        | llm
        | StrOutputParser()
    )

    # 3. ìµœì¢… ì²´ì¸: ê²€ìƒ‰ ê²°ê³¼ì™€ ë‹µë³€ ê²°ê³¼ë¥¼ í•©ì³ì„œ ë°˜í™˜
    # setup_and_retrievalì˜ ì¶œë ¥ì— 'answer' í‚¤ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ë¥¼ êµ¬ì„±
    final_chain: RAGChain = setup_and_retrieval.assign(
        answer=answer_generation,
    ).assign(
        # 'docs' í‚¤ë¥¼ 'source_documents'ë¡œ ì´ë¦„ ë³€ê²½í•˜ì—¬ main.pyì˜ ì‘ë‹µ ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜ì‹œí‚´
        source_documents=lambda x: x["docs"],
    ).with_config(
        output_keys=["answer", "source_documents"] # ë¶ˆí•„ìš”í•œ í‚¤ ì œê±°
    )
    
    return final_chain