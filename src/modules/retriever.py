# src/modules/retriever.py (ì „ë©´ ìˆ˜ì •)

from typing import List, Dict, Any

from langchain_core.runnables import Runnable, RunnablePassthrough, RunnableLambda
from langchain_core.vectorstores import VectorStore
from langchain_core.documents import Document
from langchain_core.language_models import BaseChatModel
from langchain_core.output_parsers import StrOutputParser
from langchain.chains.combine_documents import create_stuff_documents_chain

# LangChainì˜ ëŒ€í™”í˜• ì²´ì¸ ìƒì„± í—¬í¼ ì„í¬íŠ¸ (ì§ˆë¬¸ ì¬êµ¬ì„±ì—ë§Œ ì‚¬ìš©)
from langchain.chains.history_aware_retriever import create_history_aware_retriever

# llm ëª¨ë“ˆ ë° í”„ë¡¬í”„íŠ¸ ì„í¬íŠ¸
from .llm import get_solar_llm, get_solar_sql_llm
from .prompts import RAG_PROMPT, CONDENSE_QUESTION_PROMPT, TRANSLATE_PROMPT

# íƒ€ì… íŒíŠ¸
ConversationalRAGChain = Runnable[Dict[str, Any], Dict[str, Any]]

def format_docs(docs: List[Document]) -> str:
    """ (ì´ í•¨ìˆ˜ëŠ” ë³€ê²½ ì—†ìŒ) """
    return "\n\n".join(doc.page_content for doc in docs)

def get_conversational_rag_chain(vectorstore: VectorStore) -> ConversationalRAGChain:
    """
    [ì—…ê·¸ë ˆì´ë“œëœ ëŒ€í™”í˜• RAG ì²´ì¸]
    í•œêµ­ì–´ ì§ˆë¬¸ -> í•œêµ­ì–´ ë…ë¦½ ì§ˆë¬¸ -> ì˜ì–´ ë²ˆì—­ -> ì˜ë¬¸ì„œ ê²€ìƒ‰ -> í•œêµ­ì–´ ë‹µë³€
    """
    llm: BaseChatModel = get_solar_llm()
    # ë²ˆì—­ì€ ì •í™•í•´ì•¼ í•˜ë¯€ë¡œ temperature=0.0 ì‚¬ìš© (SQL LLM ì¬í™œìš©)
    translation_llm: BaseChatModel = get_solar_sql_llm() 
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # --- 1. ë…ë¦½ì ì¸ ì§ˆë¬¸ ìƒì„± ì²´ì¸ (í•œêµ­ì–´) ---
    # (ì…ë ¥: question, chat_history -> ì¶œë ¥: standalone_korean_question)
    condense_question_chain = (
        CONDENSE_QUESTION_PROMPT
        | llm
        | StrOutputParser()
    )

    # --- 2. ì§ˆë¬¸ ë²ˆì—­ ì²´ì¸ (í•œêµ­ì–´ -> ì˜ì–´) ---
    # (ì…ë ¥: question -> ì¶œë ¥: standalone_english_question)
    translation_chain = (
        TRANSLATE_PROMPT
        | translation_llm
        | StrOutputParser()
    )

    # --- 3. ë‹µë³€ ìƒì„± ì²´ì¸ (RAGì˜ í•µì‹¬) ---
    # (ì…ë ¥: context, question -> ì¶œë ¥: answer)
    # RAG_PROMPTëŠ” í•œêµ­ì–´ë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì˜ì–´ Contextì™€ í•œêµ­ì–´ Questionì„ ë°›ì•„
    # í•œêµ­ì–´ Answerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    answer_chain = create_stuff_documents_chain(
        llm,
        RAG_PROMPT 
    )

    # --- 4. LCELì„ ì‚¬ìš©í•˜ì—¬ ìœ„ 3ê°œ ì²´ì¸ + ê²€ìƒ‰(Retriever)ì„ í†µí•© ---

    # 4.1. ì›ë³¸ ì…ë ¥ì„ ë°›ì•„ 'standalone_korean_question' ìƒì„±
    chain_with_standalone_ko = RunnablePassthrough.assign(
        standalone_korean_question=condense_question_chain
    )

    # 4.2. (1)ì˜ ê²°ê³¼(standalone_korean_question)ë¥¼ 'translation_chain'ì— ì „ë‹¬
    chain_with_standalone_en = chain_with_standalone_ko.assign(
        standalone_english_question=RunnableLambda(
            lambda x: {"question": x["standalone_korean_question"]}
        ) | translation_chain
    )

    # 4.3. (2)ì˜ ê²°ê³¼(standalone_english_question)ë¥¼ 'retriever'ì— ì „ë‹¬í•˜ì—¬ 'context' ìƒì„±
    chain_with_context = chain_with_standalone_en.assign(
        context=RunnableLambda(
            lambda x: x["standalone_english_question"]
        ) | retriever
    )
    
    # 4.4. (3)ì˜ ê²°ê³¼('context')ì™€ ì›ë³¸ 'question'ì„ 'answer_chain'ì— ì „ë‹¬
    # ğŸ’¡ 'question' í‚¤ëŠ” RunnablePassthroughê°€ ì›ë³¸ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•´ì¤ë‹ˆë‹¤.
    chain_with_answer = chain_with_context.assign(
        answer=answer_chain
    )

    # 4.5. ìµœì¢… ì¶œë ¥ í¬ë§·íŒ…
    final_chain: ConversationalRAGChain = chain_with_answer.assign(
        source_documents=lambda x: x["context"],
    ).with_config(
        output_keys=["answer", "source_documents"] # main.pyì™€ í˜¸í™˜
    )
    
    return final_chain