# src/modules/retriever.py

"""
ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ëª¨ë“ˆ
- LCELì„ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ êµ¬ì„±
- LLM í˜¸ì¶œ, ê²€ìƒ‰, ì‘ë‹µ ìƒì„± ë¡œì§ í¬í•¨
"""

import os
from typing import List, Dict, Any, Final

# ì¨ë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.documents import Document
from langchain_core.runnables import Runnable, RunnablePassthrough # Runnable íƒ€ì… ì¶”ê°€
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.vectorstores import VectorStoreRetriever

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.modules.llm import get_solar_llm # Solar LLM ì„í¬íŠ¸
from src.modules.vector_database import VectorDatabaseClient

# --- ì„¤ì • ë° ìƒìˆ˜ ---
COLLECTION_NAME: Final[str] = "langchain_docs"
EMBEDDING_MODEL_NAME: Final[str] = "solar-embedding-1-large"
RETRIEVAL_K: Final[int] = 5 # ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜


# RAG ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (PEP 8 ì¤€ìˆ˜)
RAG_PROMPT_TEMPLATE: Final[str] = """
ë‹¹ì‹ ì€ LangChain ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ 'context'ë§Œì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
ë§Œì•½ ì£¼ì–´ì§„ context ë‚´ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, 'ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì‹­ì‹œì˜¤. ë‹µë³€ì— ì¶œì²˜ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

---
Question: {question}

Context: 
{context}
---
"""


# Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def _format_docs(docs: List[Document]) -> str:
    """Retrieverì—ì„œ ë°˜í™˜ëœ Document ë¦¬ìŠ¤íŠ¸ë¥¼ LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í¬ë§·í•œë‹¤."""
    # ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ í•©ì³ì„œ ë°˜í™˜
    return "\n\n".join([doc.page_content for doc in docs])


class RAGRetriever:
    """
    RAG íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤.
    """

    def __init__(self) -> None:
        """í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ(LLM, DB í´ë¼ì´ì–¸íŠ¸, ì²´ì¸)ë¥¼ ì´ˆê¸°í™”í•œë‹¤."""
        # LLM ì´ˆê¸°í™” (RAGëŠ” ì°½ì˜ì„±ë³´ë‹¤ ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ì˜¨ë„ëŠ” ë‚®ê²Œ ì„¤ì •)
        self.llm = get_solar_llm(temperature=0.05) 
        
        # VectorDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.vdb_client: VectorDatabaseClient = VectorDatabaseClient(
            collection_name=COLLECTION_NAME,
            embedding_model=EMBEDDING_MODEL_NAME
        )
        
        # Retriever ì´ˆê¸°í™”
        # ì—¬ê¸°ì„œ DB ì—°ê²°ì´ ì‹¤ì œë¡œ ë°œìƒí•˜ë©°, ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•´ì•¼ í•¨ (FastAPI startupì—ì„œ ì²˜ë¦¬)
        self.retriever: VectorStoreRetriever = self.vdb_client.get_retriever(k=RETRIEVAL_K)
        
        # LCEL RAG ì²´ì¸ ì´ˆê¸°í™”
        self.rag_chain: Runnable = self._create_rag_chain()

    def _create_rag_chain(self) -> Runnable:
        """
        LCEL (LangChain Expression Language)ì„ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ êµ¬ì„±í•œë‹¤.
        """
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

        # RAG íŒŒì´í”„ë¼ì¸ (LCEL) êµ¬ì„±
        rag_chain = (
            # 1. ì…ë ¥ (question)ì„ ë°›ì•„ì„œ
            {
                # 2. 'context' í‚¤ì—ëŠ” retrieverë¥¼ í†µí•´ ë¬¸ì„œ ê²€ìƒ‰ í›„ í¬ë§·íŒ…í•œ ê²°ê³¼ë¬¼ì„ ë„£ê³ 
                "context": self.retriever | _format_docs, 
                # 3. 'question' í‚¤ì—ëŠ” ì§ˆë¬¸(ì…ë ¥)ì„ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¨ë‹¤.
                "question": RunnablePassthrough() 
            }
            # 4. í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•˜ê³ 
            | prompt
            # 5. LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ê³ 
            | self.llm
            # 6. ë¬¸ìì—´ë¡œ íŒŒì‹±í•œë‹¤.
            | StrOutputParser() 
        )
        return rag_chain

    def answer_query(self, question: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ê³  ê²€ìƒ‰ëœ ì¶œì²˜ URLì„ ë°˜í™˜í•œë‹¤.
        
        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¬¸ìì—´.
            
        Returns:
            ë‹µë³€ ë° ì¶œì²˜ URLì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬.
        """
        
        # 1. ê²€ìƒ‰ëœ ë¬¸ì„œ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì¶œì²˜ URL ì¶”ì¶œì„ ìœ„í•´)
        # ğŸ’¡ [í•„ìˆ˜]: LLM ì²´ì¸ì´ ì•„ë‹Œ, retrieverì—ì„œ ê²€ìƒ‰ëœ ê²°ê³¼ë¬¼ì„ ë¯¸ë¦¬ ê°€ì ¸ì™€ì•¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.
        # RAG ì²´ì¸ ì‹¤í–‰ ì‹œ contextë¥¼ ìœ„í•´ retrieverê°€ í•œ ë²ˆ ë” ì‹¤í–‰ë  ìˆ˜ ìˆì§€ë§Œ, 
        # ë©”íƒ€ë°ì´í„°ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” ë³„ë„ì˜ retriever.invoke(question)ì´ í•„ìš”í•˜ë‹¤.
        retrieved_docs: List[Document] = self.retriever.invoke(question)

        # 2. RAG ì²´ì¸ ì‹¤í–‰ (ë‹µë³€ ìƒì„±)
        answer: str = self.rag_chain.invoke(question)

        # 3. ì¶œì²˜ URL ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
        source_urls: List[str] = list(
            set(
                doc.metadata["url"] 
                for doc in retrieved_docs 
                if "url" in doc.metadata
            )
        )

        # 4. ê²°ê³¼ ë°˜í™˜ (main.pyì—ì„œ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •)
        return {
            "answer": answer,
            "source_urls": source_urls
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œëŠ” VectorDatabaseClientì™€ LLMì´ ì‘ë™í•  ë•Œë§Œ ì˜ë¯¸ê°€ ìˆìœ¼ë¯€ë¡œ ê°„ë‹¨íˆ ì‘ì„±
    from dotenv import load_dotenv
    load_dotenv()
    
    print("=" * 50)
    print("RAGRetriever ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    try:
        # DBê°€ ì‹¤í–‰ ì¤‘ì´ê³  ë°ì´í„°ê°€ ì ì¬ëœ í›„ì—ë§Œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
        rag_retriever = RAGRetriever()
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_question = "LCELì„ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        
        print(f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_question}")
        
        # answer_query í˜¸ì¶œ
        response = rag_retriever.answer_query(test_question)
        
        print("\n=== RAG ì‘ë‹µ ê²°ê³¼ ===")
        print(f"ë‹µë³€: {response['answer']}")
        print(f"ì¶œì²˜ URL: {response['source_urls']}")
        
    except Exception as e:
        print(f"âŒ RAGRetriever í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e.__class__.__name__} - {e}")
        print("ChromaDB ì„œë²„ì™€ LLM API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")