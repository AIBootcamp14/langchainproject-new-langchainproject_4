# src/utils/data_collector.py

"""
ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
LangChain ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” ê¸°ëŠ¥
"""

import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

# ì¨ë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
from tqdm import tqdm
from langchain_core.documents import Document
from langchain_community.document_loaders import WebBaseLoader

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ìƒìˆ˜ëŠ” ëŒ€ë¬¸ìë¡œ
DEFAULT_BASE_URL: str = "https://python.langchain.com/"


class DataCollector:
    """LangChain ë¬¸ì„œ ìˆ˜ì§‘ ë° ì²˜ë¦¬ í´ë˜ìŠ¤ (SQLite ê¸°ëŠ¥ ì œê±°, ìˆœìˆ˜ í¬ë¡¤ë§ ê¸°ëŠ¥ë§Œ ìœ ì§€)"""

    def __init__(
        self,
        base_url: str = DEFAULT_BASE_URL,
    ) -> None:
        """
        DataCollector ì´ˆê¸°í™”. (SQLite DB ê²½ë¡œëŠ” ì œê±°ë¨)

        Args:
            base_url: LangChain ë¬¸ì„œ ê¸°ë³¸ URL.
        """
        self.base_url = base_url

    def get_sample_urls(self) -> List[str]:
        """
        ìˆ˜ì§‘í•  ìƒ˜í”Œ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ìš©)
        """
        # ê¸°ì¡´ 10ê°œì˜ ìƒ˜í”Œ URL ìœ ì§€ (í…ŒìŠ¤íŠ¸ìš©)
        urls: List[str] = [
            f"{self.base_url}docs/introduction",
            f"{self.base_url}docs/get_started/quickstart",
            f"{self.base_url}docs/concepts",
            f"{self.base_url}docs/modules/model_io/llms",
            f"{self.base_url}docs/modules/retrieval/vectorstores",
            f"{self.base_url}docs/modules/chains",
            f"{self.base_url}docs/modules/agents",
            f"{self.base_url}docs/modules/memory",
            f"{self.base_url}docs/expression_language",
            f"{self.base_url}docs/modules/callbacks",
        ]
        return urls

    def get_all_urls(self) -> List[str]:
        """
        ìˆ˜ì§‘í•  ëª¨ë“  ì£¼ìš” LangChain ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ìµœì¢… ì ì¬ìš©)

        NOTE: LangChain ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼ìš” ì„¹ì…˜ URLì„ ìˆ˜ë™ìœ¼ë¡œ ì •ì˜í•¨.
              ì „ì²´ ë¬¸ì„œë¥¼ ë™ì ìœ¼ë¡œ ì°¾ìœ¼ë ¤ë©´ ë³„ë„ ë¡œì§ (ì˜ˆ: Recursive URL Loader)ì´ í•„ìš”í•˜ë‚˜,
              ì—¬ê¸°ì„œëŠ” í”„ë¡œì íŠ¸ ì™„ë£Œë¥¼ ìœ„í•´ ì£¼ìš” ë¬¸ì„œ ëª©ë¡ì„ í™•ì¥í•¨.
        """

        # ğŸ’¡ [í•µì‹¬ ì¶”ê°€]: ì „ì²´ ë¬¸ì„œ ì ì¬ë¥¼ ìœ„í•´ URL ëª©ë¡ì„ ëŒ€í­ í™•ì¥
        all_urls: List[str] = [
            # 1. Getting Started
            f"{self.base_url}docs/introduction",
            f"{self.base_url}docs/get_started/quickstart",
            f"{self.base_url}docs/concepts",
            
            # 2. Key Modules
            f"{self.base_url}docs/modules/model_io/llms",
            f"{self.base_url}docs/modules/model_io/prompts",
            f"{self.base_url}docs/modules/model_io/chat",
            f"{self.base_url}docs/modules/retrieval/vectorstores",
            f"{self.base_url}docs/modules/retrieval/retriever",
            f"{self.base_url}docs/modules/chains",
            f"{self.base_url}docs/modules/agents",
            f"{self.base_url}docs/modules/agents/tools",
            f"{self.base_url}docs/modules/memory",
            
            # 3. Advanced Features (LCEL & Integrations)
            f"{self.base_url}docs/expression_language",
            f"{self.base_url}docs/integrations/llms/openai",
            f"{self.base_url}docs/integrations/llms/anthropic",
            f"{self.base_url}docs/integrations/vectorstores/chroma",
            f"{self.base_url}docs/integrations/vectorstores/faiss",
            f"{self.base_url}docs/modules/callbacks",
            
            # 4. Use Cases
            f"{self.base_url}docs/use_cases/question_answering",
            f"{self.base_url}docs/use_cases/summarization",
            f"{self.base_url}docs/use_cases/chatbots",
            
            # 5. Deployment/Ecosystem
            f"{self.base_url}docs/guides/deployment",
            f"{self.base_url}docs/guides/testing",
            f"{self.base_url}docs/guides/contributing",

            # 6. ì¶”ê°€ì ìœ¼ë¡œ íŒ€ì´ ë‹¤ë£¨ê¸°ë¡œ í•œ í•µì‹¬ í˜ì´ì§€ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
        ]
        return all_urls

    def extract_category(self, url: str) -> str:
        """URLì—ì„œ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        if "introduction" in url:
            return "introduction"
        elif "get_started" in url or "quickstart" in url:
            return "getting_started"
        elif "concepts" in url:
            return "concepts"
        # ì´ì „ ì½”ë“œì—ì„œ ìƒëµë˜ì—ˆë˜ ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬ ë¡œì§ì„ ëª¨ë‘ í¬í•¨
        elif "modules/model_io" in url:
            return "model_io"
        elif "modules/retrieval" in url:
            return "retrieval"
        elif "modules/chains" in url:
            return "chains"
        elif "modules/agents" in url:
            return "agents"
        elif "modules/memory" in url:
            return "memory"
        elif "expression_language" in url or "lcel" in url:
            return "lcel"
        elif "integrations" in url:
            # LLM, Vectorstore ë“± ë‹¤ì–‘í•œ í†µí•© ëª¨ë“ˆ
            return "integrations" 
        elif "use_cases" in url:
            return "use_cases"
        elif "guides" in url or "deployment" in url or "testing" in url:
            return "deployment_guides"
        else:
            return "general"

    def crawl_page(self, url: str) -> Optional[Document]:
        """
        ê°œë³„ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ LangChain Document ê°ì²´ë¡œ ë°˜í™˜
        """
        try:
            # WebBaseLoaderë¥¼ ì‚¬ìš©í•´ í˜ì´ì§€ ë¡œë“œ
            loader = WebBaseLoader(url)
            docs: List[Document] = loader.load()

            if not docs:
                logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ (ë‚´ìš© ì—†ìŒ): {url}")
                return None

            doc: Document = docs[0]

            # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category: str = self.extract_category(url)

            # ë¬¸ì„œ ID ìƒì„±
            # PEP 8: ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ í–‰ì€ í”¼í•˜ê³ , chainingì€ ê°€ë…ì„±ì„ ìœ„í•´ ëŠì„ ìˆ˜ ìˆìŒ
            doc_id: str = (
                url.replace(self.base_url, "")
                .replace("/", "_")
                .replace(".html", "")
            )
            
            # ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë° ì¶”ê°€
            metadata: Dict[str, Any] = doc.metadata
            metadata["doc_id"] = doc_id
            metadata["url"] = url
            metadata["category"] = category
            metadata["timestamp"] = datetime.now().isoformat()
            
            # titleì´ ì—†ìœ¼ë©´ URLì„ ì‚¬ìš©
            if "title" not in metadata or not metadata["title"]:
                metadata["title"] = url.split('/')[-1]

            return Document(page_content=doc.page_content, metadata=metadata)

        except Exception as e:
            # tqdm ë•Œë¬¸ì— ì¶œë ¥ ë°©ì§€í•˜ê³  ëŒ€ì‹  ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡í•˜ê±°ë‚˜, ì—ëŸ¬ ì¹´ìš´íŠ¸ë§Œ í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
            logger.error(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}", exc_info=False)
            return None

    def collect_documents(
        self,
        urls: Optional[List[str]] = None,
        max_pages: Optional[int] = 100,
        delay: float = 1.0,
    ) -> List[Document]:
        """
        ë¬¸ì„œ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜
        """
        if urls is None:
            #  URLsì´ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ì „ì²´ ëª©ë¡ì„ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½
            urls = self.get_all_urls()  
            
        # max_pagesê°€ Noneì´ ì•„ë‹ˆë©´ ìŠ¬ë¼ì´ì‹± (PEP 8 ì¸ë´í…Œì´ì…˜ ìˆ˜ì •)
        if max_pages is not None:
            urls = urls[:max_pages] # <-- ì¸ë´í…Œì´ì…˜ 4ì¹¸ìœ¼ë¡œ ìˆ˜ì • (E111 ìˆ˜ì •)

        documents: List[Document] = []
        
        print(f"ì´ {len(urls)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")

        for url in tqdm(urls, desc="í¬ë¡¤ë§ ì§„í–‰"):
            # í˜ì´ì§€ í¬ë¡¤ë§
            doc: Optional[Document] = self.crawl_page(url)

            if doc:
                documents.append(doc)
                
            # ëŒ€ê¸° ì‹œê°„
            time.sleep(delay)

        print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        return documents


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ë¡œê¹… ì„¤ì •ì„ ì¶”ê°€í•˜ì—¬ logger.error ë“±ì´ ì¶œë ¥ë˜ê²Œ í•¨
    logging.basicConfig(level=logging.INFO) 
    
    print("=" * 50)
    print("Data Collector ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    collector: DataCollector = DataCollector()

    # 1. ìƒ˜í”Œ URL í™•ì¸
    print("\n1. ìƒ˜í”Œ URL í™•ì¸ (10ê°œ):")
    sample_urls: List[str] = collector.get_sample_urls()
    print(f"  ìˆ˜ì§‘ ëŒ€ìƒ URL ê°œìˆ˜: {len(sample_urls)}")

    # 2. ì „ì²´ URL í™•ì¸
    print("\n2. ì „ì²´ URL í™•ì¸:")
    all_urls: List[str] = collector.get_all_urls()
    print(f"  ì „ì²´ URL ê°œìˆ˜: {len(all_urls)}ê°œ")

    # 3. ì‹¤ì œ í…ŒìŠ¤íŠ¸ ìˆ˜ì§‘
    print("\n3. í…ŒìŠ¤íŠ¸ ìˆ˜ì§‘ (2ê°œ í˜ì´ì§€, ì§€ì—° 0.5ì´ˆ):")
    # max_pages=2ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì‹œ í¬ë¡¤ë§ì„ 2ê°œë¡œ ì œí•œ
    test_docs: List[Document] = collector.collect_documents(max_pages=2, delay=0.5)

    if test_docs:
        print(f"  ìˆ˜ì§‘ëœ ë¬¸ì„œ: {len(test_docs)}ê°œ")
    else:
        print("  ìˆ˜ì§‘ëœ ë¬¸ì„œ ì—†ìŒ.")

    print("\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ!")